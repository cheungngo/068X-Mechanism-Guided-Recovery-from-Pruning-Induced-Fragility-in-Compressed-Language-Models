{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "gpuType": "A100",
   "authorship_tag": "ABX9TyPvEAVf8otFlfFKBkTPpI8y"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Isodose - Testing Antidepressant Treatments for Pruning-Induced Fragility in LLMs"
   ],
   "metadata": {
    "id": "cVXDbBOXiOlV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c33a49a00fcc4e6cafaf3056e5972be4",
      "ba35508a3b8f4646b561ad8d9ff2d276",
      "dacc22b4fbef40aea200ad53664d167b",
      "19c88fb01e8343fb9cd41629734fce89",
      "97f1579b07e743249bf5754d99ccc82f",
      "6b46e7e10d654e1c87f07aba2c2338ea",
      "3095e77a5fef4d529684d0d11b7d9787",
      "567e8a55fbef48acaecf21e908b623e8",
      "89de39b448cb42c6b38f64b5ddf4dd3e",
      "0fc2cf823ebc42e78fb2358e3fa37fb5",
      "792b976b426c4559a1d2cb0d95614c63",
      "9150ecfb61aa4f42a9bb06741130d800",
      "3126a42da07b419a9e0be4591898df52",
      "7ce86900a91f40eb81ed6f23efa792b9",
      "fcaa1e971c8b4052854784ca0690dabe",
      "18fd3807b97244ff90eb46445eda081a",
      "ef507fd0593e413cb0f1dbc90b88fafb",
      "85f21a077589411b9f3837d985d4e6d7",
      "d1a711843a47478087256b5a863a1e77",
      "7d15e0c0986a4b6681b9d0a32862c683",
      "fbe0573213724057b9cbc7c91e793ac2",
      "3eed33470a2e492aa968643f3d4c0245",
      "0734041ae9684a029e6a9be8a8acf86e",
      "8c4873971f264faa8e3951157ffead01",
      "9047b0bb9dd24a6ba3a70fb5ebcdbcc8",
      "fab269d3653b4470a96aa2bdea64a188",
      "fdd6785e34b24178b07a6d990e7b5f39",
      "1ba6f7ac5a274592862eb01020fe8b3e",
      "043e41af54bf4639aec957e21c3188d2",
      "10f89142729244fdbaa2a0bc26224080",
      "d18c832630e64fb19ea52badf9778fd1",
      "0414fffc7eb24f7ca271e32af129a898",
      "588da06c47f14c8ab907efe57958f512",
      "1553f2a60d824865a934e37bddb09937",
      "2112261d275543c089a5ee1f135393d9",
      "a70c1a8e677f4c8a8db6fb72e8217de7",
      "87ba525a18e541a5b7710376a119a9e4",
      "30c87bea28de4f7e965511f5d6c6b51c",
      "fce0f3876d05438c8618feea301fc616",
      "9f5d7e2d1e094be99c6ff380b5b9c19c",
      "9b532031c15c4d1a97c44ebdcfe16e42",
      "fcc8def68dc3492584a87d678694334b",
      "0b93d9ec27cb48dc82701fa3ca6fecb9",
      "34721b19fc4241caa5a7711b06c0da5b",
      "cbc36171996c4ed2a74b6c5415b7a069",
      "6be1bf1119fe4ebd8aab99ca4ce790dd",
      "10b58ff1fdbd4d889b40be4a582b412a",
      "57687163db814cc2a1a5b871e2b5a1fb",
      "a78cb7fb0e0f4e7ebe03c3a47ec8b51d",
      "bbf6308158b14406b7aeb1d1fbb47850",
      "435418d08dab4434a3e798775938024a",
      "a7afe918613b4d7fb67b2d2c6fb7ae85",
      "fbd9e66fd9894bc78d8455248ba52be1",
      "de9f880aae10497ab4c50e4cc845da7c",
      "2a330e06446e4838bf7479d469299779",
      "d464660521ca41bc9694c6a3d2d4b854",
      "d4423c4698ce4184bdd8b601b335ebf1",
      "54bfcf38d0b243dab79ee36dcec92bb7",
      "449f2c826d8e4e96b8ad0cd2ff0b4580",
      "3847bbd9e48f4b03a80f102ef39b1695",
      "b0c6c3eb69de47a79a18691073d09ecc",
      "396bb7c6f48240f9a5c38474587d7c50",
      "a03f7ea8eef84d6db2a4590fdac54fa6",
      "5ae9bbde455c4ccbbdd7026dfeeda148",
      "72ef511bc0694d469945ffc92b6c93c1",
      "77b2b7ec4950477bb969a5476e175340",
      "de8c826d97b54340aadd8379655a60df",
      "53012e6649464e659aad37a95a6df788",
      "53ff3519b15e48b1898bcdea65c5e89d",
      "dc6d6a40c7bd4da2b08d49f5a99f4fff",
      "302488a21cef4c239bd67388a33dd352",
      "6c841fa012454a30ab46c52b1693c554",
      "0d4f1357f9b146bcb7fa4fba13f8a09d",
      "ea44d11f70814e06a8c7683d3550c964",
      "b1bfdb6773de48a7b558b9742f7c8512",
      "733f555a6b3f4788a1b99fa422cc572b",
      "23a91930a7ef4c73aeffc80cb732832e",
      "3ec9fd3e8d924962b181f1c13b997b33",
      "6aa34905f7df47cc92c11f4a641b57ea",
      "23fb10c3177340de94633c680b7ae077",
      "7861cfabede84634be44f0e009531c1e",
      "617825fa9863461eb55945a412fb603b",
      "f70c33c7c97641d583f000b505e9c229",
      "39bd1f3385fd494b9b60e8a17496b23c",
      "d610343560c04d3086f9cd05ad23e8e1",
      "2954b432f4544767a95ede1363bfab97",
      "35bcff820645428883341a9657b3890e",
      "fcc017cddbb040ed9b59332275191f90",
      "4394021ff07c4328819fe047af044e37",
      "857cba24d45142e1be0dd626f09c846d",
      "35538ade12154d79ba2ac26e42455eca",
      "29baf265c49d4826ae437810c6c2090c",
      "6c0bd6b8c57c45b491cf1553279fbf9f",
      "517151bd0114416a882e1e662921dc36",
      "7430ff3ea8ba484490bec6b836ec6272",
      "4acc1fd5e9284093a7045c7d8a7f879b",
      "67d775560ba74a058a142b200c43d121",
      "fc1451a96f5645e1b7ec02b0582ee914",
      "9e546f6789ff4149a7afe366ce339371",
      "f72af590c4d3464e84b7c652128ae0e9",
      "47e6fff950864d46be1d15137118bd73",
      "4e1872981e154399b77f696b0ec36d42",
      "a12c0c284bad40d4aea709d46006745b",
      "e0e9af8987fe45b5a241f26a33bac736",
      "c65bccc427ed4127815f5fefba9c775c",
      "59b82935162c4099adb0ee78ba1982fe",
      "6e113fe3335648d794d79cdb7deb0981",
      "4abe8af599854791a423a3976823a565",
      "a1d8795f2f434d8097c7b30768e821d6",
      "69329ab240c544f7a25c2ec8f3caa188",
      "266e85dab9ab4e56bc3eca6cca5885e0",
      "91c8c38c2c3e4fd9bf11b7e44d39e54d",
      "cecc10ea2a7047e380ef4ed8df84a831",
      "1cde87a9fcc14d0d96882ae50bbec982",
      "be2312db5c2945f3af5a4f0c30d0e4d5",
      "f20f80f2465647dd9e0a62a7b566dc18",
      "0be39aea44cb4dc2a6c5086707f05cca",
      "5dff41a7184f45c3ac01d90342321562",
      "c7c2cccd1fe04c97a13451d6df397c8b",
      "3fa51217df4d48c2a3e9663564758562",
      "059c86d9d9e24b6eaaeb08f1a4d83bbc",
      "d3dfc03d8853438598fe17c38d46a61d",
      "63be62af7d1f48dcb266f4778fc2e34e",
      "dbf771e76f0f42fd98d58526ac0f107e",
      "45b8a7567ffe4a44833281661124b7a2",
      "d25f3592b0424f908749d624a16ba47c",
      "dce2f4abc71b4247bc0fac6838b83a9a",
      "5be16a50a3be4b5c9bd5aad13927af17",
      "4e7c261323304e2da2b94e0d650f4374",
      "e325c99fbd0c425fbdc66fcba9dc1fed",
      "e7201d6e0cf44f8eacabaf1be383cff5",
      "a7fa6a48cf314d87a19370a524fbfb9e",
      "6d16f2f5dc834f7eac8ae0bad78620b4",
      "60a067abe84c43da8082585cf4bb9597",
      "5ab5d5f85e614f46b7434a8b7b5b9cba",
      "377941339c3f48ef9f4657b8c69607f9",
      "5356a410a7bf4d6ba161be67d285dd24",
      "4eba72991f554198bce1cbf11bc24c77",
      "8cc06d8a3e1342b3a92dc7c44e71a04b",
      "2a1d3ad708194548870cb4529ff9b3f3",
      "f26fea3caa9f4105ac580763dcb0f982",
      "8c012a0149db4eb7910cb9e2a8d7161b",
      "6677304c692644419a44115593e79022",
      "ad80ee230ce847928f3eb0b5d20f5fb5",
      "dd68b75b41104b2591d5eff6021d332d",
      "b9904108d18a4446a15ea049ab87a90a",
      "1d6411f90f844cc499da9b032c85dc31",
      "e7e69dc203644553b3299e40b9db1340",
      "f5682041e3564c5fab7955c848183e60",
      "93e4c63a7d5248909148fc797ecc4a40",
      "ea054fe5b9a74ba6910185a2197f7d59",
      "e709439c63e043089d193d73490b22ed",
      "fcfd25bc8c24469ca0a4f9136e88e950",
      "0ed81bde5ef94fa19502bcddcf87214e",
      "a7c1baec2c064e689f10779d10806600",
      "f829b960c8424a87a56209c794b31ef8",
      "004ee218b15847d9a181cc10c632f8d6",
      "a3f5c27bd498416aa9eb6b2d602c311a",
      "c4e123b054ec416fa834da7ed5faff20",
      "b8287adf3467484492eabb5a9821f5fc",
      "fb809bf3a593488f92bf608ff50931d5",
      "4f522eaf1f3d48aca648aaed4ac2e2be",
      "c6ba5c34363d43b0803e1c593c5d5adb",
      "bf401a640d994f78859fb5ae746bc526",
      "ad5821c02ba84cbb8a09dc07d4e942d3",
      "25c14110ff4a4e628683ca5b85094cbc",
      "fc04a43302df4f488b12582c4cc47d99",
      "86ca7b156c0e4c4daf8359ac3f8baf23",
      "56db4a00a18f4cc8a1a9c8e7b490d4a8",
      "c268bc63f7104053ab402fb2b55407ce",
      "9e158020d47b4157891c209423e2616a",
      "5d8e86a3141c4d4596196466b549631d",
      "450c5f2ffdf844ad85ae7617d95f665e",
      "e1c02edc1dfb40739300c0beb027cc0e",
      "993eb44d531e4b9ba697c94bf7eb036d",
      "9c747581cc78469cafcd4c3612604724",
      "ad5ccaed8214442b9fe0ea60b9bf1576",
      "6a5045b6c65441d882c7b01ad3bfeafb",
      "a25133f676944bf29b5ba17b9c225850",
      "c9e61047c741419dad28175bad1243d6",
      "769a3794cde3463a8472182333ca3671",
      "9ca284875f4747fc9a47903febfbf17b",
      "7bf3bd35b8ea4a77b0211e45757eb9a2",
      "7fcc6c0a36d04912a39a2bb647edd873",
      "ed1f7350b03b4dea8e7d19f70bf2d841",
      "fbdccb5375b94559aa16942783ffe2ca",
      "f4618b27aabb4a52892880829b3d3bbb",
      "63e91e66a65c4919b6b62c717898c0ab",
      "3d7107792c664d4d840112aad81e9eed",
      "e3737912f26a4c13af797dd24faf27a1",
      "33b5f6a488e74a3dbf9ce1655be3d87f",
      "739561fc2ac246a6b12f30536b1817f3",
      "975a0a2069dd4e9cb2f5baa72ec1b9f3",
      "ad91d111f80f487998d75c6090f189e2",
      "545046db229248aebfdc60cef9af1843",
      "e1fe299e02b94a629fa739bd1ccf2f26",
      "bb33821e4fe948d28ac32eb2cae441f9",
      "213e99c4b67448589d45f9119f7c421d",
      "3de2dc889cbf4b48a7e3f810362f1250",
      "8cb479a4901f4847996def0af9950d5f",
      "abc9482d9416407491c78980a2dc144f",
      "e8e891dbd5b749d29c9bf8c05f0a7bbd",
      "f63b911bda044d81bc8966d9e3ef6943",
      "2d38cfde3cf04417878d2df21c632a72",
      "55ee7fea1d3d4bbc96796b105ebb0625",
      "b914b5d813f84b3b81553878523e31fb",
      "1e33d06a7cbd43c795c5647e5c12682b",
      "4c257e3c3a924574b3b72f8adecfc35f",
      "1ee71d900e3840b890a086e36bb7d7a6",
      "a1c3f4abec264e4393ca981c96d06e4a",
      "8b9361dc5729491c8f61ebf8c3c8b41e",
      "18bacc6523504cc6ba6bdcfcdea97d3d",
      "eb34f91d9e494d799fe241643e310c65",
      "ebfe27996f50488095c6df463dbe713b",
      "e03e204d893b44a98dae99eee94835b2",
      "fd02c57096ee477e91c958ee6598cd96",
      "0f92c7450faa4fb89b77417307f8810a",
      "3da1b9f7f1794e9591caade84e9fb777",
      "b1df1e33802a4005abd13b9969a87144",
      "53ef035979a94eb8ad9e169b75830f2c",
      "40fe2a47949a4a06af53460401e3e46a",
      "cce19873fa84423e86b28ea0b148fbc7",
      "39c73da56d74415b9732508ed3c265b6",
      "c5e5bacb6db74bdea750b3fb2641381b",
      "e964348af0d54b1f9e36d18f74084a0f",
      "475623d883ee46bf9c2b9bd49e569b77",
      "fe8a61eb94cb4d39aaf387d4fc36a9f8",
      "41fc9e71c9f441ba8fa1c5df5f9fca7b",
      "f20bd41ef1ea494c97f012cfcf075e02",
      "5762aee4b45c4d8a826304be3d1e2167",
      "ab39b4ca4ff341f3b864599104bb1280",
      "97cb3666ecdf4ce3b668e17488a88e4f",
      "51dded9374124a47b8cba06741aa4052",
      "8e2cc7a786fe492ebc9db0fd4e2dbd90",
      "c86ce036ef84440691e26b4da71e9e1d",
      "783bb019bd994961a8d9d41bff19579e",
      "493cf13156cf4a19be1ae4b882970b68",
      "4cb00bba2b044c4891f12628875c91f4",
      "ec481fe3d4cd46a9a62cc91ec3cee577",
      "1d5be7e9cf904662a30cdd7d37d72dd7",
      "99d4f730411c4c60905dff090b1edf43",
      "73076bca99d04e15a01bef088c4a857b",
      "2558251903b946a2ba3706077122e1ca",
      "25075a70f50f4156a357e184b19e1a4b",
      "b6858d61fc4143fa92d37c050c1f06c0",
      "8ebc0aaf6ac44ed29003d1a0ccd761e3",
      "cda2c9331c7d447eb0894f2eb096fa07",
      "7f2d71f418d94d4784d96beb0051bcf5",
      "8bbed459f12b461fac8aabd60f149e0b",
      "b165de2dc42c48b392ba53745e023492",
      "b6adc50a9ca94ba18afa1a55afd28f31",
      "2319bc88c7344d5b8768a5524189a73d",
      "60fb05dc7c524e86bacd914e909de2c4",
      "c06b408b8b8e4c0f8cb0f98fa05a633d",
      "57513f142af3448e97a02a19b4aecf2e",
      "9f200300236648afb4e0a4784dcf9c57",
      "1793086a80cb47fdae73ea383e81de11",
      "3647b44aacb94ce58bdb30787235dc08",
      "67baa637ae5c4fc58e2ded507be41141",
      "a23d8ef4938c4a91a04f2aa64e03de2f",
      "81c2b264883b43f487ae9716a1e0d129",
      "958cb61aedb94cb7a1baff5074929ecd",
      "501ddb212fd742698a3244c6cd70c25e",
      "63defdede7fe440a92ad3b5b46e64198",
      "b8cee63e3bd74bb58acc9eda4d32f388",
      "4fdd8ebdb65a4cc99595407e67caf841",
      "98126f925cc8487cbc87949f23cd7bd4",
      "e706ae5626c04bdaa085e0e2a79c222b",
      "243bf172be4b45619a69d38e87397fb1",
      "4d205f646a0844c78986827e2f696acf",
      "f21cc33aefea4da692175aefd45fda61",
      "4e9376c99b5b45d1adb87aa3ec05751d",
      "51d70de5a0f44ecaae474d33b5aaea9f",
      "207c00dab08949689f430e5106c2918d",
      "ba9a7d73826549a4a3906b43c0a04a88",
      "96f80a7c27464f61ba9b336394469e0d",
      "fadb5ad249f8461cad18ca5388ec42ed",
      "d924db19c6d74a2d80e7cf7e3cd45dff",
      "1aec56e470da44fb9144c2caba494bfd",
      "21c62c86be4c4dbe8cfe540186df4b8e",
      "4a17172001394a6289aa576d74059f82",
      "3bd79975e46647acb1e5bab227a7dc96",
      "18102e973d8a4fe5b8511983472f02f6",
      "6fb4826d6ecd4d76a1f37d8976f48b4a",
      "914470826eb84f1eb2aacafd460d1acd",
      "2629e1199aba4292ac57a7ace3fcd83e",
      "536f265bdcce4a9cb11fbbc50f148d61",
      "3ecb5cc5e3f44c3682a82da0535a411f",
      "4cdaf6dd1ff347cbb18092eb6c24a4d9",
      "8afa622943bb4edabb1308e9cddda1be",
      "9862423edcf04fdd83d59868ca3aaaba",
      "ec775a5b0a5a4d8abf4e373a5944a34d",
      "4d900ac3c9f14195b97d7564aae8d08d",
      "e8da95a2007047f485fb9ead2f2da6aa",
      "de64958638bd419dad43d27e189335c6",
      "4285481006c9498e89788188a8cb3ed7",
      "751f44119bd944049410cc68a6d85607",
      "95438861f31d4ad8843c7dbd1ec745fe",
      "9621e359743a43929cb5894ef1415d00",
      "0109555274304b8791b87fabe51d663c",
      "78a021014def4a9f927306a87529debb",
      "58dcdde242ae43c2896e7a6c8ba5c4a2",
      "e298c213526b4ee1adfec8b7f50b656d",
      "713bb9c6925e413c8fd9ea7ebc112f0b",
      "e1a92292ec2e41688747d43bfd719ca3",
      "a8c2a3c76f9945cdb6810e807a11949f",
      "616b6745771d441d93aa3451c03f0c9a",
      "a5eea3a9ad3343179d7e030449ce0df2",
      "a7b352c320f941b2afdcb9a49351cb24",
      "5030ea2de3a142778a64991f97ff927c",
      "a316c7bd5b974745903754db04fbc870",
      "180e62947a824b6fbcc19079db377dba",
      "8c90f6be7a42482da73dd091fde7fe6b",
      "8d90be53f9004f77afa3b2ca0a41778b",
      "faece2da45d240bfb960b8f8b212df6a",
      "5ebdf66f0b7c47ca98cb7b8e65200f2a",
      "ad7ca42b9d0a45d1a98d4bdeeb0af793",
      "fb2dde7f96134a8abd0f1c5562f964c0",
      "5e49b5ebcd97484f90d68a50fa64a795",
      "91976bb3802b43098b6ee9f663f6d319",
      "06af8eb1099f4dfd98f2f844f79204f0",
      "f7e6fb7215fa4adfb991324a5751fd74",
      "143ec88620ce45b4879e0e1baaf1cb59",
      "55ffda0f4abb44f9acc91af8ea5dda55",
      "35b814bbd2154abdbd586bac76f461a1",
      "a53df95ed54240a6b77eabd4788b4539",
      "60e2a99b19804d2ba1df82b74bdfb764",
      "9c7dd212d379437a874032bb0fa5cf3e",
      "9c345c0694f743aeabcc007887b4dcae",
      "fa82b2ced32244ce9c566b525ec2c576"
     ]
    },
    "id": "XW8cglr1iODJ",
    "outputId": "9372d49c-28c7-45bf-c4bf-49c11940df98",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1769768765020,
     "user_tz": -480,
     "elapsed": 12729802,
     "user": {
      "displayName": "Ngo Cheung",
      "userId": "02091267041339546959"
     }
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "================================================================================\n",
      "EXPERIMENT MODE SELECTION\n",
      "================================================================================\n",
      "  isodose_mode = True\n",
      "  -> Treatments will be calibrated to EQUIVALENT computational cost\n",
      "  -> This enables FAIR comparison of mechanism efficacy\n",
      "================================================================================\n",
      "\n",
      "\n",
      "################################################################################\n",
      "################################################################################\n",
      "#                                                                              #\n",
      "#           MULTI-MECHANISM RECOVERY FROM PRUNING-INDUCED FRAGILITY            #\n",
      "#                           IN LARGE LANGUAGE MODELS                           #\n",
      "#                          [WITH ISODOSE COMPARISON]                           #\n",
      "#                                                                              #\n",
      "################################################################################\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT CONFIGURATION\n",
      "================================================================================\n",
      "  Base model (pre-pruned): oopere/pruned60-llama-3.2-1B\n",
      "  Pruning level: 60% MLP neurons removed (severe fragility)\n",
      "  Number of seeds: 3\n",
      "  Evaluation samples: 200\n",
      "  Fine-tune dataset: databricks/databricks-dolly-15k\n",
      "  Fine-tune subset: 500 samples\n",
      "  Longitudinal cycles: 8\n",
      "  Acute relapse pruning: 30%\n",
      "  Chronic stress pruning: 10% per cycle\n",
      "  ISODOSE MODE: True\n",
      "\n",
      "======================================================================\n",
      "ISODOSE CALIBRATION\n",
      "======================================================================\n",
      "  Reference treatment: KETAMINE\n",
      "  Calibration strategy: Adjust epochs to match reference FLOPs\n",
      "  Preserved characteristics:\n",
      "    - Ketamine: High rank (64), many modules\n",
      "    - SSRI: Low rank (8), gentle LR (1e-06)\n",
      "    - Neurosteroid: High dropout (0.5)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "  REFERENCE: KETAMINE\n",
      "    LoRA rank: 64\n",
      "    Epochs: 3\n",
      "    Trainable params: 29,360,128\n",
      "    Estimated FLOPs: 1.35e+14\n",
      "\n",
      "    Calibrating SSRI to match 1.35e+14 FLOPs\n",
      "      Original epochs: 15\n",
      "      Isodose epochs: 84\n",
      "      FLOP ratio: 1.00x\n",
      "\n",
      "    Calibrating NEUROSTEROID to match 1.35e+14 FLOPs\n",
      "      Original epochs: 5\n",
      "      Isodose epochs: 10\n",
      "      FLOP ratio: 0.95x\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "  ISODOSE CALIBRATION SUMMARY\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "  Treatment         Rank   Epochs       Params          FLOPs    Ratio\n",
      "  -----------------------------------------------------------------\n",
      "  Ketamine            64        3   29,360,128       1.35e+14    1.00x\n",
      "  Ssri                 8       84    1,048,576       1.35e+14    1.00x\n",
      "  Neurosteroid        32       10    8,388,608       1.29e+14    0.95x\n",
      "======================================================================\n",
      "================================================================================\n",
      "\n",
      "********************************************************************************\n",
      "  STARTING SEED 1/3\n",
      "********************************************************************************\n",
      "\n",
      "######################################################################\n",
      "#  SEED 0 EXPERIMENT\n",
      "#  [ISODOSE MODE ACTIVE]\n",
      "######################################################################\n",
      "----------------------------------------------------------------------\n",
      "LOADING EVALUATION DATASETS\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c33a49a00fcc4e6cafaf3056e5972be4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "ARC-Easy/train-00000-of-00001.parquet:   0%|          | 0.00/331k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9150ecfb61aa4f42a9bb06741130d800"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "ARC-Easy/test-00000-of-00001.parquet:   0%|          | 0.00/346k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0734041ae9684a029e6a9be8a8acf86e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "ARC-Easy/validation-00000-of-00001.parqu(…):   0%|          | 0.00/86.1k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1553f2a60d824865a934e37bddb09937"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/2251 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cbc36171996c4ed2a74b6c5415b7a069"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/2376 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d464660521ca41bc9694c6a3d2d4b854"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/570 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de8c826d97b54340aadd8379655a60df"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  ARC-Easy: 200 samples loaded\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ec9fd3e8d924962b181f1c13b997b33"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "plain_text/train-00000-of-00002.parquet:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4394021ff07c4328819fe047af044e37"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "plain_text/train-00001-of-00002.parquet:   0%|          | 0.00/281M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f72af590c4d3464e84b7c652128ae0e9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "266e85dab9ab4e56bc3eca6cca5885e0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "plain_text/validation-00000-of-00001.par(…):   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3dfc03d8853438598fe17c38d46a61d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/2662 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d16f2f5dc834f7eac8ae0bad78620b4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/5153 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad80ee230ce847928f3eb0b5d20f5fb5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/4869 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a7c1baec2c064e689f10779d10806600"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  LAMBADA: 100 samples loaded\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[STEP 1] Loading pruned model (untreated baseline)\n",
      "======================================================================\n",
      "LOADING PRE-PRUNED MODEL\n",
      "======================================================================\n",
      "  Model: oopere/pruned60-llama-3.2-1B\n",
      "  Dtype: torch.float16 (NO 4-bit quantization)\n",
      "  Device map: auto\n",
      "  Expected state: 60% MLP neurons removed (fragile knowledge)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/883 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "25c14110ff4a4e628683ca5b85094cbc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.51G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad5ccaed8214442b9fe0ea60b9bf1576"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/180 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63e91e66a65c4919b6b62c717898c0ab"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3de2dc889cbf4b48a7e3f810362f1250"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1c3f4abec264e4393ca981c96d06e4a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/335 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40fe2a47949a4a06af53460401e3e46a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Load time: 7.87s\n",
      "  Total parameters: 752,650,240\n",
      "  Trainable parameters: 752,650,240\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float16\n",
      "======================================================================\n",
      "  [SUCCESS] Pruned model loaded (fragile state confirmed)\n",
      "======================================================================\n",
      "\n",
      "[STEP 2] Evaluating UNTREATED (pruned) baseline\n",
      "----------------------------------------------------------------------\n",
      "    Running ARC-Easy evaluation... Accuracy: 22.5%\n",
      "    Running LAMBADA evaluation... Accuracy: 20.0%\n",
      "  UNTREATED BASELINE:\n",
      "    ARC-Easy: 22.5%\n",
      "    LAMBADA:  20.0%\n",
      "    Composite: 21.2%\n",
      "----------------------------------------------------------------------\n",
      "PREPARING FINE-TUNING DATASET (seed=0)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97cb3666ecdf4ce3b668e17488a88e4f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "databricks-dolly-15k.jsonl:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2558251903b946a2ba3706077122e1ca"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c06b408b8b8e4c0f8cb0f98fa05a633d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Dataset: databricks/databricks-dolly-15k\n",
      "  Subset size: 500 samples\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8cee63e3bd74bb58acc9eda4d32f388"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96f80a7c27464f61ba9b336394469e0d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Tokenized: max_length=512\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[STEP 3.1] Processing KETAMINE treatment\n",
      "  Using ISODOSE parameters (target FLOPs: 1.35e+14)\n",
      "  Reloading fresh pruned model...\n",
      "======================================================================\n",
      "LOADING PRE-PRUNED MODEL\n",
      "======================================================================\n",
      "  Model: oopere/pruned60-llama-3.2-1B\n",
      "  Dtype: torch.float16 (NO 4-bit quantization)\n",
      "  Device map: auto\n",
      "  Expected state: 60% MLP neurons removed (fragile knowledge)\n",
      "----------------------------------------------------------------------\n",
      "  Load time: 3.53s\n",
      "  Total parameters: 752,650,240\n",
      "  Trainable parameters: 752,650,240\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float16\n",
      "======================================================================\n",
      "  [SUCCESS] Pruned model loaded (fragile state confirmed)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "APPLYING KETAMINE TREATMENT\n",
      "  [ISODOSE MODE]\n",
      "======================================================================\n",
      "  LoRA rank: 64\n",
      "  LoRA alpha: 128\n",
      "  Training epochs: 3\n",
      "  Learning rate: 5e-05\n",
      "  Dropout: 0.05\n",
      "  Target modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
      "  Estimated FLOPs: 1.35e+14\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Actual trainable parameters: 29,989,888 (3.83%)\n",
      "  Starting training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 00:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.800200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Training complete in 61.6s\n",
      "======================================================================\n",
      "\n",
      "  Evaluating POST-KETAMINE performance:\n",
      "    Running ARC-Easy evaluation... Accuracy: 34.5%\n",
      "    Running LAMBADA evaluation... Accuracy: 14.0%\n",
      "    ARC-Easy: 34.5%\n",
      "    LAMBADA:  14.0%\n",
      "    Composite: 24.2%\n",
      "    Recovery from untreated: +3.0%\n",
      "    Efficiency (recovery/PetaFLOP): 22.17\n",
      "\n",
      "  Simulating ACUTE RELAPSE:\n",
      "    Applying acute relapse: 30% unstructured pruning\n",
      "    Pruned 337 linear layers\n",
      "  Evaluating POST-RELAPSE performance:\n",
      "    Running ARC-Easy evaluation... Accuracy: 27.5%\n",
      "    Running LAMBADA evaluation... Accuracy: 16.0%\n",
      "    ARC-Easy: 27.5%\n",
      "    LAMBADA:  16.0%\n",
      "    Composite: 21.8%\n",
      "    Relapse drop: 2.5%\n",
      "\n",
      "  Running LONGITUDINAL simulation for ketamine:\n",
      "======================================================================\n",
      "LOADING PRE-PRUNED MODEL\n",
      "======================================================================\n",
      "  Model: oopere/pruned60-llama-3.2-1B\n",
      "  Dtype: torch.float16 (NO 4-bit quantization)\n",
      "  Device map: auto\n",
      "  Expected state: 60% MLP neurons removed (fragile knowledge)\n",
      "----------------------------------------------------------------------\n",
      "  Load time: 1.62s\n",
      "  Total parameters: 752,650,240\n",
      "  Trainable parameters: 752,650,240\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float16\n",
      "======================================================================\n",
      "  [SUCCESS] Pruned model loaded (fragile state confirmed)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "APPLYING KETAMINE TREATMENT\n",
      "  [ISODOSE MODE]\n",
      "======================================================================\n",
      "  LoRA rank: 64\n",
      "  LoRA alpha: 128\n",
      "  Training epochs: 3\n",
      "  Learning rate: 5e-05\n",
      "  Dropout: 0.05\n",
      "  Target modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
      "  Estimated FLOPs: 1.35e+14\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Actual trainable parameters: 29,989,888 (3.83%)\n",
      "  Starting training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.800200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Training complete in 61.9s\n",
      "======================================================================\n",
      "\n",
      "  --- Longitudinal Simulation for KETAMINE ---\n",
      "    Cycle 0: ARC-Easy = 30.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 1: ARC-Easy = 20.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 2: ARC-Easy = 14.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 3: ARC-Easy = 16.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 4: ARC-Easy = 26.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 5: ARC-Easy = 6.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 6: ARC-Easy = 14.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 7: ARC-Easy = 20.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 8: ARC-Easy = 20.0%\n",
      "\n",
      "[STEP 3.2] Processing SSRI treatment\n",
      "  Using ISODOSE parameters (target FLOPs: 1.35e+14)\n",
      "  Reloading fresh pruned model...\n",
      "======================================================================\n",
      "LOADING PRE-PRUNED MODEL\n",
      "======================================================================\n",
      "  Model: oopere/pruned60-llama-3.2-1B\n",
      "  Dtype: torch.float16 (NO 4-bit quantization)\n",
      "  Device map: auto\n",
      "  Expected state: 60% MLP neurons removed (fragile knowledge)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Load time: 1.70s\n",
      "  Total parameters: 752,650,240\n",
      "  Trainable parameters: 752,650,240\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float16\n",
      "======================================================================\n",
      "  [SUCCESS] Pruned model loaded (fragile state confirmed)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "APPLYING SSRI TREATMENT\n",
      "  [ISODOSE MODE]\n",
      "======================================================================\n",
      "  LoRA rank: 8\n",
      "  LoRA alpha: 16\n",
      "  Training epochs: 84\n",
      "  Learning rate: 1e-06\n",
      "  Dropout: 0.1\n",
      "  Target modules: ['q_proj', 'v_proj']\n",
      "  Estimated FLOPs: 1.35e+14\n",
      "----------------------------------------------------------------------\n",
      "  Actual trainable parameters: 851,968 (0.11%)\n",
      "  Starting training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2688' max='2688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2688/2688 20:46, Epoch 84/84]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.342600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.366400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.333600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.338000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>5.303300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>5.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>5.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>5.139600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>5.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>5.016800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4.962600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.921700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>4.852100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.883100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>4.811700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.763900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>4.774100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.718500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>4.709900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>4.670900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>4.645600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.612300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>4.599500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>4.550100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>4.577500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>4.531200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>4.521600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.509400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>4.489400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>4.463900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>4.450100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>4.481000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>4.438200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>4.413600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>4.433800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>4.422500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>4.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.388300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>4.376400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>4.355400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>4.374600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>4.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>4.353500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>4.356700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>4.346200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>4.360400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>4.344000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.320700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>4.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>4.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>4.337200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Training complete in 1247.4s\n",
      "======================================================================\n",
      "\n",
      "  Evaluating POST-SSRI performance:\n",
      "    Running ARC-Easy evaluation... Accuracy: 23.0%\n",
      "    Running LAMBADA evaluation... Accuracy: 16.0%\n",
      "    ARC-Easy: 23.0%\n",
      "    LAMBADA:  16.0%\n",
      "    Composite: 19.5%\n",
      "    Recovery from untreated: -1.8%\n",
      "    Efficiency (recovery/PetaFLOP): -12.94\n",
      "\n",
      "  Simulating ACUTE RELAPSE:\n",
      "    Applying acute relapse: 30% unstructured pruning\n",
      "    Pruned 177 linear layers\n",
      "  Evaluating POST-RELAPSE performance:\n",
      "    Running ARC-Easy evaluation... Accuracy: 31.5%\n",
      "    Running LAMBADA evaluation... Accuracy: 13.0%\n",
      "    ARC-Easy: 31.5%\n",
      "    LAMBADA:  13.0%\n",
      "    Composite: 22.2%\n",
      "    Relapse drop: -2.8%\n",
      "\n",
      "  Running LONGITUDINAL simulation for ssri:\n",
      "======================================================================\n",
      "LOADING PRE-PRUNED MODEL\n",
      "======================================================================\n",
      "  Model: oopere/pruned60-llama-3.2-1B\n",
      "  Dtype: torch.float16 (NO 4-bit quantization)\n",
      "  Device map: auto\n",
      "  Expected state: 60% MLP neurons removed (fragile knowledge)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Load time: 1.80s\n",
      "  Total parameters: 752,650,240\n",
      "  Trainable parameters: 752,650,240\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float16\n",
      "======================================================================\n",
      "  [SUCCESS] Pruned model loaded (fragile state confirmed)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "APPLYING SSRI TREATMENT\n",
      "  [ISODOSE MODE]\n",
      "======================================================================\n",
      "  LoRA rank: 8\n",
      "  LoRA alpha: 16\n",
      "  Training epochs: 84\n",
      "  Learning rate: 1e-06\n",
      "  Dropout: 0.1\n",
      "  Target modules: ['q_proj', 'v_proj']\n",
      "  Estimated FLOPs: 1.35e+14\n",
      "----------------------------------------------------------------------\n",
      "  Actual trainable parameters: 851,968 (0.11%)\n",
      "  Starting training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2688' max='2688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2688/2688 20:48, Epoch 84/84]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.342600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.366400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.333500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.338000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>5.303300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>5.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>5.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>5.139600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>5.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>5.016800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4.962600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.921700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>4.852000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.883100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>4.811700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.763900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>4.774100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.718500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>4.709900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>4.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>4.645600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.612300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>4.599500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>4.550100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>4.577500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>4.531200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>4.521600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.509400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>4.489400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>4.463900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>4.450200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>4.481100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>4.438200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>4.413600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>4.433800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>4.422500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>4.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.388300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>4.376400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>4.355400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>4.374600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>4.342100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>4.353500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>4.356700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>4.346200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>4.360400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>4.344000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.320700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>4.351900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>4.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>4.337200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Training complete in 1249.6s\n",
      "======================================================================\n",
      "\n",
      "  --- Longitudinal Simulation for SSRI ---\n",
      "    Cycle 0: ARC-Easy = 20.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 1: ARC-Easy = 24.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 2: ARC-Easy = 6.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 3: ARC-Easy = 12.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 4: ARC-Easy = 8.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 5: ARC-Easy = 30.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 6: ARC-Easy = 4.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 7: ARC-Easy = 28.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 8: ARC-Easy = 18.0%\n",
      "\n",
      "[STEP 3.3] Processing NEUROSTEROID treatment\n",
      "  Using ISODOSE parameters (target FLOPs: 1.29e+14)\n",
      "  Reloading fresh pruned model...\n",
      "======================================================================\n",
      "LOADING PRE-PRUNED MODEL\n",
      "======================================================================\n",
      "  Model: oopere/pruned60-llama-3.2-1B\n",
      "  Dtype: torch.float16 (NO 4-bit quantization)\n",
      "  Device map: auto\n",
      "  Expected state: 60% MLP neurons removed (fragile knowledge)\n",
      "----------------------------------------------------------------------\n",
      "  Load time: 39.45s\n",
      "  Total parameters: 752,650,240\n",
      "  Trainable parameters: 752,650,240\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float16\n",
      "======================================================================\n",
      "  [SUCCESS] Pruned model loaded (fragile state confirmed)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "APPLYING NEUROSTEROID TREATMENT\n",
      "  [ISODOSE MODE]\n",
      "======================================================================\n",
      "  LoRA rank: 32\n",
      "  LoRA alpha: 64\n",
      "  Training epochs: 10\n",
      "  Learning rate: 3e-05\n",
      "  Dropout: 0.5\n",
      "  Target modules: ['q_proj', 'v_proj', 'gate_proj', 'up_proj']\n",
      "  Estimated FLOPs: 1.29e+14\n",
      "----------------------------------------------------------------------\n",
      "  Actual trainable parameters: 8,860,672 (1.16%)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Starting training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 02:46, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.624400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.612300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.372100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.279200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.150600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Training complete in 167.9s\n",
      "======================================================================\n",
      "\n",
      "  Evaluating POST-NEUROSTEROID performance:\n",
      "    Running ARC-Easy evaluation... Accuracy: 30.0%\n",
      "    Running LAMBADA evaluation... Accuracy: 11.0%\n",
      "    ARC-Easy: 30.0%\n",
      "    LAMBADA:  11.0%\n",
      "    Composite: 20.5%\n",
      "    Recovery from untreated: -0.8%\n",
      "    Efficiency (recovery/PetaFLOP): -5.82\n",
      "\n",
      "  Simulating ACUTE RELAPSE:\n",
      "    Applying acute relapse: 30% unstructured pruning\n",
      "    Pruned 241 linear layers\n",
      "  Evaluating POST-RELAPSE performance:\n",
      "    Running ARC-Easy evaluation... Accuracy: 22.0%\n",
      "    Running LAMBADA evaluation... Accuracy: 15.0%\n",
      "    ARC-Easy: 22.0%\n",
      "    LAMBADA:  15.0%\n",
      "    Composite: 18.5%\n",
      "    Relapse drop: 2.0%\n",
      "\n",
      "  Running LONGITUDINAL simulation for neurosteroid:\n",
      "======================================================================\n",
      "LOADING PRE-PRUNED MODEL\n",
      "======================================================================\n",
      "  Model: oopere/pruned60-llama-3.2-1B\n",
      "  Dtype: torch.float16 (NO 4-bit quantization)\n",
      "  Device map: auto\n",
      "  Expected state: 60% MLP neurons removed (fragile knowledge)\n",
      "----------------------------------------------------------------------\n",
      "  Load time: 2.49s\n",
      "  Total parameters: 752,650,240\n",
      "  Trainable parameters: 752,650,240\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float16\n",
      "======================================================================\n",
      "  [SUCCESS] Pruned model loaded (fragile state confirmed)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "APPLYING NEUROSTEROID TREATMENT\n",
      "  [ISODOSE MODE]\n",
      "======================================================================\n",
      "  LoRA rank: 32\n",
      "  LoRA alpha: 64\n",
      "  Training epochs: 10\n",
      "  Learning rate: 3e-05\n",
      "  Dropout: 0.5\n",
      "  Target modules: ['q_proj', 'v_proj', 'gate_proj', 'up_proj']\n",
      "  Estimated FLOPs: 1.29e+14\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Actual trainable parameters: 8,860,672 (1.16%)\n",
      "  Starting training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 02:48, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.624400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.612300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.372000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.279200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.169900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.150600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Training complete in 169.4s\n",
      "======================================================================\n",
      "\n",
      "  --- Longitudinal Simulation for NEUROSTEROID ---\n",
      "    Cycle 0: ARC-Easy = 24.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 1: ARC-Easy = 34.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 2: ARC-Easy = 8.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 3: ARC-Easy = 28.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 4: ARC-Easy = 30.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 5: ARC-Easy = 8.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 6: ARC-Easy = 18.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 7: ARC-Easy = 20.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 8: ARC-Easy = 18.0%\n",
      "\n",
      "######################################################################\n",
      "#  SEED 0 COMPLETE\n",
      "######################################################################\n",
      "\n",
      "  [SAVED] Seed 0 results saved to ./llm_treatment_results/seed_0_results.json\n",
      "\n",
      "********************************************************************************\n",
      "  STARTING SEED 2/3\n",
      "********************************************************************************\n",
      "\n",
      "######################################################################\n",
      "#  SEED 1 EXPERIMENT\n",
      "#  [ISODOSE MODE ACTIVE]\n",
      "######################################################################\n",
      "----------------------------------------------------------------------\n",
      "LOADING EVALUATION DATASETS\n",
      "----------------------------------------------------------------------\n",
      "  ARC-Easy: 200 samples loaded\n",
      "  LAMBADA: 100 samples loaded\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[STEP 1] Loading pruned model (untreated baseline)\n",
      "======================================================================\n",
      "LOADING PRE-PRUNED MODEL\n",
      "======================================================================\n",
      "  Model: oopere/pruned60-llama-3.2-1B\n",
      "  Dtype: torch.float16 (NO 4-bit quantization)\n",
      "  Device map: auto\n",
      "  Expected state: 60% MLP neurons removed (fragile knowledge)\n",
      "----------------------------------------------------------------------\n",
      "  Load time: 2.83s\n",
      "  Total parameters: 752,650,240\n",
      "  Trainable parameters: 752,650,240\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float16\n",
      "======================================================================\n",
      "  [SUCCESS] Pruned model loaded (fragile state confirmed)\n",
      "======================================================================\n",
      "\n",
      "[STEP 2] Evaluating UNTREATED (pruned) baseline\n",
      "----------------------------------------------------------------------\n",
      "    Running ARC-Easy evaluation... Accuracy: 22.5%\n",
      "    Running LAMBADA evaluation... Accuracy: 20.0%\n",
      "  UNTREATED BASELINE:\n",
      "    ARC-Easy: 22.5%\n",
      "    LAMBADA:  20.0%\n",
      "    Composite: 21.2%\n",
      "----------------------------------------------------------------------\n",
      "PREPARING FINE-TUNING DATASET (seed=1)\n",
      "----------------------------------------------------------------------\n",
      "  Dataset: databricks/databricks-dolly-15k\n",
      "  Subset size: 500 samples\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "536f265bdcce4a9cb11fbbc50f148d61"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95438861f31d4ad8843c7dbd1ec745fe"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Tokenized: max_length=512\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[STEP 3.1] Processing KETAMINE treatment\n",
      "  Using ISODOSE parameters (target FLOPs: 1.35e+14)\n",
      "  Reloading fresh pruned model...\n",
      "======================================================================\n",
      "LOADING PRE-PRUNED MODEL\n",
      "======================================================================\n",
      "  Model: oopere/pruned60-llama-3.2-1B\n",
      "  Dtype: torch.float16 (NO 4-bit quantization)\n",
      "  Device map: auto\n",
      "  Expected state: 60% MLP neurons removed (fragile knowledge)\n",
      "----------------------------------------------------------------------\n",
      "  Load time: 1.59s\n",
      "  Total parameters: 752,650,240\n",
      "  Trainable parameters: 752,650,240\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float16\n",
      "======================================================================\n",
      "  [SUCCESS] Pruned model loaded (fragile state confirmed)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "APPLYING KETAMINE TREATMENT\n",
      "  [ISODOSE MODE]\n",
      "======================================================================\n",
      "  LoRA rank: 64\n",
      "  LoRA alpha: 128\n",
      "  Training epochs: 3\n",
      "  Learning rate: 5e-05\n",
      "  Dropout: 0.05\n",
      "  Target modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
      "  Estimated FLOPs: 1.35e+14\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Actual trainable parameters: 29,989,888 (3.83%)\n",
      "  Starting training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.795800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Training complete in 61.8s\n",
      "======================================================================\n",
      "\n",
      "  Evaluating POST-KETAMINE performance:\n",
      "    Running ARC-Easy evaluation... Accuracy: 34.0%\n",
      "    Running LAMBADA evaluation... Accuracy: 16.0%\n",
      "    ARC-Easy: 34.0%\n",
      "    LAMBADA:  16.0%\n",
      "    Composite: 25.0%\n",
      "    Recovery from untreated: +3.8%\n",
      "    Efficiency (recovery/PetaFLOP): 27.72\n",
      "\n",
      "  Simulating ACUTE RELAPSE:\n",
      "    Applying acute relapse: 30% unstructured pruning\n",
      "    Pruned 337 linear layers\n",
      "  Evaluating POST-RELAPSE performance:\n",
      "    Running ARC-Easy evaluation... Accuracy: 27.5%\n",
      "    Running LAMBADA evaluation... Accuracy: 20.0%\n",
      "    ARC-Easy: 27.5%\n",
      "    LAMBADA:  20.0%\n",
      "    Composite: 23.8%\n",
      "    Relapse drop: 1.2%\n",
      "\n",
      "  Running LONGITUDINAL simulation for ketamine:\n",
      "======================================================================\n",
      "LOADING PRE-PRUNED MODEL\n",
      "======================================================================\n",
      "  Model: oopere/pruned60-llama-3.2-1B\n",
      "  Dtype: torch.float16 (NO 4-bit quantization)\n",
      "  Device map: auto\n",
      "  Expected state: 60% MLP neurons removed (fragile knowledge)\n",
      "----------------------------------------------------------------------\n",
      "  Load time: 26.27s\n",
      "  Total parameters: 752,650,240\n",
      "  Trainable parameters: 752,650,240\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float16\n",
      "======================================================================\n",
      "  [SUCCESS] Pruned model loaded (fragile state confirmed)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "APPLYING KETAMINE TREATMENT\n",
      "  [ISODOSE MODE]\n",
      "======================================================================\n",
      "  LoRA rank: 64\n",
      "  LoRA alpha: 128\n",
      "  Training epochs: 3\n",
      "  Learning rate: 5e-05\n",
      "  Dropout: 0.05\n",
      "  Target modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
      "  Estimated FLOPs: 1.35e+14\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Actual trainable parameters: 29,989,888 (3.83%)\n",
      "  Starting training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.795700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Training complete in 61.8s\n",
      "======================================================================\n",
      "\n",
      "  --- Longitudinal Simulation for KETAMINE ---\n",
      "    Cycle 0: ARC-Easy = 30.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 1: ARC-Easy = 22.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 2: ARC-Easy = 16.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 3: ARC-Easy = 24.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 4: ARC-Easy = 18.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 5: ARC-Easy = 8.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 6: ARC-Easy = 18.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 7: ARC-Easy = 20.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 8: ARC-Easy = 16.0%\n",
      "\n",
      "[STEP 3.2] Processing SSRI treatment\n",
      "  Using ISODOSE parameters (target FLOPs: 1.35e+14)\n",
      "  Reloading fresh pruned model...\n",
      "======================================================================\n",
      "LOADING PRE-PRUNED MODEL\n",
      "======================================================================\n",
      "  Model: oopere/pruned60-llama-3.2-1B\n",
      "  Dtype: torch.float16 (NO 4-bit quantization)\n",
      "  Device map: auto\n",
      "  Expected state: 60% MLP neurons removed (fragile knowledge)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Load time: 2.36s\n",
      "  Total parameters: 752,650,240\n",
      "  Trainable parameters: 752,650,240\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float16\n",
      "======================================================================\n",
      "  [SUCCESS] Pruned model loaded (fragile state confirmed)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "APPLYING SSRI TREATMENT\n",
      "  [ISODOSE MODE]\n",
      "======================================================================\n",
      "  LoRA rank: 8\n",
      "  LoRA alpha: 16\n",
      "  Training epochs: 84\n",
      "  Learning rate: 1e-06\n",
      "  Dropout: 0.1\n",
      "  Target modules: ['q_proj', 'v_proj']\n",
      "  Estimated FLOPs: 1.35e+14\n",
      "----------------------------------------------------------------------\n",
      "  Actual trainable parameters: 851,968 (0.11%)\n",
      "  Starting training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2688' max='2688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2688/2688 20:49, Epoch 84/84]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.358600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.347300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.351900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.332400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>5.308300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.277200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>5.241200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>5.207900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>5.109400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.075900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>5.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>5.016500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4.980700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.906900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>4.881400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.852300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>4.795000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.782100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>4.738100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.710800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>4.684900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>4.647000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>4.652300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.603300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>4.582200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>4.578800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>4.557600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>4.514400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>4.529400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.521700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>4.492700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>4.475500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>4.479500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>4.427600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>4.459200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>4.402900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>4.412400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>4.413400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>4.402500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.380200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>4.410100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>4.393100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>4.364700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>4.351700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>4.356900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>4.363100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>4.359400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>4.331900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>4.331500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.349500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>4.353700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>4.321900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>4.313700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Training complete in 1250.0s\n",
      "======================================================================\n",
      "\n",
      "  Evaluating POST-SSRI performance:\n",
      "    Running ARC-Easy evaluation... Accuracy: 22.5%\n",
      "    Running LAMBADA evaluation... Accuracy: 18.0%\n",
      "    ARC-Easy: 22.5%\n",
      "    LAMBADA:  18.0%\n",
      "    Composite: 20.2%\n",
      "    Recovery from untreated: -1.0%\n",
      "    Efficiency (recovery/PetaFLOP): -7.39\n",
      "\n",
      "  Simulating ACUTE RELAPSE:\n",
      "    Applying acute relapse: 30% unstructured pruning\n",
      "    Pruned 177 linear layers\n",
      "  Evaluating POST-RELAPSE performance:\n",
      "    Running ARC-Easy evaluation... Accuracy: 30.5%\n",
      "    Running LAMBADA evaluation... Accuracy: 13.0%\n",
      "    ARC-Easy: 30.5%\n",
      "    LAMBADA:  13.0%\n",
      "    Composite: 21.8%\n",
      "    Relapse drop: -1.5%\n",
      "\n",
      "  Running LONGITUDINAL simulation for ssri:\n",
      "======================================================================\n",
      "LOADING PRE-PRUNED MODEL\n",
      "======================================================================\n",
      "  Model: oopere/pruned60-llama-3.2-1B\n",
      "  Dtype: torch.float16 (NO 4-bit quantization)\n",
      "  Device map: auto\n",
      "  Expected state: 60% MLP neurons removed (fragile knowledge)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Load time: 1.86s\n",
      "  Total parameters: 752,650,240\n",
      "  Trainable parameters: 752,650,240\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float16\n",
      "======================================================================\n",
      "  [SUCCESS] Pruned model loaded (fragile state confirmed)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "APPLYING SSRI TREATMENT\n",
      "  [ISODOSE MODE]\n",
      "======================================================================\n",
      "  LoRA rank: 8\n",
      "  LoRA alpha: 16\n",
      "  Training epochs: 84\n",
      "  Learning rate: 1e-06\n",
      "  Dropout: 0.1\n",
      "  Target modules: ['q_proj', 'v_proj']\n",
      "  Estimated FLOPs: 1.35e+14\n",
      "----------------------------------------------------------------------\n",
      "  Actual trainable parameters: 851,968 (0.11%)\n",
      "  Starting training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2688' max='2688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2688/2688 20:49, Epoch 84/84]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.358700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.347300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.351900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.332400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>5.308300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.277100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>5.241200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>5.207900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>5.109300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.075900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>5.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>5.016500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4.980600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.906900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>4.881400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.852200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>4.795000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.782000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>4.738100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.710700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>4.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>4.647000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>4.652300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.603300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>4.582200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>4.578900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>4.557600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>4.514400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>4.529400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.521700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>4.492700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>4.475500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>4.479500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>4.427600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>4.459200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>4.402900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>4.412400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>4.413400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>4.402400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.380100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>4.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>4.393100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>4.364700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>4.351700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>4.356900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>4.363100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>4.359400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>4.331900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>4.331400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.349400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>4.353800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>4.321800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>4.313600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Training complete in 1250.1s\n",
      "======================================================================\n",
      "\n",
      "  --- Longitudinal Simulation for SSRI ---\n",
      "    Cycle 0: ARC-Easy = 24.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 1: ARC-Easy = 24.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 2: ARC-Easy = 6.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 3: ARC-Easy = 12.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 4: ARC-Easy = 8.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 5: ARC-Easy = 30.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 6: ARC-Easy = 4.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 7: ARC-Easy = 20.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 8: ARC-Easy = 18.0%\n",
      "\n",
      "[STEP 3.3] Processing NEUROSTEROID treatment\n",
      "  Using ISODOSE parameters (target FLOPs: 1.29e+14)\n",
      "  Reloading fresh pruned model...\n",
      "======================================================================\n",
      "LOADING PRE-PRUNED MODEL\n",
      "======================================================================\n",
      "  Model: oopere/pruned60-llama-3.2-1B\n",
      "  Dtype: torch.float16 (NO 4-bit quantization)\n",
      "  Device map: auto\n",
      "  Expected state: 60% MLP neurons removed (fragile knowledge)\n",
      "----------------------------------------------------------------------\n",
      "  Load time: 1.85s\n",
      "  Total parameters: 752,650,240\n",
      "  Trainable parameters: 752,650,240\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float16\n",
      "======================================================================\n",
      "  [SUCCESS] Pruned model loaded (fragile state confirmed)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "APPLYING NEUROSTEROID TREATMENT\n",
      "  [ISODOSE MODE]\n",
      "======================================================================\n",
      "  LoRA rank: 32\n",
      "  LoRA alpha: 64\n",
      "  Training epochs: 10\n",
      "  Learning rate: 3e-05\n",
      "  Dropout: 0.5\n",
      "  Target modules: ['q_proj', 'v_proj', 'gate_proj', 'up_proj']\n",
      "  Estimated FLOPs: 1.29e+14\n",
      "----------------------------------------------------------------------\n",
      "  Actual trainable parameters: 8,860,672 (1.16%)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Starting training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 02:47, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.628500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.619900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.395200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.175400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.133300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Training complete in 168.5s\n",
      "======================================================================\n",
      "\n",
      "  Evaluating POST-NEUROSTEROID performance:\n",
      "    Running ARC-Easy evaluation... Accuracy: 27.5%\n",
      "    Running LAMBADA evaluation... Accuracy: 11.0%\n",
      "    ARC-Easy: 27.5%\n",
      "    LAMBADA:  11.0%\n",
      "    Composite: 19.2%\n",
      "    Recovery from untreated: -2.0%\n",
      "    Efficiency (recovery/PetaFLOP): -15.52\n",
      "\n",
      "  Simulating ACUTE RELAPSE:\n",
      "    Applying acute relapse: 30% unstructured pruning\n",
      "    Pruned 241 linear layers\n",
      "  Evaluating POST-RELAPSE performance:\n",
      "    Running ARC-Easy evaluation... Accuracy: 24.0%\n",
      "    Running LAMBADA evaluation... Accuracy: 22.0%\n",
      "    ARC-Easy: 24.0%\n",
      "    LAMBADA:  22.0%\n",
      "    Composite: 23.0%\n",
      "    Relapse drop: -3.8%\n",
      "\n",
      "  Running LONGITUDINAL simulation for neurosteroid:\n",
      "======================================================================\n",
      "LOADING PRE-PRUNED MODEL\n",
      "======================================================================\n",
      "  Model: oopere/pruned60-llama-3.2-1B\n",
      "  Dtype: torch.float16 (NO 4-bit quantization)\n",
      "  Device map: auto\n",
      "  Expected state: 60% MLP neurons removed (fragile knowledge)\n",
      "----------------------------------------------------------------------\n",
      "  Load time: 1.65s\n",
      "  Total parameters: 752,650,240\n",
      "  Trainable parameters: 752,650,240\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float16\n",
      "======================================================================\n",
      "  [SUCCESS] Pruned model loaded (fragile state confirmed)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "APPLYING NEUROSTEROID TREATMENT\n",
      "  [ISODOSE MODE]\n",
      "======================================================================\n",
      "  LoRA rank: 32\n",
      "  LoRA alpha: 64\n",
      "  Training epochs: 10\n",
      "  Learning rate: 3e-05\n",
      "  Dropout: 0.5\n",
      "  Target modules: ['q_proj', 'v_proj', 'gate_proj', 'up_proj']\n",
      "  Estimated FLOPs: 1.29e+14\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Actual trainable parameters: 8,860,672 (1.16%)\n",
      "  Starting training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 02:47, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.628500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.619900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.395200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.175400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.133300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Training complete in 168.6s\n",
      "======================================================================\n",
      "\n",
      "  --- Longitudinal Simulation for NEUROSTEROID ---\n",
      "    Cycle 0: ARC-Easy = 28.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 1: ARC-Easy = 28.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 2: ARC-Easy = 4.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 3: ARC-Easy = 30.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 4: ARC-Easy = 28.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 5: ARC-Easy = 6.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 6: ARC-Easy = 18.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 7: ARC-Easy = 22.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 8: ARC-Easy = 18.0%\n",
      "\n",
      "######################################################################\n",
      "#  SEED 1 COMPLETE\n",
      "######################################################################\n",
      "\n",
      "  [SAVED] Seed 1 results saved to ./llm_treatment_results/seed_1_results.json\n",
      "\n",
      "********************************************************************************\n",
      "  STARTING SEED 3/3\n",
      "********************************************************************************\n",
      "\n",
      "######################################################################\n",
      "#  SEED 2 EXPERIMENT\n",
      "#  [ISODOSE MODE ACTIVE]\n",
      "######################################################################\n",
      "----------------------------------------------------------------------\n",
      "LOADING EVALUATION DATASETS\n",
      "----------------------------------------------------------------------\n",
      "  ARC-Easy: 200 samples loaded\n",
      "  LAMBADA: 100 samples loaded\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[STEP 1] Loading pruned model (untreated baseline)\n",
      "======================================================================\n",
      "LOADING PRE-PRUNED MODEL\n",
      "======================================================================\n",
      "  Model: oopere/pruned60-llama-3.2-1B\n",
      "  Dtype: torch.float16 (NO 4-bit quantization)\n",
      "  Device map: auto\n",
      "  Expected state: 60% MLP neurons removed (fragile knowledge)\n",
      "----------------------------------------------------------------------\n",
      "  Load time: 1.61s\n",
      "  Total parameters: 752,650,240\n",
      "  Trainable parameters: 752,650,240\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float16\n",
      "======================================================================\n",
      "  [SUCCESS] Pruned model loaded (fragile state confirmed)\n",
      "======================================================================\n",
      "\n",
      "[STEP 2] Evaluating UNTREATED (pruned) baseline\n",
      "----------------------------------------------------------------------\n",
      "    Running ARC-Easy evaluation... Accuracy: 22.5%\n",
      "    Running LAMBADA evaluation... Accuracy: 20.0%\n",
      "  UNTREATED BASELINE:\n",
      "    ARC-Easy: 22.5%\n",
      "    LAMBADA:  20.0%\n",
      "    Composite: 21.2%\n",
      "----------------------------------------------------------------------\n",
      "PREPARING FINE-TUNING DATASET (seed=2)\n",
      "----------------------------------------------------------------------\n",
      "  Dataset: databricks/databricks-dolly-15k\n",
      "  Subset size: 500 samples\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a7b352c320f941b2afdcb9a49351cb24"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91976bb3802b43098b6ee9f663f6d319"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Tokenized: max_length=512\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[STEP 3.1] Processing KETAMINE treatment\n",
      "  Using ISODOSE parameters (target FLOPs: 1.35e+14)\n",
      "  Reloading fresh pruned model...\n",
      "======================================================================\n",
      "LOADING PRE-PRUNED MODEL\n",
      "======================================================================\n",
      "  Model: oopere/pruned60-llama-3.2-1B\n",
      "  Dtype: torch.float16 (NO 4-bit quantization)\n",
      "  Device map: auto\n",
      "  Expected state: 60% MLP neurons removed (fragile knowledge)\n",
      "----------------------------------------------------------------------\n",
      "  Load time: 1.50s\n",
      "  Total parameters: 752,650,240\n",
      "  Trainable parameters: 752,650,240\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float16\n",
      "======================================================================\n",
      "  [SUCCESS] Pruned model loaded (fragile state confirmed)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "APPLYING KETAMINE TREATMENT\n",
      "  [ISODOSE MODE]\n",
      "======================================================================\n",
      "  LoRA rank: 64\n",
      "  LoRA alpha: 128\n",
      "  Training epochs: 3\n",
      "  Learning rate: 5e-05\n",
      "  Dropout: 0.05\n",
      "  Target modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
      "  Estimated FLOPs: 1.35e+14\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Actual trainable parameters: 29,989,888 (3.83%)\n",
      "  Starting training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.802700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Training complete in 61.7s\n",
      "======================================================================\n",
      "\n",
      "  Evaluating POST-KETAMINE performance:\n",
      "    Running ARC-Easy evaluation... Accuracy: 28.5%\n",
      "    Running LAMBADA evaluation... Accuracy: 13.0%\n",
      "    ARC-Easy: 28.5%\n",
      "    LAMBADA:  13.0%\n",
      "    Composite: 20.8%\n",
      "    Recovery from untreated: -0.5%\n",
      "    Efficiency (recovery/PetaFLOP): -3.70\n",
      "\n",
      "  Simulating ACUTE RELAPSE:\n",
      "    Applying acute relapse: 30% unstructured pruning\n",
      "    Pruned 337 linear layers\n",
      "  Evaluating POST-RELAPSE performance:\n",
      "    Running ARC-Easy evaluation... Accuracy: 23.0%\n",
      "    Running LAMBADA evaluation... Accuracy: 17.0%\n",
      "    ARC-Easy: 23.0%\n",
      "    LAMBADA:  17.0%\n",
      "    Composite: 20.0%\n",
      "    Relapse drop: 0.8%\n",
      "\n",
      "  Running LONGITUDINAL simulation for ketamine:\n",
      "======================================================================\n",
      "LOADING PRE-PRUNED MODEL\n",
      "======================================================================\n",
      "  Model: oopere/pruned60-llama-3.2-1B\n",
      "  Dtype: torch.float16 (NO 4-bit quantization)\n",
      "  Device map: auto\n",
      "  Expected state: 60% MLP neurons removed (fragile knowledge)\n",
      "----------------------------------------------------------------------\n",
      "  Load time: 1.63s\n",
      "  Total parameters: 752,650,240\n",
      "  Trainable parameters: 752,650,240\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float16\n",
      "======================================================================\n",
      "  [SUCCESS] Pruned model loaded (fragile state confirmed)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "APPLYING KETAMINE TREATMENT\n",
      "  [ISODOSE MODE]\n",
      "======================================================================\n",
      "  LoRA rank: 64\n",
      "  LoRA alpha: 128\n",
      "  Training epochs: 3\n",
      "  Learning rate: 5e-05\n",
      "  Dropout: 0.05\n",
      "  Target modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
      "  Estimated FLOPs: 1.35e+14\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Actual trainable parameters: 29,989,888 (3.83%)\n",
      "  Starting training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.802700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Training complete in 61.9s\n",
      "======================================================================\n",
      "\n",
      "  --- Longitudinal Simulation for KETAMINE ---\n",
      "    Cycle 0: ARC-Easy = 22.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 1: ARC-Easy = 16.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 2: ARC-Easy = 20.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 3: ARC-Easy = 18.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 4: ARC-Easy = 22.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 5: ARC-Easy = 8.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 6: ARC-Easy = 18.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 7: ARC-Easy = 20.0% -> Stress applied (10% prune) -> Maintenance (2 epochs)\n",
      "    Cycle 8: ARC-Easy = 16.0%\n",
      "\n",
      "[STEP 3.2] Processing SSRI treatment\n",
      "  Using ISODOSE parameters (target FLOPs: 1.35e+14)\n",
      "  Reloading fresh pruned model...\n",
      "======================================================================\n",
      "LOADING PRE-PRUNED MODEL\n",
      "======================================================================\n",
      "  Model: oopere/pruned60-llama-3.2-1B\n",
      "  Dtype: torch.float16 (NO 4-bit quantization)\n",
      "  Device map: auto\n",
      "  Expected state: 60% MLP neurons removed (fragile knowledge)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Load time: 1.68s\n",
      "  Total parameters: 752,650,240\n",
      "  Trainable parameters: 752,650,240\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float16\n",
      "======================================================================\n",
      "  [SUCCESS] Pruned model loaded (fragile state confirmed)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "APPLYING SSRI TREATMENT\n",
      "  [ISODOSE MODE]\n",
      "======================================================================\n",
      "  LoRA rank: 8\n",
      "  LoRA alpha: 16\n",
      "  Training epochs: 84\n",
      "  Learning rate: 1e-06\n",
      "  Dropout: 0.1\n",
      "  Target modules: ['q_proj', 'v_proj']\n",
      "  Estimated FLOPs: 1.35e+14\n",
      "----------------------------------------------------------------------\n",
      "  Actual trainable parameters: 851,968 (0.11%)\n",
      "  Starting training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2688' max='2688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2688/2688 20:49, Epoch 84/84]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.353600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.367100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.361300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>5.333700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>5.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>5.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>5.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.111600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>5.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>5.014800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4.970200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.929600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>4.900800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.849200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>4.805800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.795100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>4.747900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.718200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>4.696400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>4.683200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>4.639700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>4.584400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>4.557500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>4.585600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>4.559100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>4.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.504400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>4.503200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>4.474300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>4.462900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>4.447500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>4.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>4.412100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>4.452800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>4.401500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>4.377700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.381700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>4.386500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>4.379300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>4.338300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>4.360800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>4.360900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>4.374900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>4.336600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>4.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>4.355900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.325800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>4.356200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>4.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>4.341400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Training complete in 1250.2s\n",
      "======================================================================\n",
      "\n",
      "  Evaluating POST-SSRI performance:\n",
      "    Running ARC-Easy evaluation... Accuracy: 21.5%\n",
      "    Running LAMBADA evaluation... Accuracy: 17.0%\n",
      "    ARC-Easy: 21.5%\n",
      "    LAMBADA:  17.0%\n",
      "    Composite: 19.2%\n",
      "    Recovery from untreated: -2.0%\n",
      "    Efficiency (recovery/PetaFLOP): -14.78\n",
      "\n",
      "  Simulating ACUTE RELAPSE:\n",
      "    Applying acute relapse: 30% unstructured pruning\n",
      "    Pruned 177 linear layers\n",
      "  Evaluating POST-RELAPSE performance:\n",
      "    Running ARC-Easy evaluation... Accuracy: 32.0%\n",
      "    Running LAMBADA evaluation... Accuracy: 13.0%\n",
      "    ARC-Easy: 32.0%\n",
      "    LAMBADA:  13.0%\n",
      "    Composite: 22.5%\n",
      "    Relapse drop: -3.2%\n",
      "\n",
      "  Running LONGITUDINAL simulation for ssri:\n",
      "======================================================================\n",
      "LOADING PRE-PRUNED MODEL\n",
      "======================================================================\n",
      "  Model: oopere/pruned60-llama-3.2-1B\n",
      "  Dtype: torch.float16 (NO 4-bit quantization)\n",
      "  Device map: auto\n",
      "  Expected state: 60% MLP neurons removed (fragile knowledge)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Load time: 1.71s\n",
      "  Total parameters: 752,650,240\n",
      "  Trainable parameters: 752,650,240\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float16\n",
      "======================================================================\n",
      "  [SUCCESS] Pruned model loaded (fragile state confirmed)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "APPLYING SSRI TREATMENT\n",
      "  [ISODOSE MODE]\n",
      "======================================================================\n",
      "  LoRA rank: 8\n",
      "  LoRA alpha: 16\n",
      "  Training epochs: 84\n",
      "  Learning rate: 1e-06\n",
      "  Dropout: 0.1\n",
      "  Target modules: ['q_proj', 'v_proj']\n",
      "  Estimated FLOPs: 1.35e+14\n",
      "----------------------------------------------------------------------\n",
      "  Actual trainable parameters: 851,968 (0.11%)\n",
      "  Starting training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2688' max='2688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2688/2688 20:53, Epoch 84/84]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.353600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.367100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.361300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>5.333700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>5.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>5.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>5.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.111700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>5.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>5.014700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4.970200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.929700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>4.900800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.849300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>4.805800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.795100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>4.747900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.718200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>4.696400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>4.683100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>4.639600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.652500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>4.584400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>4.557500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>4.585500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>4.559100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>4.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.504400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>4.503200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>4.474400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>4.462800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>4.447600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>4.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>4.412100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>4.452900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>4.401500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>4.377600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.381800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>4.386500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>4.379200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>4.338300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>4.360800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>4.360900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>4.374900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>4.336500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>4.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>4.355900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.325800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>4.356200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>4.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>4.341400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Training complete in 1254.4s\n",
      "======================================================================\n",
      "\n",
      "  --- Longitudinal Simulation for SSRI ---\n",
      "    Cycle 0: ARC-Easy = 20.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 1: ARC-Easy = 24.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 2: ARC-Easy = 4.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 3: ARC-Easy = 12.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 4: ARC-Easy = 8.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 5: ARC-Easy = 30.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 6: ARC-Easy = 4.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 7: ARC-Easy = 26.0% -> Stress applied (10% prune) -> Maintenance (5 epochs)\n",
      "    Cycle 8: ARC-Easy = 18.0%\n",
      "\n",
      "[STEP 3.3] Processing NEUROSTEROID treatment\n",
      "  Using ISODOSE parameters (target FLOPs: 1.29e+14)\n",
      "  Reloading fresh pruned model...\n",
      "======================================================================\n",
      "LOADING PRE-PRUNED MODEL\n",
      "======================================================================\n",
      "  Model: oopere/pruned60-llama-3.2-1B\n",
      "  Dtype: torch.float16 (NO 4-bit quantization)\n",
      "  Device map: auto\n",
      "  Expected state: 60% MLP neurons removed (fragile knowledge)\n",
      "----------------------------------------------------------------------\n",
      "  Load time: 1.67s\n",
      "  Total parameters: 752,650,240\n",
      "  Trainable parameters: 752,650,240\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float16\n",
      "======================================================================\n",
      "  [SUCCESS] Pruned model loaded (fragile state confirmed)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "APPLYING NEUROSTEROID TREATMENT\n",
      "  [ISODOSE MODE]\n",
      "======================================================================\n",
      "  LoRA rank: 32\n",
      "  LoRA alpha: 64\n",
      "  Training epochs: 10\n",
      "  Learning rate: 3e-05\n",
      "  Dropout: 0.5\n",
      "  Target modules: ['q_proj', 'v_proj', 'gate_proj', 'up_proj']\n",
      "  Estimated FLOPs: 1.29e+14\n",
      "----------------------------------------------------------------------\n",
      "  Actual trainable parameters: 8,860,672 (1.16%)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Starting training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 02:47, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.614200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.582800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.372500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.263300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.168400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.122000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Training complete in 168.3s\n",
      "======================================================================\n",
      "\n",
      "  Evaluating POST-NEUROSTEROID performance:\n",
      "    Running ARC-Easy evaluation... Accuracy: 28.0%\n",
      "    Running LAMBADA evaluation... Accuracy: 12.0%\n",
      "    ARC-Easy: 28.0%\n",
      "    LAMBADA:  12.0%\n",
      "    Composite: 20.0%\n",
      "    Recovery from untreated: -1.2%\n",
      "    Efficiency (recovery/PetaFLOP): -9.70\n",
      "\n",
      "  Simulating ACUTE RELAPSE:\n",
      "    Applying acute relapse: 30% unstructured pruning\n",
      "    Pruned 241 linear layers\n",
      "  Evaluating POST-RELAPSE performance:\n",
      "    Running ARC-Easy evaluation... Accuracy: 25.0%\n",
      "    Running LAMBADA evaluation... Accuracy: 17.0%\n",
      "    ARC-Easy: 25.0%\n",
      "    LAMBADA:  17.0%\n",
      "    Composite: 21.0%\n",
      "    Relapse drop: -1.0%\n",
      "\n",
      "  Running LONGITUDINAL simulation for neurosteroid:\n",
      "======================================================================\n",
      "LOADING PRE-PRUNED MODEL\n",
      "======================================================================\n",
      "  Model: oopere/pruned60-llama-3.2-1B\n",
      "  Dtype: torch.float16 (NO 4-bit quantization)\n",
      "  Device map: auto\n",
      "  Expected state: 60% MLP neurons removed (fragile knowledge)\n",
      "----------------------------------------------------------------------\n",
      "  Load time: 1.61s\n",
      "  Total parameters: 752,650,240\n",
      "  Trainable parameters: 752,650,240\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float16\n",
      "======================================================================\n",
      "  [SUCCESS] Pruned model loaded (fragile state confirmed)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "APPLYING NEUROSTEROID TREATMENT\n",
      "  [ISODOSE MODE]\n",
      "======================================================================\n",
      "  LoRA rank: 32\n",
      "  LoRA alpha: 64\n",
      "  Training epochs: 10\n",
      "  Learning rate: 3e-05\n",
      "  Dropout: 0.5\n",
      "  Target modules: ['q_proj', 'v_proj', 'gate_proj', 'up_proj']\n",
      "  Estimated FLOPs: 1.29e+14\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Actual trainable parameters: 8,860,672 (1.16%)\n",
      "  Starting training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 02:48, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.614100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.582800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.372500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.263400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.168500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.122000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Training complete in 169.2s\n",
      "======================================================================\n",
      "\n",
      "  --- Longitudinal Simulation for NEUROSTEROID ---\n",
      "    Cycle 0: ARC-Easy = 24.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 1: ARC-Easy = 30.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 2: ARC-Easy = 2.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 3: ARC-Easy = 30.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 4: ARC-Easy = 26.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 5: ARC-Easy = 8.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 6: ARC-Easy = 20.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 7: ARC-Easy = 24.0% -> Stress applied (10% prune) -> Maintenance (3 epochs)\n",
      "    Cycle 8: ARC-Easy = 18.0%\n",
      "\n",
      "######################################################################\n",
      "#  SEED 2 COMPLETE\n",
      "######################################################################\n",
      "\n",
      "  [SAVED] Seed 2 results saved to ./llm_treatment_results/seed_2_results.json\n",
      "\n",
      "================================================================================\n",
      "AGGREGATING RESULTS ACROSS SEEDS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "================================================================================\n",
      "  FINAL AGGREGATED RESULTS\n",
      "  3 seeds completed\n",
      "  [ISODOSE COMPARISON MODE]\n",
      "================================================================================\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "  ISODOSE CONFIGURATION\n",
      "  (All treatments calibrated to equivalent computational cost)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Treatment         Rank   Epochs  Dropout          FLOPs\n",
      "  -------------------------------------------------------\n",
      "  Ketamine            64        3     0.05       1.35e+14\n",
      "  Ssri                 8       84     0.10       1.35e+14\n",
      "  Neurosteroid        32       10     0.50       1.29e+14\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "  TABLE 1: POST-TREATMENT PERFORMANCE\n",
      "  (Mean ± Std across seeds, percentages)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Treatment                   ARC-Easy         LAMBADA       Composite\n",
      "  -----------------------------------------------------------------\n",
      "  Untreated (pruned)          22.5±0.0        20.0±0.0        21.2±0.0\n",
      "  Ketamine-like               32.3±2.7        14.3±1.2        23.3±1.9\n",
      "  SSRI-like                   22.3±0.6        17.0±0.8        19.7±0.4\n",
      "  Neurosteroid-like           28.5±1.1        11.3±0.5        19.9±0.5\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "  TABLE 2: TREATMENT EFFECTS\n",
      "  (Recovery = improvement over untreated, Relapse Drop = loss after acute stress)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Treatment                      Recovery       Relapse Drop    Net Retained\n",
      "  ----------------------------------------------------------------------\n",
      "  Ketamine-like                 +2.1±1.9%           1.5±0.7%           +0.6%\n",
      "  SSRI-like                     -1.6±0.4%          -2.5±0.7%           +0.9%\n",
      "  Neurosteroid-like             -1.3±0.5%          -0.9±2.3%           -0.4%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "  TABLE 3: ISODOSE EFFICIENCY COMPARISON\n",
      "  (Recovery per unit computational cost - higher is more efficient)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Treatment                Recovery          FLOPs     Efficiency*\n",
      "  ------------------------------------------------------------\n",
      "  Ketamine-like               +2.1%       1.35e+14     15.40±13.69\n",
      "  SSRI-like                   -1.6%       1.35e+14     -11.70±3.14\n",
      "  Neurosteroid-like           -1.3%       1.29e+14     -10.35±3.99\n",
      "\n",
      "  * Efficiency = Recovery (%) / PetaFLOPs\n",
      "\n",
      "  ISODOSE RANKING (by efficiency):\n",
      "    1. Ketamine-like: 15.40\n",
      "    2. Neurosteroid-like: -10.35\n",
      "    3. SSRI-like: -11.70\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "  TABLE 4: LONGITUDINAL TRAJECTORY (ARC-Easy % over stress cycles)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Cycle              Ketamine               Ssri       Neurosteroid\n",
      "  --------------------------------------------------------------\n",
      "  0           27.3±  3.8%    21.3±  1.9%    25.3±  1.9%\n",
      "  1           19.3±  2.5%    24.0±  0.0%    30.7±  2.5%\n",
      "  2           16.7±  2.5%     5.3±  0.9%     4.7±  2.5%\n",
      "  3           19.3±  3.4%    12.0±  0.0%    29.3±  0.9%\n",
      "  4           22.0±  3.3%     8.0±  0.0%    28.0±  1.6%\n",
      "  5            7.3±  0.9%    30.0±  0.0%     7.3±  0.9%\n",
      "  6           16.7±  1.9%     4.0±  0.0%    18.7±  0.9%\n",
      "  7           20.0±  0.0%    24.7±  3.4%    22.0±  1.6%\n",
      "  8           17.3±  1.9%    18.0±  0.0%    18.0±  0.0%\n",
      "\n",
      "  --------------------------------------------------------------\n",
      "  LONGITUDINAL SUMMARY:\n",
      "    Ketamine-like       \n",
      "      Initial (cycle 0): 27.3%\n",
      "      Final (cycle 8):   17.3%\n",
      "      Total drop:        10.0±3.3%\n",
      "    SSRI-like           \n",
      "      Initial (cycle 0): 21.3%\n",
      "      Final (cycle 8):   18.0%\n",
      "      Total drop:        3.3±1.9%\n",
      "    Neurosteroid-like   \n",
      "      Initial (cycle 0): 25.3%\n",
      "      Final (cycle 8):   18.0%\n",
      "      Total drop:        7.3±1.9%\n",
      "\n",
      "================================================================================\n",
      "  INTERPRETATION GUIDE\n",
      "================================================================================\n",
      "\n",
      "    ISODOSE COMPARISON INTERPRETATION:\n",
      "    \n",
      "    The isodose framework enables FAIR comparison by equalizing computational cost.\n",
      "    This answers: \"Given the SAME therapeutic effort, which mechanism is superior?\"\n",
      "    \n",
      "    KEY ISODOSE METRICS:\n",
      "    1. Recovery (%) - Improvement over untreated baseline\n",
      "    2. Efficiency - Recovery per PetaFLOP of computation\n",
      "    3. Relapse Drop - Vulnerability to acute stress\n",
      "    4. Longitudinal Stability - Durability under chronic stress\n",
      "    \n",
      "    WHAT ISODOSE REVEALS:\n",
      "    - If a treatment with LOW structural capacity (SSRI) matches HIGH capacity (ketamine)\n",
      "      at equal FLOPs, the MECHANISM itself may be more efficient\n",
      "    - If HIGH capacity (ketamine) still wins at isodose, structural regrowth may be\n",
      "      fundamentally more effective for this type of damage\n",
      "    \n",
      "    EXPECTED PATTERNS AT ISODOSE:\n",
      "    \n",
      "    KETAMINE-LIKE (r=64, short duration):\n",
      "    - High efficiency IF structural regrowth is critical for knowledge recovery\n",
      "    - May show diminishing returns if extra capacity isn't fully utilized\n",
      "    \n",
      "    SSRI-LIKE (r=8, extended duration):\n",
      "    - At isodose, gets MORE epochs than default (scaled up to match FLOPs)\n",
      "    - Tests whether gradual learning can compensate for low capacity\n",
      "    - May show improved efficiency if default was \"undertreated\"\n",
      "    \n",
      "    NEUROSTEROID-LIKE (r=32, high dropout):\n",
      "    - At isodose, dropout regularization applied over calibrated epochs\n",
      "    - Tests whether inhibition-based stabilization is cost-effective\n",
      "    \n",
      "    COMPARING DEFAULT vs ISODOSE:\n",
      "    - If rankings CHANGE between modes, the default comparison was unfair\n",
      "    - If rankings PERSIST, the mechanism difference is robust to dosing\n",
      "    \n",
      "================================================================================\n",
      "\n",
      "[SAVED] Aggregated results saved to ./llm_treatment_results/aggregated_results.json\n",
      "[SAVED] Isodose calibration saved to ./llm_treatment_results/isodose_calibration.json\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "MULTI-MECHANISM RECOVERY FROM PRUNING-INDUCED FRAGILITY IN LLMs\n",
    "WITH ISODOSE TREATMENT COMPARISON\n",
    "================================================================================\n",
    "\n",
    "Adapts the antidepressant comparison experiment to real Large Language Models\n",
    "using structured width pruning as the \"disease\" model.\n",
    "\n",
    "NEW FEATURE: ISODOSE COMPARISON\n",
    "- Equalizes treatments based on computational cost (training FLOPs)\n",
    "- Enables fair comparison of treatment efficacy per unit of \"therapeutic effort\"\n",
    "- Analogous to comparing drugs at equivalent doses rather than arbitrary doses\n",
    "\n",
    "MODIFICATIONS APPLIED:\n",
    "- Removed bitsandbytes/4-bit quantization (Colab compatibility)\n",
    "- Uses fp16 with device_map=\"auto\"\n",
    "- Starts from pre-pruned model oopere/pruned60-llama-3.2-1B\n",
    "- Implements proxy evaluations (ARC-Easy, LAMBADA)\n",
    "- Includes acute relapse via unstructured pruning\n",
    "- Simplified longitudinal stress cycles\n",
    "- ADDED: Isodose calibration and comparison framework\n",
    "\n",
    "Based on:\n",
    "- Pere Martra's pruning course: https://github.com/peremartra/Large-Language-Model-Notebooks-Course\n",
    "- Llama-pruning tools: https://github.com/MedITSolutionsKurman/llama-pruning\n",
    "- Paper: \"Fragile Knowledge, Robust Instruction-Following\" (arXiv 2512.22671)\n",
    "\n",
    "Treatments tested:\n",
    "1. KETAMINE-LIKE: High-rank LoRA (structural regrowth) + short aggressive fine-tuning\n",
    "2. SSRI-LIKE: Low-rank LoRA + prolonged gradual stabilization\n",
    "3. NEUROSTEROID-LIKE: High dropout (tonic inhibition) + consolidation training\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 1: INSTALLATIONS (Run first in Colab)\n",
    "# ============================================================================\n",
    "# !pip install -q transformers accelerate peft datasets torch --extra-index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install -q sentencepiece einops\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 2: IMPORTS AND CONFIGURATION\n",
    "# ============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import prune\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"\n",
    "    Configuration for the LLM pruning recovery experiment.\n",
    "\n",
    "    DESIGN NOTES:\n",
    "    - pruned_model: Using oopere/pruned60-llama-3.2-1B which has 60% MLP neurons\n",
    "      removed, representing severe \"fragility\" state analogous to depression\n",
    "    - No 4-bit quantization: Avoids bitsandbytes issues on Colab\n",
    "    - eval_samples: Reduced for speed while maintaining statistical validity\n",
    "\n",
    "    ISODOSE PARAMETERS:\n",
    "    - target_flops: The computational budget all treatments must match\n",
    "    - isodose_mode: Whether to run in isodose mode or default mode\n",
    "    \"\"\"\n",
    "\n",
    "    # Model settings - using pre-pruned model to avoid pruning overhead\n",
    "    # This model has 60% MLP pruning = strong knowledge fragility\n",
    "    pruned_model: str = \"oopere/pruned60-llama-3.2-1B\"\n",
    "\n",
    "    # CRITICAL: No 4-bit quantization - uses fp16 instead\n",
    "    # This avoids bitsandbytes package issues on Colab\n",
    "    use_4bit: bool = False\n",
    "\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Experiment settings\n",
    "    n_seeds: int = 3\n",
    "    output_dir: str = \"./llm_treatment_results\"\n",
    "\n",
    "    # Evaluation settings - using subset for speed\n",
    "    eval_samples: int = 200\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # ISODOSE CONFIGURATION\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Enable isodose comparison mode\n",
    "    isodose_mode: bool = True\n",
    "\n",
    "    # Target computational budget (in estimated FLOPs)\n",
    "    # Will be calibrated based on ketamine treatment as reference\n",
    "    # Set to None for automatic calibration\n",
    "    target_flops: Optional[float] = None\n",
    "\n",
    "    # Isodose calibration reference treatment\n",
    "    isodose_reference: str = \"ketamine\"\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # DEFAULT TREATMENT PARAMETERS (used when isodose_mode=False)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # KETAMINE-LIKE\n",
    "    ketamine_lora_rank: int = 64\n",
    "    ketamine_lora_alpha: int = 128\n",
    "    ketamine_epochs: int = 3\n",
    "    ketamine_lr: float = 5e-5\n",
    "    ketamine_target_modules: List[str] = field(\n",
    "        default_factory=lambda: [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
    "                                  \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    )\n",
    "    ketamine_dropout: float = 0.05\n",
    "\n",
    "    # SSRI-LIKE\n",
    "    ssri_lora_rank: int = 8\n",
    "    ssri_lora_alpha: int = 16\n",
    "    ssri_epochs: int = 15\n",
    "    ssri_lr: float = 1e-6\n",
    "    ssri_dropout: float = 0.1\n",
    "\n",
    "    # NEUROSTEROID-LIKE\n",
    "    neuro_lora_rank: int = 32\n",
    "    neuro_lora_alpha: int = 64\n",
    "    neuro_epochs: int = 5\n",
    "    neuro_lr: float = 3e-5\n",
    "    neuro_dropout: float = 0.5\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # LONGITUDINAL SIMULATION PARAMETERS\n",
    "    # -------------------------------------------------------------------------\n",
    "    longitudinal_cycles: int = 8\n",
    "    additional_prune_per_cycle: float = 0.10\n",
    "\n",
    "    # Maintenance training epochs per treatment\n",
    "    ketamine_maintenance_epochs: int = 2\n",
    "    ssri_maintenance_epochs: int = 5\n",
    "    neuro_maintenance_epochs: int = 3\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # ACUTE RELAPSE SIMULATION\n",
    "    # -------------------------------------------------------------------------\n",
    "    acute_relapse_prune_amount: float = 0.30\n",
    "\n",
    "    # Dataset settings\n",
    "    finetune_dataset: str = \"databricks/databricks-dolly-15k\"\n",
    "    finetune_subset_size: int = 500\n",
    "    max_seq_length: int = 512\n",
    "\n",
    "\n",
    "# Initialize global config\n",
    "CONFIG = ExperimentConfig()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 3: ISODOSE CALCULATION FRAMEWORK\n",
    "# ============================================================================\n",
    "@dataclass\n",
    "class TreatmentDose:\n",
    "    \"\"\"\n",
    "    Represents a treatment's computational \"dose\" and parameters.\n",
    "\n",
    "    ISODOSE CONCEPT:\n",
    "    In pharmacology, isodose comparison means comparing drugs at doses that\n",
    "    produce equivalent biological effects or have equivalent potency.\n",
    "\n",
    "    For LLM treatments, we define \"dose\" as computational cost:\n",
    "    - Training FLOPs = f(trainable_params, epochs, dataset_size, seq_length)\n",
    "    - This captures both the \"intensity\" (params) and \"duration\" (epochs) of treatment\n",
    "\n",
    "    This allows fair comparison: \"Given the same computational budget,\n",
    "    which treatment mechanism produces better recovery?\"\n",
    "    \"\"\"\n",
    "    treatment_name: str\n",
    "    lora_rank: int\n",
    "    lora_alpha: int\n",
    "    epochs: int\n",
    "    lr: float\n",
    "    dropout: float\n",
    "    target_modules: List[str]\n",
    "    trainable_params: int = 0\n",
    "    estimated_flops: float = 0.0\n",
    "\n",
    "    def compute_flops(self, dataset_size: int, seq_length: int,\n",
    "                      hidden_size: int = 2048, num_layers: int = 16):\n",
    "        \"\"\"\n",
    "        Estimate training FLOPs for this treatment configuration.\n",
    "\n",
    "        FLOP ESTIMATION MODEL:\n",
    "        - Forward pass: ~2 * params * seq_length * batch_size per sample\n",
    "        - Backward pass: ~4 * params * seq_length * batch_size per sample\n",
    "        - Total per epoch: ~6 * trainable_params * dataset_size * seq_length\n",
    "        - Total training: epochs * per_epoch_flops\n",
    "\n",
    "        This is a simplified model but captures the key scaling factors.\n",
    "\n",
    "        Args:\n",
    "            dataset_size: Number of training samples\n",
    "            seq_length: Sequence length\n",
    "            hidden_size: Model hidden dimension\n",
    "            num_layers: Number of transformer layers\n",
    "\n",
    "        Returns:\n",
    "            Estimated total training FLOPs\n",
    "        \"\"\"\n",
    "        # Estimate trainable parameters based on LoRA config\n",
    "        # LoRA adds rank * (in_features + out_features) params per adapted layer\n",
    "        params_per_module = 2 * self.lora_rank * hidden_size\n",
    "        num_adapted_modules = len(self.target_modules) * num_layers\n",
    "        self.trainable_params = params_per_module * num_adapted_modules\n",
    "\n",
    "        # FLOPs per training step (forward + backward)\n",
    "        flops_per_sample = 6 * self.trainable_params * seq_length\n",
    "\n",
    "        # Total FLOPs\n",
    "        self.estimated_flops = flops_per_sample * dataset_size * self.epochs\n",
    "\n",
    "        return self.estimated_flops\n",
    "\n",
    "\n",
    "def calculate_isodose_parameters(reference_dose: TreatmentDose,\n",
    "                                  target_treatment: str,\n",
    "                                  config: ExperimentConfig) -> TreatmentDose:\n",
    "    \"\"\"\n",
    "    Calculate parameters for a treatment to match the reference dose's FLOPs.\n",
    "\n",
    "    ISODOSE CALIBRATION STRATEGY:\n",
    "    Each treatment has characteristic features we want to preserve:\n",
    "    - Ketamine: High rank, short duration, aggressive LR\n",
    "    - SSRI: Low rank, long duration, gentle LR\n",
    "    - Neurosteroid: Moderate rank, high dropout\n",
    "\n",
    "    To achieve isodose, we adjust EPOCHS while preserving characteristic features.\n",
    "\n",
    "    The key insight: FLOPs scale linearly with epochs, so we can solve for\n",
    "    the number of epochs needed to match target FLOPs:\n",
    "\n",
    "    target_epochs = target_flops / (flops_per_epoch)\n",
    "\n",
    "    Args:\n",
    "        reference_dose: The reference treatment dose (typically ketamine)\n",
    "        target_treatment: Name of treatment to calibrate\n",
    "        config: Experiment configuration\n",
    "\n",
    "    Returns:\n",
    "        TreatmentDose with calibrated parameters\n",
    "    \"\"\"\n",
    "    target_flops = reference_dose.estimated_flops\n",
    "\n",
    "    print(f\"\\n    Calibrating {target_treatment.upper()} to match {reference_dose.estimated_flops:.2e} FLOPs\")\n",
    "\n",
    "    if target_treatment == 'ketamine':\n",
    "        # Ketamine is typically the reference, return as-is\n",
    "        return TreatmentDose(\n",
    "            treatment_name='ketamine',\n",
    "            lora_rank=config.ketamine_lora_rank,\n",
    "            lora_alpha=config.ketamine_lora_alpha,\n",
    "            epochs=config.ketamine_epochs,\n",
    "            lr=config.ketamine_lr,\n",
    "            dropout=config.ketamine_dropout,\n",
    "            target_modules=config.ketamine_target_modules,\n",
    "            trainable_params=reference_dose.trainable_params,\n",
    "            estimated_flops=reference_dose.estimated_flops\n",
    "        )\n",
    "\n",
    "    elif target_treatment == 'ssri':\n",
    "        # SSRI: Preserve low rank and gentle LR, adjust epochs\n",
    "        # Characteristic: Low rank (8), limited target modules\n",
    "        ssri_modules = [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "        # Create temporary dose to calculate FLOPs per epoch\n",
    "        temp_dose = TreatmentDose(\n",
    "            treatment_name='ssri',\n",
    "            lora_rank=config.ssri_lora_rank,\n",
    "            lora_alpha=config.ssri_lora_alpha,\n",
    "            epochs=1,  # Calculate for 1 epoch\n",
    "            lr=config.ssri_lr,\n",
    "            dropout=config.ssri_dropout,\n",
    "            target_modules=ssri_modules\n",
    "        )\n",
    "        flops_per_epoch = temp_dose.compute_flops(\n",
    "            config.finetune_subset_size,\n",
    "            config.max_seq_length\n",
    "        )\n",
    "\n",
    "        # Calculate epochs needed to match target FLOPs\n",
    "        isodose_epochs = max(1, int(round(target_flops / flops_per_epoch)))\n",
    "\n",
    "        # Create calibrated dose\n",
    "        calibrated = TreatmentDose(\n",
    "            treatment_name='ssri',\n",
    "            lora_rank=config.ssri_lora_rank,\n",
    "            lora_alpha=config.ssri_lora_alpha,\n",
    "            epochs=isodose_epochs,\n",
    "            lr=config.ssri_lr,\n",
    "            dropout=config.ssri_dropout,\n",
    "            target_modules=ssri_modules,\n",
    "            trainable_params=temp_dose.trainable_params,\n",
    "            estimated_flops=flops_per_epoch * isodose_epochs\n",
    "        )\n",
    "\n",
    "        print(f\"      Original epochs: {config.ssri_epochs}\")\n",
    "        print(f\"      Isodose epochs: {isodose_epochs}\")\n",
    "        print(f\"      FLOP ratio: {calibrated.estimated_flops / target_flops:.2f}x\")\n",
    "\n",
    "        return calibrated\n",
    "\n",
    "    elif target_treatment == 'neurosteroid':\n",
    "        # Neurosteroid: Preserve high dropout and moderate rank, adjust epochs\n",
    "        neuro_modules = [\"q_proj\", \"v_proj\", \"gate_proj\", \"up_proj\"]\n",
    "\n",
    "        temp_dose = TreatmentDose(\n",
    "            treatment_name='neurosteroid',\n",
    "            lora_rank=config.neuro_lora_rank,\n",
    "            lora_alpha=config.neuro_lora_alpha,\n",
    "            epochs=1,\n",
    "            lr=config.neuro_lr,\n",
    "            dropout=config.neuro_dropout,\n",
    "            target_modules=neuro_modules\n",
    "        )\n",
    "        flops_per_epoch = temp_dose.compute_flops(\n",
    "            config.finetune_subset_size,\n",
    "            config.max_seq_length\n",
    "        )\n",
    "\n",
    "        isodose_epochs = max(1, int(round(target_flops / flops_per_epoch)))\n",
    "\n",
    "        calibrated = TreatmentDose(\n",
    "            treatment_name='neurosteroid',\n",
    "            lora_rank=config.neuro_lora_rank,\n",
    "            lora_alpha=config.neuro_lora_alpha,\n",
    "            epochs=isodose_epochs,\n",
    "            lr=config.neuro_lr,\n",
    "            dropout=config.neuro_dropout,\n",
    "            target_modules=neuro_modules,\n",
    "            trainable_params=temp_dose.trainable_params,\n",
    "            estimated_flops=flops_per_epoch * isodose_epochs\n",
    "        )\n",
    "\n",
    "        print(f\"      Original epochs: {config.neuro_epochs}\")\n",
    "        print(f\"      Isodose epochs: {isodose_epochs}\")\n",
    "        print(f\"      FLOP ratio: {calibrated.estimated_flops / target_flops:.2f}x\")\n",
    "\n",
    "        return calibrated\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown treatment: {target_treatment}\")\n",
    "\n",
    "\n",
    "def calibrate_all_treatments(config: ExperimentConfig) -> Dict[str, TreatmentDose]:\n",
    "    \"\"\"\n",
    "    Calibrate all treatments to isodose based on reference treatment.\n",
    "\n",
    "    CALIBRATION PROCESS:\n",
    "    1. Calculate FLOPs for reference treatment (ketamine by default)\n",
    "    2. For each other treatment, adjust epochs to match reference FLOPs\n",
    "    3. Return dictionary of calibrated treatment doses\n",
    "\n",
    "    This ensures all treatments receive the same \"computational dose\"\n",
    "    while preserving their characteristic mechanisms.\n",
    "\n",
    "    Args:\n",
    "        config: Experiment configuration\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping treatment names to calibrated TreatmentDose objects\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ISODOSE CALIBRATION\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"  Reference treatment: {config.isodose_reference.upper()}\")\n",
    "    print(f\"  Calibration strategy: Adjust epochs to match reference FLOPs\")\n",
    "    print(f\"  Preserved characteristics:\")\n",
    "    print(f\"    - Ketamine: High rank ({config.ketamine_lora_rank}), many modules\")\n",
    "    print(f\"    - SSRI: Low rank ({config.ssri_lora_rank}), gentle LR ({config.ssri_lr})\")\n",
    "    print(f\"    - Neurosteroid: High dropout ({config.neuro_dropout})\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # Calculate reference dose (ketamine)\n",
    "    reference = TreatmentDose(\n",
    "        treatment_name='ketamine',\n",
    "        lora_rank=config.ketamine_lora_rank,\n",
    "        lora_alpha=config.ketamine_lora_alpha,\n",
    "        epochs=config.ketamine_epochs,\n",
    "        lr=config.ketamine_lr,\n",
    "        dropout=config.ketamine_dropout,\n",
    "        target_modules=config.ketamine_target_modules\n",
    "    )\n",
    "    reference.compute_flops(config.finetune_subset_size, config.max_seq_length)\n",
    "\n",
    "    print(f\"\\n  REFERENCE: {config.isodose_reference.upper()}\")\n",
    "    print(f\"    LoRA rank: {reference.lora_rank}\")\n",
    "    print(f\"    Epochs: {reference.epochs}\")\n",
    "    print(f\"    Trainable params: {reference.trainable_params:,}\")\n",
    "    print(f\"    Estimated FLOPs: {reference.estimated_flops:.2e}\")\n",
    "\n",
    "    # Calibrate all treatments\n",
    "    calibrated_doses = {\n",
    "        'ketamine': reference\n",
    "    }\n",
    "\n",
    "    for treatment in ['ssri', 'neurosteroid']:\n",
    "        calibrated_doses[treatment] = calculate_isodose_parameters(\n",
    "            reference, treatment, config\n",
    "        )\n",
    "\n",
    "    # Print summary table\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"  ISODOSE CALIBRATION SUMMARY\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"\\n  {'Treatment':<15} {'Rank':>6} {'Epochs':>8} {'Params':>12} {'FLOPs':>14} {'Ratio':>8}\")\n",
    "    print(\"  \" + \"-\" * 65)\n",
    "\n",
    "    for name, dose in calibrated_doses.items():\n",
    "        ratio = dose.estimated_flops / reference.estimated_flops\n",
    "        print(f\"  {name.capitalize():<15} {dose.lora_rank:>6} {dose.epochs:>8} \"\n",
    "              f\"{dose.trainable_params:>12,} {dose.estimated_flops:>14.2e} {ratio:>7.2f}x\")\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    return calibrated_doses\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 4: MODEL LOADING (fp16, NO 4-bit quantization)\n",
    "# ============================================================================\n",
    "def load_pruned_model(config: ExperimentConfig = None) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"\n",
    "    Load the pre-pruned Llama model in fp16.\n",
    "\n",
    "    IMPLEMENTATION NOTES:\n",
    "    - Uses oopere/pruned60-llama-3.2-1B: A Llama-3.2-1B model with 60% of MLP\n",
    "      neurons removed via structured width pruning\n",
    "    - This creates a \"fragile\" model where factual knowledge is impaired but\n",
    "      instruction-following capabilities are preserved\n",
    "    - fp16 + device_map=\"auto\" fits comfortably on Colab T4 (16GB) or A100\n",
    "    - NO bitsandbytes dependency - avoids Colab CUDA wheel issues\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = CONFIG\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"LOADING PRE-PRUNED MODEL\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"  Model: {config.pruned_model}\")\n",
    "    print(f\"  Dtype: torch.float16 (NO 4-bit quantization)\")\n",
    "    print(f\"  Device map: auto\")\n",
    "    print(f\"  Expected state: 60% MLP neurons removed (fragile knowledge)\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.pruned_model,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config.pruned_model,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"  [INFO] Set pad_token = eos_token\")\n",
    "\n",
    "    load_time = time.time() - start_time\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"  Load time: {load_time:.2f}s\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"  Model device: {next(model.parameters()).device}\")\n",
    "    print(f\"  Model dtype: {next(model.parameters()).dtype}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"  [SUCCESS] Pruned model loaded (fragile state confirmed)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 5: EVALUATION FUNCTIONS (Proxy metrics for fragility)\n",
    "# ============================================================================\n",
    "def load_evaluation_datasets(config: ExperimentConfig = None) -> Dict:\n",
    "    \"\"\"Load evaluation datasets for measuring knowledge fragility.\"\"\"\n",
    "    if config is None:\n",
    "        config = CONFIG\n",
    "\n",
    "    print(\"-\" * 70)\n",
    "    print(\"LOADING EVALUATION DATASETS\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    datasets = {}\n",
    "\n",
    "    try:\n",
    "        arc = load_dataset(\n",
    "            \"allenai/ai2_arc\",\n",
    "            \"ARC-Easy\",\n",
    "            split=\"test\"\n",
    "        ).shuffle(seed=42).select(range(config.eval_samples))\n",
    "        datasets['arc'] = arc\n",
    "        print(f\"  ARC-Easy: {len(arc)} samples loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [WARNING] Failed to load ARC-Easy: {e}\")\n",
    "        datasets['arc'] = None\n",
    "\n",
    "    try:\n",
    "        lambada = load_dataset(\n",
    "            \"lambada\",\n",
    "            split=\"test\"\n",
    "        ).shuffle(seed=42).select(range(config.eval_samples // 2))\n",
    "        datasets['lambada'] = lambada\n",
    "        print(f\"  LAMBADA: {len(lambada)} samples loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [WARNING] Failed to load LAMBADA: {e}\")\n",
    "        datasets['lambada'] = None\n",
    "\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def evaluate_arc_easy(model, tokenizer, dataset, max_samples: int = None) -> Dict:\n",
    "    \"\"\"Evaluate model on ARC-Easy multiple choice questions.\"\"\"\n",
    "    if dataset is None:\n",
    "        return {'accuracy': 0.0, 'error': 'Dataset not loaded'}\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results = []\n",
    "\n",
    "    samples = dataset if max_samples is None else dataset.select(range(min(max_samples, len(dataset))))\n",
    "\n",
    "    for example in samples:\n",
    "        question = example['question']\n",
    "        answer_key = example['answerKey']\n",
    "        choices = example['choices']\n",
    "\n",
    "        choice_text = \"\\n\".join([\n",
    "            f\"{label}: {text}\"\n",
    "            for label, text in zip(choices['label'], choices['text'])\n",
    "        ])\n",
    "\n",
    "        prompt = f\"Question: {question}\\n\\nChoices:\\n{choice_text}\\n\\nAnswer:\"\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,\n",
    "                temperature=0.0,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = generated[len(prompt):].strip().upper()\n",
    "\n",
    "        is_correct = answer_key.upper() in response[:5]\n",
    "\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "        results.append({\n",
    "            'question': question[:50] + '...',\n",
    "            'correct_answer': answer_key,\n",
    "            'response': response[:20],\n",
    "            'is_correct': is_correct\n",
    "        })\n",
    "\n",
    "    accuracy = 100.0 * correct / total if total > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'correct': correct,\n",
    "        'total': total,\n",
    "        'results': results\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_lambada(model, tokenizer, dataset, max_samples: int = None) -> Dict:\n",
    "    \"\"\"Evaluate model on LAMBADA word prediction task.\"\"\"\n",
    "    if dataset is None:\n",
    "        return {'accuracy': 0.0, 'error': 'Dataset not loaded'}\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results = []\n",
    "\n",
    "    samples = dataset if max_samples is None else dataset.select(range(min(max_samples, len(dataset))))\n",
    "\n",
    "    for example in samples:\n",
    "        text = example['text']\n",
    "        words = text.split()\n",
    "\n",
    "        if len(words) < 2:\n",
    "            continue\n",
    "\n",
    "        target_word = words[-1].lower().strip('.,!?')\n",
    "        context = ' '.join(words[:-1])\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            context,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=5,\n",
    "                temperature=0.0,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        prediction = generated[len(context):].strip().lower().split()[0] if generated[len(context):].strip() else \"\"\n",
    "        prediction = prediction.strip('.,!?')\n",
    "\n",
    "        is_correct = target_word == prediction or target_word.startswith(prediction)\n",
    "\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "        results.append({\n",
    "            'target': target_word,\n",
    "            'prediction': prediction,\n",
    "            'is_correct': is_correct\n",
    "        })\n",
    "\n",
    "    accuracy = 100.0 * correct / total if total > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'correct': correct,\n",
    "        'total': total\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model_full(model, tokenizer, eval_datasets: Dict,\n",
    "                        config: ExperimentConfig = None) -> Dict:\n",
    "    \"\"\"Run full evaluation suite on model.\"\"\"\n",
    "    if config is None:\n",
    "        config = CONFIG\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    print(\"    Running ARC-Easy evaluation...\", end=\" \")\n",
    "    arc_results = evaluate_arc_easy(model, tokenizer, eval_datasets.get('arc'))\n",
    "    results['arc_easy'] = arc_results['accuracy']\n",
    "    print(f\"Accuracy: {arc_results['accuracy']:.1f}%\")\n",
    "\n",
    "    print(\"    Running LAMBADA evaluation...\", end=\" \")\n",
    "    lambada_results = evaluate_lambada(model, tokenizer, eval_datasets.get('lambada'))\n",
    "    results['lambada'] = lambada_results['accuracy']\n",
    "    print(f\"Accuracy: {lambada_results['accuracy']:.1f}%\")\n",
    "\n",
    "    results['composite'] = (results['arc_easy'] + results['lambada']) / 2\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 6: DATASET PREPARATION\n",
    "# ============================================================================\n",
    "def prepare_finetune_dataset(tokenizer, config: ExperimentConfig, seed: int):\n",
    "    \"\"\"Prepare fine-tuning dataset for knowledge recovery training.\"\"\"\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"PREPARING FINE-TUNING DATASET (seed={seed})\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    dataset = load_dataset(\n",
    "        config.finetune_dataset,\n",
    "        split=\"train\"\n",
    "    )\n",
    "\n",
    "    dataset = dataset.shuffle(seed=seed).select(\n",
    "        range(min(config.finetune_subset_size, len(dataset)))\n",
    "    )\n",
    "\n",
    "    print(f\"  Dataset: {config.finetune_dataset}\")\n",
    "    print(f\"  Subset size: {len(dataset)} samples\")\n",
    "\n",
    "    def format_example(example):\n",
    "        instruction = example.get('instruction', example.get('context', ''))\n",
    "        response = example.get('response', example.get('text', ''))\n",
    "\n",
    "        text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n",
    "        return {'text': text}\n",
    "\n",
    "    dataset = dataset.map(format_example)\n",
    "\n",
    "    def tokenize(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            truncation=True,\n",
    "            max_length=config.max_seq_length,\n",
    "            padding='max_length'\n",
    "        )\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "    dataset.set_format('torch')\n",
    "\n",
    "    print(f\"  Tokenized: max_length={config.max_seq_length}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 7: TREATMENT APPLICATION WITH ISODOSE SUPPORT\n",
    "# ============================================================================\n",
    "def apply_treatment_with_dose(base_model, tokenizer, train_dataset,\n",
    "                               dose: TreatmentDose, config: ExperimentConfig,\n",
    "                               seed: int) -> Tuple[nn.Module, Dict]:\n",
    "    \"\"\"\n",
    "    Apply a treatment using calibrated isodose parameters.\n",
    "\n",
    "    UNIFIED TREATMENT APPLICATION:\n",
    "    This function applies any treatment using parameters from a TreatmentDose\n",
    "    object, enabling both default and isodose experiments with the same code.\n",
    "\n",
    "    Args:\n",
    "        base_model: The pruned model to treat\n",
    "        tokenizer: Tokenizer\n",
    "        train_dataset: Fine-tuning dataset\n",
    "        dose: TreatmentDose object with calibrated parameters\n",
    "        config: Experiment configuration\n",
    "        seed: Random seed\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (treated_model, treatment_stats)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"APPLYING {dose.treatment_name.upper()} TREATMENT\")\n",
    "    if config.isodose_mode:\n",
    "        print(\"  [ISODOSE MODE]\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"  LoRA rank: {dose.lora_rank}\")\n",
    "    print(f\"  LoRA alpha: {dose.lora_alpha}\")\n",
    "    print(f\"  Training epochs: {dose.epochs}\")\n",
    "    print(f\"  Learning rate: {dose.lr}\")\n",
    "    print(f\"  Dropout: {dose.dropout}\")\n",
    "    print(f\"  Target modules: {dose.target_modules}\")\n",
    "    print(f\"  Estimated FLOPs: {dose.estimated_flops:.2e}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    model = copy.deepcopy(base_model)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=dose.lora_rank,\n",
    "        lora_alpha=dose.lora_alpha,\n",
    "        target_modules=dose.target_modules,\n",
    "        lora_dropout=dose.dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    print(f\"  Actual trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=os.path.join(config.output_dir, f\"{dose.treatment_name}_seed{seed}\"),\n",
    "        num_train_epochs=dose.epochs,\n",
    "        learning_rate=dose.lr,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_ratio=0.1,\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"no\",\n",
    "        fp16=True,\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    print(\"  Starting training...\")\n",
    "    start_time = time.time()\n",
    "    trainer.train()\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    print(f\"  Training complete in {train_time:.1f}s\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    stats = {\n",
    "        'treatment': dose.treatment_name,\n",
    "        'isodose_mode': config.isodose_mode,\n",
    "        'lora_rank': dose.lora_rank,\n",
    "        'epochs': dose.epochs,\n",
    "        'lr': dose.lr,\n",
    "        'dropout': dose.dropout,\n",
    "        'trainable_params': trainable_params,\n",
    "        'estimated_flops': dose.estimated_flops,\n",
    "        'train_time': train_time\n",
    "    }\n",
    "\n",
    "    return model, stats\n",
    "\n",
    "\n",
    "def apply_ketamine_treatment(base_model, tokenizer, train_dataset,\n",
    "                             config: ExperimentConfig, seed: int,\n",
    "                             dose: TreatmentDose = None):\n",
    "    \"\"\"Apply ketamine-like treatment (wrapper for compatibility).\"\"\"\n",
    "    if dose is None:\n",
    "        dose = TreatmentDose(\n",
    "            treatment_name='ketamine',\n",
    "            lora_rank=config.ketamine_lora_rank,\n",
    "            lora_alpha=config.ketamine_lora_alpha,\n",
    "            epochs=config.ketamine_epochs,\n",
    "            lr=config.ketamine_lr,\n",
    "            dropout=config.ketamine_dropout,\n",
    "            target_modules=config.ketamine_target_modules\n",
    "        )\n",
    "        dose.compute_flops(config.finetune_subset_size, config.max_seq_length)\n",
    "\n",
    "    return apply_treatment_with_dose(base_model, tokenizer, train_dataset,\n",
    "                                      dose, config, seed)\n",
    "\n",
    "\n",
    "def apply_ssri_treatment(base_model, tokenizer, train_dataset,\n",
    "                         config: ExperimentConfig, seed: int,\n",
    "                         dose: TreatmentDose = None):\n",
    "    \"\"\"Apply SSRI-like treatment (wrapper for compatibility).\"\"\"\n",
    "    if dose is None:\n",
    "        dose = TreatmentDose(\n",
    "            treatment_name='ssri',\n",
    "            lora_rank=config.ssri_lora_rank,\n",
    "            lora_alpha=config.ssri_lora_alpha,\n",
    "            epochs=config.ssri_epochs,\n",
    "            lr=config.ssri_lr,\n",
    "            dropout=config.ssri_dropout,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"]\n",
    "        )\n",
    "        dose.compute_flops(config.finetune_subset_size, config.max_seq_length)\n",
    "\n",
    "    return apply_treatment_with_dose(base_model, tokenizer, train_dataset,\n",
    "                                      dose, config, seed)\n",
    "\n",
    "\n",
    "def apply_neurosteroid_treatment(base_model, tokenizer, train_dataset,\n",
    "                                  config: ExperimentConfig, seed: int,\n",
    "                                  dose: TreatmentDose = None):\n",
    "    \"\"\"Apply neurosteroid-like treatment (wrapper for compatibility).\"\"\"\n",
    "    if dose is None:\n",
    "        dose = TreatmentDose(\n",
    "            treatment_name='neurosteroid',\n",
    "            lora_rank=config.neuro_lora_rank,\n",
    "            lora_alpha=config.neuro_lora_alpha,\n",
    "            epochs=config.neuro_epochs,\n",
    "            lr=config.neuro_lr,\n",
    "            dropout=config.neuro_dropout,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"gate_proj\", \"up_proj\"]\n",
    "        )\n",
    "        dose.compute_flops(config.finetune_subset_size, config.max_seq_length)\n",
    "\n",
    "    return apply_treatment_with_dose(base_model, tokenizer, train_dataset,\n",
    "                                      dose, config, seed)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 8: ACUTE RELAPSE SIMULATION\n",
    "# ============================================================================\n",
    "def apply_acute_relapse(model, prune_amount: float = 0.30):\n",
    "    \"\"\"Simulate acute relapse by applying unstructured pruning.\"\"\"\n",
    "    print(f\"    Applying acute relapse: {prune_amount*100:.0f}% unstructured pruning\")\n",
    "\n",
    "    pruned_layers = 0\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            try:\n",
    "                prune.l1_unstructured(module, name='weight', amount=prune_amount)\n",
    "                prune.remove(module, 'weight')\n",
    "                pruned_layers += 1\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    print(f\"    Pruned {pruned_layers} linear layers\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 9: LONGITUDINAL SIMULATION\n",
    "# ============================================================================\n",
    "def run_longitudinal_simulation(treated_model, tokenizer, train_dataset,\n",
    "                                 eval_datasets, treatment_name: str,\n",
    "                                 config: ExperimentConfig):\n",
    "    \"\"\"Simulate longitudinal course with chronic stress and maintenance.\"\"\"\n",
    "    print(f\"\\n  --- Longitudinal Simulation for {treatment_name.upper()} ---\")\n",
    "\n",
    "    trajectory = {\n",
    "        'accuracy': [],\n",
    "        'cycle': []\n",
    "    }\n",
    "\n",
    "    model = treated_model\n",
    "\n",
    "    if treatment_name == 'ketamine':\n",
    "        maintenance_epochs = config.ketamine_maintenance_epochs\n",
    "    elif treatment_name == 'ssri':\n",
    "        maintenance_epochs = config.ssri_maintenance_epochs\n",
    "    else:\n",
    "        maintenance_epochs = config.neuro_maintenance_epochs\n",
    "\n",
    "    for cycle in range(config.longitudinal_cycles + 1):\n",
    "        print(f\"    Cycle {cycle}:\", end=\" \")\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            arc_result = evaluate_arc_easy(model, tokenizer, eval_datasets.get('arc'), max_samples=50)\n",
    "\n",
    "        accuracy = arc_result['accuracy']\n",
    "        trajectory['accuracy'].append(accuracy)\n",
    "        trajectory['cycle'].append(cycle)\n",
    "\n",
    "        print(f\"ARC-Easy = {accuracy:.1f}%\", end=\"\")\n",
    "\n",
    "        if cycle < config.longitudinal_cycles:\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    try:\n",
    "                        prune.random_unstructured(\n",
    "                            module,\n",
    "                            name='weight',\n",
    "                            amount=config.additional_prune_per_cycle\n",
    "                        )\n",
    "                        prune.remove(module, 'weight')\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            print(f\" -> Stress applied ({config.additional_prune_per_cycle*100:.0f}% prune)\", end=\"\")\n",
    "\n",
    "            if maintenance_epochs > 0:\n",
    "                model.train()\n",
    "                optimizer = torch.optim.AdamW(\n",
    "                    [p for p in model.parameters() if p.requires_grad],\n",
    "                    lr=1e-6\n",
    "                )\n",
    "\n",
    "                data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "                maintenance_data = train_dataset.select(range(min(100, len(train_dataset))))\n",
    "                train_loader = DataLoader(\n",
    "                    maintenance_data,\n",
    "                    batch_size=4,\n",
    "                    collate_fn=data_collator\n",
    "                )\n",
    "\n",
    "                for epoch in range(maintenance_epochs):\n",
    "                    for batch in train_loader:\n",
    "                        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(**batch)\n",
    "                        outputs.loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                print(f\" -> Maintenance ({maintenance_epochs} epochs)\")\n",
    "            else:\n",
    "                print()\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 10: SINGLE SEED EXPERIMENT WITH ISODOSE SUPPORT\n",
    "# ============================================================================\n",
    "def run_single_seed(seed: int, config: ExperimentConfig = None,\n",
    "                    calibrated_doses: Dict[str, TreatmentDose] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Run complete experiment for a single random seed.\n",
    "\n",
    "    ISODOSE INTEGRATION:\n",
    "    If calibrated_doses is provided and config.isodose_mode is True,\n",
    "    treatments will use the isodose-calibrated parameters.\n",
    "    Otherwise, default parameters from config are used.\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = CONFIG\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    print(\"\\n\" + \"#\" * 70)\n",
    "    print(f\"#  SEED {seed} EXPERIMENT\")\n",
    "    if config.isodose_mode:\n",
    "        print(\"#  [ISODOSE MODE ACTIVE]\")\n",
    "    print(\"#\" * 70)\n",
    "\n",
    "    results = {\n",
    "        'seed': seed,\n",
    "        'isodose_mode': config.isodose_mode,\n",
    "        'treatments': {},\n",
    "        'longitudinal': {}\n",
    "    }\n",
    "\n",
    "    eval_datasets = load_evaluation_datasets(config)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 1: Load pruned model and evaluate untreated baseline\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n[STEP 1] Loading pruned model (untreated baseline)\")\n",
    "    base_model, tokenizer = load_pruned_model(config)\n",
    "\n",
    "    print(\"\\n[STEP 2] Evaluating UNTREATED (pruned) baseline\")\n",
    "    print(\"-\" * 70)\n",
    "    untreated_results = evaluate_model_full(base_model, tokenizer, eval_datasets, config)\n",
    "\n",
    "    results['treatments']['untreated'] = {\n",
    "        'post_treatment': untreated_results,\n",
    "        'post_relapse': None,\n",
    "        'relapse_drop': None\n",
    "    }\n",
    "\n",
    "    print(f\"  UNTREATED BASELINE:\")\n",
    "    print(f\"    ARC-Easy: {untreated_results['arc_easy']:.1f}%\")\n",
    "    print(f\"    LAMBADA:  {untreated_results['lambada']:.1f}%\")\n",
    "    print(f\"    Composite: {untreated_results['composite']:.1f}%\")\n",
    "\n",
    "    train_dataset = prepare_finetune_dataset(tokenizer, config, seed)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 3: Apply treatments and evaluate\n",
    "    # -------------------------------------------------------------------------\n",
    "    treatments = ['ketamine', 'ssri', 'neurosteroid']\n",
    "\n",
    "    for treatment_name in treatments:\n",
    "        print(f\"\\n[STEP 3.{treatments.index(treatment_name)+1}] Processing {treatment_name.upper()} treatment\")\n",
    "\n",
    "        # Get dose (isodose-calibrated or default)\n",
    "        if config.isodose_mode and calibrated_doses:\n",
    "            dose = calibrated_doses[treatment_name]\n",
    "            print(f\"  Using ISODOSE parameters (target FLOPs: {dose.estimated_flops:.2e})\")\n",
    "        else:\n",
    "            dose = None\n",
    "            print(f\"  Using DEFAULT parameters\")\n",
    "\n",
    "        print(\"  Reloading fresh pruned model...\")\n",
    "        fresh_model, _ = load_pruned_model(config)\n",
    "\n",
    "        # Apply treatment\n",
    "        if treatment_name == 'ketamine':\n",
    "            treated_model, treatment_stats = apply_ketamine_treatment(\n",
    "                fresh_model, tokenizer, train_dataset, config, seed, dose\n",
    "            )\n",
    "        elif treatment_name == 'ssri':\n",
    "            treated_model, treatment_stats = apply_ssri_treatment(\n",
    "                fresh_model, tokenizer, train_dataset, config, seed, dose\n",
    "            )\n",
    "        else:\n",
    "            treated_model, treatment_stats = apply_neurosteroid_treatment(\n",
    "                fresh_model, tokenizer, train_dataset, config, seed, dose\n",
    "            )\n",
    "\n",
    "        # Evaluate post-treatment\n",
    "        print(f\"\\n  Evaluating POST-{treatment_name.upper()} performance:\")\n",
    "        post_treatment = evaluate_model_full(treated_model, tokenizer, eval_datasets, config)\n",
    "\n",
    "        print(f\"    ARC-Easy: {post_treatment['arc_easy']:.1f}%\")\n",
    "        print(f\"    LAMBADA:  {post_treatment['lambada']:.1f}%\")\n",
    "        print(f\"    Composite: {post_treatment['composite']:.1f}%\")\n",
    "\n",
    "        recovery = post_treatment['composite'] - untreated_results['composite']\n",
    "        print(f\"    Recovery from untreated: {recovery:+.1f}%\")\n",
    "\n",
    "        # Calculate efficiency metric for isodose comparison\n",
    "        if config.isodose_mode and calibrated_doses:\n",
    "            efficiency = recovery / (treatment_stats['estimated_flops'] / 1e15)  # Recovery per PetaFLOP\n",
    "            print(f\"    Efficiency (recovery/PetaFLOP): {efficiency:.2f}\")\n",
    "            treatment_stats['efficiency'] = efficiency\n",
    "\n",
    "        # Apply acute relapse\n",
    "        print(f\"\\n  Simulating ACUTE RELAPSE:\")\n",
    "        apply_acute_relapse(treated_model, config.acute_relapse_prune_amount)\n",
    "\n",
    "        # Evaluate post-relapse\n",
    "        print(f\"  Evaluating POST-RELAPSE performance:\")\n",
    "        post_relapse = evaluate_model_full(treated_model, tokenizer, eval_datasets, config)\n",
    "\n",
    "        relapse_drop = post_treatment['composite'] - post_relapse['composite']\n",
    "        print(f\"    ARC-Easy: {post_relapse['arc_easy']:.1f}%\")\n",
    "        print(f\"    LAMBADA:  {post_relapse['lambada']:.1f}%\")\n",
    "        print(f\"    Composite: {post_relapse['composite']:.1f}%\")\n",
    "        print(f\"    Relapse drop: {relapse_drop:.1f}%\")\n",
    "\n",
    "        results['treatments'][treatment_name] = {\n",
    "            'stats': treatment_stats,\n",
    "            'post_treatment': post_treatment,\n",
    "            'post_relapse': post_relapse,\n",
    "            'recovery': recovery,\n",
    "            'relapse_drop': relapse_drop\n",
    "        }\n",
    "\n",
    "        # Run longitudinal simulation\n",
    "        print(f\"\\n  Running LONGITUDINAL simulation for {treatment_name}:\")\n",
    "\n",
    "        fresh_model2, _ = load_pruned_model(config)\n",
    "\n",
    "        if treatment_name == 'ketamine':\n",
    "            long_model, _ = apply_ketamine_treatment(\n",
    "                fresh_model2, tokenizer, train_dataset, config, seed, dose\n",
    "            )\n",
    "        elif treatment_name == 'ssri':\n",
    "            long_model, _ = apply_ssri_treatment(\n",
    "                fresh_model2, tokenizer, train_dataset, config, seed, dose\n",
    "            )\n",
    "        else:\n",
    "            long_model, _ = apply_neurosteroid_treatment(\n",
    "                fresh_model2, tokenizer, train_dataset, config, seed, dose\n",
    "            )\n",
    "\n",
    "        trajectory = run_longitudinal_simulation(\n",
    "            long_model, tokenizer, train_dataset, eval_datasets,\n",
    "            treatment_name, config\n",
    "        )\n",
    "\n",
    "        results['longitudinal'][treatment_name] = trajectory\n",
    "\n",
    "        del treated_model, fresh_model, long_model\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\n\" + \"#\" * 70)\n",
    "    print(f\"#  SEED {seed} COMPLETE\")\n",
    "    print(\"#\" * 70)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 11: RESULTS AGGREGATION AND PRINTING\n",
    "# ============================================================================\n",
    "def aggregate_results(all_results: List[Dict]) -> Dict:\n",
    "    \"\"\"Aggregate results across all seeds.\"\"\"\n",
    "    aggregated = {\n",
    "        'n_seeds': len(all_results),\n",
    "        'isodose_mode': all_results[0].get('isodose_mode', False) if all_results else False,\n",
    "        'treatments': {},\n",
    "        'longitudinal': {}\n",
    "    }\n",
    "\n",
    "    treatments = ['untreated', 'ketamine', 'ssri', 'neurosteroid']\n",
    "\n",
    "    for treatment in treatments:\n",
    "        metrics = defaultdict(list)\n",
    "\n",
    "        for seed_result in all_results:\n",
    "            if treatment in seed_result.get('treatments', {}):\n",
    "                treat_data = seed_result['treatments'][treatment]\n",
    "\n",
    "                if 'post_treatment' in treat_data and treat_data['post_treatment']:\n",
    "                    for key, value in treat_data['post_treatment'].items():\n",
    "                        if isinstance(value, (int, float)):\n",
    "                            metrics[f'post_{key}'].append(value)\n",
    "\n",
    "                if treat_data.get('recovery') is not None:\n",
    "                    metrics['recovery'].append(treat_data['recovery'])\n",
    "                if treat_data.get('relapse_drop') is not None:\n",
    "                    metrics['relapse_drop'].append(treat_data['relapse_drop'])\n",
    "\n",
    "                # Collect isodose-specific metrics\n",
    "                if 'stats' in treat_data and treat_data['stats']:\n",
    "                    if 'efficiency' in treat_data['stats']:\n",
    "                        metrics['efficiency'].append(treat_data['stats']['efficiency'])\n",
    "                    if 'estimated_flops' in treat_data['stats']:\n",
    "                        metrics['flops'].append(treat_data['stats']['estimated_flops'])\n",
    "\n",
    "                if 'post_relapse' in treat_data and treat_data['post_relapse']:\n",
    "                    for key, value in treat_data['post_relapse'].items():\n",
    "                        if isinstance(value, (int, float)):\n",
    "                            metrics[f'relapse_{key}'].append(value)\n",
    "\n",
    "        aggregated['treatments'][treatment] = {}\n",
    "        for key, values in metrics.items():\n",
    "            if values:\n",
    "                aggregated['treatments'][treatment][key] = {\n",
    "                    'mean': np.mean(values),\n",
    "                    'std': np.std(values),\n",
    "                    'values': values\n",
    "                }\n",
    "\n",
    "    # Aggregate longitudinal\n",
    "    for treat in ['ketamine', 'ssri', 'neurosteroid']:\n",
    "        trajectories = []\n",
    "\n",
    "        for seed_result in all_results:\n",
    "            if 'longitudinal' in seed_result and treat in seed_result['longitudinal']:\n",
    "                trajectories.append(seed_result['longitudinal'][treat]['accuracy'])\n",
    "\n",
    "        if trajectories:\n",
    "            traj_array = np.array(trajectories)\n",
    "            aggregated['longitudinal'][treat] = {\n",
    "                'accuracy_mean': np.mean(traj_array, axis=0).tolist(),\n",
    "                'accuracy_std': np.std(traj_array, axis=0).tolist(),\n",
    "                'initial_mean': float(np.mean(traj_array[:, 0])),\n",
    "                'final_mean': float(np.mean(traj_array[:, -1])),\n",
    "                'total_drop_mean': float(np.mean(traj_array[:, 0] - traj_array[:, -1])),\n",
    "                'total_drop_std': float(np.std(traj_array[:, 0] - traj_array[:, -1]))\n",
    "            }\n",
    "\n",
    "    return aggregated\n",
    "\n",
    "\n",
    "def print_final_results(aggregated: Dict, config: ExperimentConfig,\n",
    "                        calibrated_doses: Dict[str, TreatmentDose] = None):\n",
    "    \"\"\"Print comprehensive final results tables.\"\"\"\n",
    "    print(\"\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"=\" * 80)\n",
    "    print(\"  FINAL AGGREGATED RESULTS\")\n",
    "    print(f\"  {aggregated['n_seeds']} seeds completed\")\n",
    "    if aggregated.get('isodose_mode'):\n",
    "        print(\"  [ISODOSE COMPARISON MODE]\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # ISODOSE CONFIGURATION SUMMARY (if applicable)\n",
    "    # -------------------------------------------------------------------------\n",
    "    if aggregated.get('isodose_mode') and calibrated_doses:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"  ISODOSE CONFIGURATION\")\n",
    "        print(\"  (All treatments calibrated to equivalent computational cost)\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        print(f\"\\n  {'Treatment':<15} {'Rank':>6} {'Epochs':>8} {'Dropout':>8} {'FLOPs':>14}\")\n",
    "        print(\"  \" + \"-\" * 55)\n",
    "\n",
    "        for name in ['ketamine', 'ssri', 'neurosteroid']:\n",
    "            if name in calibrated_doses:\n",
    "                dose = calibrated_doses[name]\n",
    "                print(f\"  {name.capitalize():<15} {dose.lora_rank:>6} {dose.epochs:>8} \"\n",
    "                      f\"{dose.dropout:>8.2f} {dose.estimated_flops:>14.2e}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TABLE 1: Post-Treatment Performance\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"  TABLE 1: POST-TREATMENT PERFORMANCE\")\n",
    "    print(\"  (Mean ± Std across seeds, percentages)\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    print(f\"\\n  {'Treatment':<20} {'ARC-Easy':>15} {'LAMBADA':>15} {'Composite':>15}\")\n",
    "    print(\"  \" + \"-\" * 65)\n",
    "\n",
    "    labels = {\n",
    "        'untreated': 'Untreated (pruned)',\n",
    "        'ketamine': 'Ketamine-like',\n",
    "        'ssri': 'SSRI-like',\n",
    "        'neurosteroid': 'Neurosteroid-like'\n",
    "    }\n",
    "\n",
    "    for treat in ['untreated', 'ketamine', 'ssri', 'neurosteroid']:\n",
    "        if treat in aggregated['treatments']:\n",
    "            data = aggregated['treatments'][treat]\n",
    "\n",
    "            arc = data.get('post_arc_easy', {})\n",
    "            lamb = data.get('post_lambada', {})\n",
    "            comp = data.get('post_composite', {})\n",
    "\n",
    "            arc_str = f\"{arc.get('mean', 0):.1f}±{arc.get('std', 0):.1f}\" if arc else \"N/A\"\n",
    "            lamb_str = f\"{lamb.get('mean', 0):.1f}±{lamb.get('std', 0):.1f}\" if lamb else \"N/A\"\n",
    "            comp_str = f\"{comp.get('mean', 0):.1f}±{comp.get('std', 0):.1f}\" if comp else \"N/A\"\n",
    "\n",
    "            print(f\"  {labels[treat]:<20} {arc_str:>15} {lamb_str:>15} {comp_str:>15}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TABLE 2: Treatment Effects (Recovery and Relapse Resilience)\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"  TABLE 2: TREATMENT EFFECTS\")\n",
    "    print(\"  (Recovery = improvement over untreated, Relapse Drop = loss after acute stress)\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    print(f\"\\n  {'Treatment':<20} {'Recovery':>18} {'Relapse Drop':>18} {'Net Retained':>15}\")\n",
    "    print(\"  \" + \"-\" * 70)\n",
    "\n",
    "    for treat in ['ketamine', 'ssri', 'neurosteroid']:\n",
    "        if treat in aggregated['treatments']:\n",
    "            data = aggregated['treatments'][treat]\n",
    "\n",
    "            rec = data.get('recovery', {})\n",
    "            drop = data.get('relapse_drop', {})\n",
    "\n",
    "            rec_mean = rec.get('mean', 0)\n",
    "            rec_std = rec.get('std', 0)\n",
    "            drop_mean = drop.get('mean', 0)\n",
    "            drop_std = drop.get('std', 0)\n",
    "            net_retained = rec_mean - drop_mean\n",
    "\n",
    "            rec_str = f\"{rec_mean:+.1f}±{rec_std:.1f}%\"\n",
    "            drop_str = f\"{drop_mean:.1f}±{drop_std:.1f}%\"\n",
    "            net_str = f\"{net_retained:+.1f}%\"\n",
    "\n",
    "            print(f\"  {labels[treat]:<20} {rec_str:>18} {drop_str:>18} {net_str:>15}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TABLE 3: ISODOSE EFFICIENCY COMPARISON (NEW)\n",
    "    # -------------------------------------------------------------------------\n",
    "    if aggregated.get('isodose_mode'):\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"  TABLE 3: ISODOSE EFFICIENCY COMPARISON\")\n",
    "        print(\"  (Recovery per unit computational cost - higher is more efficient)\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        print(f\"\\n  {'Treatment':<20} {'Recovery':>12} {'FLOPs':>14} {'Efficiency*':>15}\")\n",
    "        print(\"  \" + \"-\" * 60)\n",
    "\n",
    "        efficiencies = []\n",
    "        for treat in ['ketamine', 'ssri', 'neurosteroid']:\n",
    "            if treat in aggregated['treatments']:\n",
    "                data = aggregated['treatments'][treat]\n",
    "\n",
    "                rec = data.get('recovery', {}).get('mean', 0)\n",
    "                flops = data.get('flops', {}).get('mean', 0)\n",
    "                eff = data.get('efficiency', {})\n",
    "\n",
    "                eff_mean = eff.get('mean', 0) if eff else 0\n",
    "                eff_std = eff.get('std', 0) if eff else 0\n",
    "\n",
    "                efficiencies.append((treat, eff_mean))\n",
    "\n",
    "                rec_str = f\"{rec:+.1f}%\"\n",
    "                flops_str = f\"{flops:.2e}\" if flops else \"N/A\"\n",
    "                eff_str = f\"{eff_mean:.2f}±{eff_std:.2f}\" if eff else \"N/A\"\n",
    "\n",
    "                print(f\"  {labels[treat]:<20} {rec_str:>12} {flops_str:>14} {eff_str:>15}\")\n",
    "\n",
    "        print(\"\\n  * Efficiency = Recovery (%) / PetaFLOPs\")\n",
    "\n",
    "        # Rank treatments by efficiency\n",
    "        if efficiencies:\n",
    "            efficiencies.sort(key=lambda x: x[1], reverse=True)\n",
    "            print(f\"\\n  ISODOSE RANKING (by efficiency):\")\n",
    "            for i, (treat, eff) in enumerate(efficiencies):\n",
    "                print(f\"    {i+1}. {labels[treat]}: {eff:.2f}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TABLE 4: Longitudinal Trajectory\n",
    "    # -------------------------------------------------------------------------\n",
    "    if aggregated['longitudinal']:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"  TABLE 4: LONGITUDINAL TRAJECTORY (ARC-Easy % over stress cycles)\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        n_cycles = len(aggregated['longitudinal']['ketamine']['accuracy_mean'])\n",
    "\n",
    "        header = f\"\\n  {'Cycle':<8}\"\n",
    "        for treat in ['ketamine', 'ssri', 'neurosteroid']:\n",
    "            header += f\" {treat.capitalize():>18}\"\n",
    "        print(header)\n",
    "        print(\"  \" + \"-\" * 62)\n",
    "\n",
    "        for c in range(n_cycles):\n",
    "            row = f\"  {c:<8}\"\n",
    "            for treat in ['ketamine', 'ssri', 'neurosteroid']:\n",
    "                mean = aggregated['longitudinal'][treat]['accuracy_mean'][c]\n",
    "                std = aggregated['longitudinal'][treat]['accuracy_std'][c]\n",
    "                row += f\" {mean:>7.1f}±{std:>5.1f}%\"\n",
    "            print(row)\n",
    "\n",
    "        print(\"\\n  \" + \"-\" * 62)\n",
    "        print(\"  LONGITUDINAL SUMMARY:\")\n",
    "\n",
    "        for treat in ['ketamine', 'ssri', 'neurosteroid']:\n",
    "            long = aggregated['longitudinal'][treat]\n",
    "            print(f\"    {labels[treat]:<20}\")\n",
    "            print(f\"      Initial (cycle 0): {long['initial_mean']:.1f}%\")\n",
    "            print(f\"      Final (cycle {n_cycles-1}):   {long['final_mean']:.1f}%\")\n",
    "            print(f\"      Total drop:        {long['total_drop_mean']:.1f}±{long['total_drop_std']:.1f}%\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # INTERPRETATION GUIDE\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"  INTERPRETATION GUIDE\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    if aggregated.get('isodose_mode'):\n",
    "        print(\"\"\"\n",
    "    ISODOSE COMPARISON INTERPRETATION:\n",
    "\n",
    "    The isodose framework enables FAIR comparison by equalizing computational cost.\n",
    "    This answers: \"Given the SAME therapeutic effort, which mechanism is superior?\"\n",
    "\n",
    "    KEY ISODOSE METRICS:\n",
    "    1. Recovery (%) - Improvement over untreated baseline\n",
    "    2. Efficiency - Recovery per PetaFLOP of computation\n",
    "    3. Relapse Drop - Vulnerability to acute stress\n",
    "    4. Longitudinal Stability - Durability under chronic stress\n",
    "\n",
    "    WHAT ISODOSE REVEALS:\n",
    "    - If a treatment with LOW structural capacity (SSRI) matches HIGH capacity (ketamine)\n",
    "      at equal FLOPs, the MECHANISM itself may be more efficient\n",
    "    - If HIGH capacity (ketamine) still wins at isodose, structural regrowth may be\n",
    "      fundamentally more effective for this type of damage\n",
    "\n",
    "    EXPECTED PATTERNS AT ISODOSE:\n",
    "\n",
    "    KETAMINE-LIKE (r=64, short duration):\n",
    "    - High efficiency IF structural regrowth is critical for knowledge recovery\n",
    "    - May show diminishing returns if extra capacity isn't fully utilized\n",
    "\n",
    "    SSRI-LIKE (r=8, extended duration):\n",
    "    - At isodose, gets MORE epochs than default (scaled up to match FLOPs)\n",
    "    - Tests whether gradual learning can compensate for low capacity\n",
    "    - May show improved efficiency if default was \"undertreated\"\n",
    "\n",
    "    NEUROSTEROID-LIKE (r=32, high dropout):\n",
    "    - At isodose, dropout regularization applied over calibrated epochs\n",
    "    - Tests whether inhibition-based stabilization is cost-effective\n",
    "\n",
    "    COMPARING DEFAULT vs ISODOSE:\n",
    "    - If rankings CHANGE between modes, the default comparison was unfair\n",
    "    - If rankings PERSIST, the mechanism difference is robust to dosing\n",
    "    \"\"\")\n",
    "    else:\n",
    "        print(\"\"\"\n",
    "    EXPECTED PATTERNS (based on mechanistic analogies):\n",
    "\n",
    "    KETAMINE-LIKE (High-rank LoRA, r=64, aggressive training):\n",
    "    - EXPECTED: Best acute recovery due to high new parameter capacity\n",
    "    - EXPECTED: Smallest relapse drop (structural changes are robust)\n",
    "    - EXPECTED: Best longitudinal stability under chronic stress\n",
    "    - ANALOGY: Rapid synaptogenesis provides lasting structural benefits\n",
    "\n",
    "    SSRI-LIKE (Low-rank LoRA, r=8, slow gradual training):\n",
    "    - EXPECTED: Slowest, most variable recovery\n",
    "    - EXPECTED: Moderate-to-large relapse vulnerability\n",
    "    - EXPECTED: Variable longitudinal outcomes across seeds\n",
    "    - ANALOGY: Gradual neuromodulation without major structural rewiring\n",
    "\n",
    "    NEUROSTEROID-LIKE (Moderate LoRA, high dropout=0.5):\n",
    "    - EXPECTED: Fast initial response due to regularization effects\n",
    "    - EXPECTED: Intermediate relapse resilience\n",
    "    - EXPECTED: May show state-dependence (dropout during training vs eval)\n",
    "    - ANALOGY: Tonic inhibition provides rapid stabilization\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 12: MAIN EXPERIMENT RUNNER\n",
    "# ============================================================================\n",
    "def run_experiment(config: ExperimentConfig = None):\n",
    "    \"\"\"\n",
    "    Run the complete multi-seed experiment.\n",
    "\n",
    "    ISODOSE MODE:\n",
    "    If config.isodose_mode is True, treatments are first calibrated to\n",
    "    equivalent computational cost before running experiments.\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = CONFIG\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"#\" * 80)\n",
    "    print(\"#\" * 80)\n",
    "    print(\"#\" + \" \" * 78 + \"#\")\n",
    "    print(\"#\" + \" MULTI-MECHANISM RECOVERY FROM PRUNING-INDUCED FRAGILITY \".center(78) + \"#\")\n",
    "    print(\"#\" + \" IN LARGE LANGUAGE MODELS \".center(78) + \"#\")\n",
    "    if config.isodose_mode:\n",
    "        print(\"#\" + \" [WITH ISODOSE COMPARISON] \".center(78) + \"#\")\n",
    "    print(\"#\" + \" \" * 78 + \"#\")\n",
    "    print(\"#\" * 80)\n",
    "    print(\"#\" * 80)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXPERIMENT CONFIGURATION\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"  Base model (pre-pruned): {config.pruned_model}\")\n",
    "    print(f\"  Pruning level: 60% MLP neurons removed (severe fragility)\")\n",
    "    print(f\"  Number of seeds: {config.n_seeds}\")\n",
    "    print(f\"  Evaluation samples: {config.eval_samples}\")\n",
    "    print(f\"  Fine-tune dataset: {config.finetune_dataset}\")\n",
    "    print(f\"  Fine-tune subset: {config.finetune_subset_size} samples\")\n",
    "    print(f\"  Longitudinal cycles: {config.longitudinal_cycles}\")\n",
    "    print(f\"  Acute relapse pruning: {config.acute_relapse_prune_amount*100:.0f}%\")\n",
    "    print(f\"  Chronic stress pruning: {config.additional_prune_per_cycle*100:.0f}% per cycle\")\n",
    "    print(f\"  ISODOSE MODE: {config.isodose_mode}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # ISODOSE CALIBRATION (if enabled)\n",
    "    # -------------------------------------------------------------------------\n",
    "    calibrated_doses = None\n",
    "\n",
    "    if config.isodose_mode:\n",
    "        calibrated_doses = calibrate_all_treatments(config)\n",
    "    else:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"DEFAULT TREATMENT PARAMETERS\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"  KETAMINE-LIKE:\")\n",
    "        print(f\"    LoRA rank: {config.ketamine_lora_rank}, alpha: {config.ketamine_lora_alpha}\")\n",
    "        print(f\"    Epochs: {config.ketamine_epochs}, LR: {config.ketamine_lr}\")\n",
    "        print(f\"    Dropout: {config.ketamine_dropout}\")\n",
    "\n",
    "        print(f\"  SSRI-LIKE:\")\n",
    "        print(f\"    LoRA rank: {config.ssri_lora_rank}, alpha: {config.ssri_lora_alpha}\")\n",
    "        print(f\"    Epochs: {config.ssri_epochs}, LR: {config.ssri_lr}\")\n",
    "        print(f\"    Dropout: {config.ssri_dropout}\")\n",
    "\n",
    "        print(f\"  NEUROSTEROID-LIKE:\")\n",
    "        print(f\"    LoRA rank: {config.neuro_lora_rank}, alpha: {config.neuro_lora_alpha}\")\n",
    "        print(f\"    Epochs: {config.neuro_epochs}, LR: {config.neuro_lr}\")\n",
    "        print(f\"    Dropout: {config.neuro_dropout} (HIGH - tonic inhibition)\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "    # Run experiments\n",
    "    all_results = []\n",
    "\n",
    "    for seed in range(config.n_seeds):\n",
    "        print(f\"\\n{'*'*80}\")\n",
    "        print(f\"  STARTING SEED {seed+1}/{config.n_seeds}\")\n",
    "        print(f\"{'*'*80}\")\n",
    "\n",
    "        try:\n",
    "            seed_results = run_single_seed(seed, config, calibrated_doses)\n",
    "            all_results.append(seed_results)\n",
    "\n",
    "            # Save intermediate results\n",
    "            with open(os.path.join(config.output_dir, f'seed_{seed}_results.json'), 'w') as f:\n",
    "                json.dump({\n",
    "                    'seed': seed,\n",
    "                    'isodose_mode': config.isodose_mode,\n",
    "                    'treatments': {\n",
    "                        k: {\n",
    "                            'post_treatment': v.get('post_treatment'),\n",
    "                            'recovery': v.get('recovery'),\n",
    "                            'relapse_drop': v.get('relapse_drop'),\n",
    "                            'stats': {\n",
    "                                'epochs': v.get('stats', {}).get('epochs'),\n",
    "                                'flops': v.get('stats', {}).get('estimated_flops'),\n",
    "                                'efficiency': v.get('stats', {}).get('efficiency')\n",
    "                            } if v.get('stats') else None\n",
    "                        } for k, v in seed_results['treatments'].items()\n",
    "                    }\n",
    "                }, f, indent=2)\n",
    "\n",
    "            print(f\"\\n  [SAVED] Seed {seed} results saved to {config.output_dir}/seed_{seed}_results.json\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n  [ERROR] Seed {seed} failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "    if not all_results:\n",
    "        print(\"\\n[FATAL] No successful seed runs!\")\n",
    "        return {}\n",
    "\n",
    "    # Aggregate results\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"AGGREGATING RESULTS ACROSS SEEDS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    aggregated = aggregate_results(all_results)\n",
    "\n",
    "    # Print final results\n",
    "    print_final_results(aggregated, config, calibrated_doses)\n",
    "\n",
    "    # Save final aggregated results\n",
    "    try:\n",
    "        with open(os.path.join(config.output_dir, 'aggregated_results.json'), 'w') as f:\n",
    "            def convert_to_serializable(obj):\n",
    "                if isinstance(obj, np.ndarray):\n",
    "                    return obj.tolist()\n",
    "                elif isinstance(obj, (np.float32, np.float64)):\n",
    "                    return float(obj)\n",
    "                elif isinstance(obj, (np.int32, np.int64)):\n",
    "                    return int(obj)\n",
    "                elif isinstance(obj, dict):\n",
    "                    return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "                elif isinstance(obj, list):\n",
    "                    return [convert_to_serializable(i) for i in obj]\n",
    "                return obj\n",
    "\n",
    "            json.dump(convert_to_serializable(aggregated), f, indent=2)\n",
    "\n",
    "        print(f\"\\n[SAVED] Aggregated results saved to {config.output_dir}/aggregated_results.json\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[WARNING] Could not save aggregated results: {e}\")\n",
    "\n",
    "    # Save isodose calibration if applicable\n",
    "    if config.isodose_mode and calibrated_doses:\n",
    "        try:\n",
    "            isodose_config = {\n",
    "                name: {\n",
    "                    'lora_rank': dose.lora_rank,\n",
    "                    'lora_alpha': dose.lora_alpha,\n",
    "                    'epochs': dose.epochs,\n",
    "                    'lr': dose.lr,\n",
    "                    'dropout': dose.dropout,\n",
    "                    'target_modules': dose.target_modules,\n",
    "                    'trainable_params': dose.trainable_params,\n",
    "                    'estimated_flops': dose.estimated_flops\n",
    "                }\n",
    "                for name, dose in calibrated_doses.items()\n",
    "            }\n",
    "\n",
    "            with open(os.path.join(config.output_dir, 'isodose_calibration.json'), 'w') as f:\n",
    "                json.dump(isodose_config, f, indent=2)\n",
    "\n",
    "            print(f\"[SAVED] Isodose calibration saved to {config.output_dir}/isodose_calibration.json\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Could not save isodose calibration: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXPERIMENT COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return {\n",
    "        'all_results': all_results,\n",
    "        'aggregated': aggregated,\n",
    "        'calibrated_doses': calibrated_doses\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 13: ENTRY POINT WITH MODE SELECTION\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure experiment mode\n",
    "    config = ExperimentConfig()\n",
    "\n",
    "    # Enable isodose comparison (set to False for default mode)\n",
    "    config.isodose_mode = True\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXPERIMENT MODE SELECTION\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"  isodose_mode = {config.isodose_mode}\")\n",
    "    if config.isodose_mode:\n",
    "        print(\"  -> Treatments will be calibrated to EQUIVALENT computational cost\")\n",
    "        print(\"  -> This enables FAIR comparison of mechanism efficacy\")\n",
    "    else:\n",
    "        print(\"  -> Treatments will use DEFAULT parameters\")\n",
    "        print(\"  -> Comparisons reflect both mechanism AND dose differences\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Run the experiment\n",
    "    results = run_experiment(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Isodose Sweep and Generalization"
   ],
   "metadata": {
    "id": "WeLpb4R-vM8D"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "KETAMINE ISODOSE SWEEP & GENERALIZATION TEST\n",
    "================================================================================\n",
    "\n",
    "Extension of the Multi-Mechanism Recovery experiment to:\n",
    "1. Sweep Ketamine-like treatment parameters to find optimal dosing\n",
    "2. Test generalization of optimal dose across different pruned LLM architectures\n",
    "\n",
    "Based on isodose results showing:\n",
    "- Ketamine: +2.1% recovery, +15.4 recovery/PetaFLOP efficiency\n",
    "- Superior resilience to relapse and longitudinal stress\n",
    "\n",
    "This module adds:\n",
    "- KetamineSweepConfig: Configuration for dose parameter sweeps\n",
    "- Isodose sweep: Vary rank while maintaining constant FLOPs\n",
    "- Budget sweep: Vary total computational budget at optimal rank\n",
    "- Generalization test: Validate optimal dose on different model architectures\n",
    "\n",
    "References:\n",
    "- llama-pruning: https://github.com/MedITSolutionsKurman/llama-pruning\n",
    "- LLM Course: https://github.com/peremartra/Large-Language-Model-Notebooks-Course\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 1: INSTALLATIONS (Run first in Colab)\n",
    "# ============================================================================\n",
    "# !pip install -q transformers accelerate peft datasets torch --extra-index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install -q sentencepiece einops\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 2: IMPORTS AND CONFIGURATION\n",
    "# ============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import prune\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "import warnings\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 3: EXTENDED CONFIGURATION WITH KETAMINE SWEEP\n",
    "# ============================================================================\n",
    "@dataclass\n",
    "class KetamineSweepConfig:\n",
    "    \"\"\"\n",
    "    Configuration for Ketamine dosing sweep experiments.\n",
    "\n",
    "    SWEEP MODES:\n",
    "    - \"isodose\": Vary rank while adjusting epochs to maintain constant FLOPs\n",
    "    - \"budget\": Vary total computational budget at fixed rank\n",
    "    - \"full_grid\": Sweep multiple parameters simultaneously\n",
    "\n",
    "    DESIGN RATIONALE:\n",
    "    Ketamine-like treatment showed superior efficacy in isodose comparison.\n",
    "    This sweep aims to find the optimal \"dose\" (rank/capacity) and test\n",
    "    whether this optimal configuration generalizes across model architectures.\n",
    "    \"\"\"\n",
    "    # Sweep mode selection\n",
    "    sweep_mode: str = \"isodose\"  # \"isodose\", \"budget\", or \"full_grid\"\n",
    "\n",
    "    # Rank sweep (capacity/synaptogenesis strength)\n",
    "    ranks: List[int] = field(default_factory=lambda: [16, 32, 64, 96, 128])\n",
    "\n",
    "    # Alpha scaling strategy: alpha = rank * alpha_multiplier\n",
    "    alpha_multiplier: float = 2.0\n",
    "\n",
    "    # Learning rate options for grid search\n",
    "    learning_rates: List[float] = field(default_factory=lambda: [5e-5])\n",
    "\n",
    "    # Dropout options\n",
    "    dropouts: List[float] = field(default_factory=lambda: [0.05])\n",
    "\n",
    "    # Target module configurations to test\n",
    "    target_module_configs: Dict[str, List[str]] = field(default_factory=lambda: {\n",
    "        'full': [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        'attention_only': [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "        'mlp_only': [\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        'minimal': [\"q_proj\", \"v_proj\"]\n",
    "    })\n",
    "\n",
    "    # Default target module config for rank sweep\n",
    "    default_target_config: str = 'full'\n",
    "\n",
    "    # Base FLOPs target (calibrated from reference ketamine treatment)\n",
    "    # Set to None for automatic calibration from rank=64, epochs=3\n",
    "    base_flops: Optional[float] = None\n",
    "\n",
    "    # Budget sweep multipliers (relative to base_flops)\n",
    "    budget_multipliers: List[float] = field(default_factory=lambda: [0.5, 1.0, 1.5, 2.0])\n",
    "\n",
    "    # Generalization models to test\n",
    "    # Format: List of (model_id, description, expected_architecture) tuples\n",
    "    generalization_models: List[Tuple[str, str, str]] = field(default_factory=lambda: [\n",
    "        (\"oopere/pruned60-llama-3.2-1B\", \"Primary: Llama-3.2-1B 60% pruned\", \"llama\"),\n",
    "        # Add alternative models here - examples:\n",
    "        # (\"username/pruned-mistral-7b\", \"Mistral-7B pruned\", \"mistral\"),\n",
    "        # (\"username/pruned-phi-3\", \"Phi-3 pruned\", \"phi\"),\n",
    "    ])\n",
    "\n",
    "    # Number of seeds for sweep (1 for fast ranking, 3 for validation)\n",
    "    sweep_seeds: int = 1\n",
    "    validation_seeds: int = 3\n",
    "\n",
    "    # Number of top configurations to validate with more seeds\n",
    "    top_k_validate: int = 2\n",
    "\n",
    "    # Evaluation settings for sweep (reduced for speed)\n",
    "    sweep_eval_samples: int = 100\n",
    "\n",
    "    # Whether to run acute relapse test during sweep\n",
    "    test_relapse_in_sweep: bool = True\n",
    "\n",
    "    # Whether to run longitudinal test during sweep (slower)\n",
    "    test_longitudinal_in_sweep: bool = False\n",
    "\n",
    "    # Output directory for sweep results\n",
    "    sweep_output_dir: str = \"./ketamine_sweep_results\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"\n",
    "    Extended configuration including Ketamine sweep parameters.\n",
    "    \"\"\"\n",
    "    # Model settings\n",
    "    pruned_model: str = \"oopere/pruned60-llama-3.2-1B\"\n",
    "    use_4bit: bool = False\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Experiment settings\n",
    "    n_seeds: int = 3\n",
    "    output_dir: str = \"./llm_treatment_results\"\n",
    "    eval_samples: int = 200\n",
    "\n",
    "    # Isodose settings\n",
    "    isodose_mode: bool = True\n",
    "    target_flops: Optional[float] = None\n",
    "    isodose_reference: str = \"ketamine\"\n",
    "\n",
    "    # Ketamine sweep configuration\n",
    "    ketamine_sweep: KetamineSweepConfig = field(default_factory=KetamineSweepConfig)\n",
    "\n",
    "    # Run mode flags\n",
    "    run_main_experiment: bool = False  # Set to True to run original 3-treatment comparison\n",
    "    run_ketamine_sweep: bool = True    # Set to True to run Ketamine optimization sweep\n",
    "    run_generalization_test: bool = True  # Set to True to test on other models\n",
    "\n",
    "    # Default treatment parameters (used when not in sweep mode)\n",
    "    ketamine_lora_rank: int = 64\n",
    "    ketamine_lora_alpha: int = 128\n",
    "    ketamine_epochs: int = 3\n",
    "    ketamine_lr: float = 5e-5\n",
    "    ketamine_target_modules: List[str] = field(\n",
    "        default_factory=lambda: [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
    "                                  \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    )\n",
    "    ketamine_dropout: float = 0.05\n",
    "\n",
    "    # SSRI parameters (for comparison if needed)\n",
    "    ssri_lora_rank: int = 8\n",
    "    ssri_lora_alpha: int = 16\n",
    "    ssri_epochs: int = 15\n",
    "    ssri_lr: float = 1e-6\n",
    "    ssri_dropout: float = 0.1\n",
    "\n",
    "    # Neurosteroid parameters (for comparison if needed)\n",
    "    neuro_lora_rank: int = 32\n",
    "    neuro_lora_alpha: int = 64\n",
    "    neuro_epochs: int = 5\n",
    "    neuro_lr: float = 3e-5\n",
    "    neuro_dropout: float = 0.5\n",
    "\n",
    "    # Longitudinal settings\n",
    "    longitudinal_cycles: int = 8\n",
    "    additional_prune_per_cycle: float = 0.10\n",
    "    ketamine_maintenance_epochs: int = 2\n",
    "    ssri_maintenance_epochs: int = 5\n",
    "    neuro_maintenance_epochs: int = 3\n",
    "\n",
    "    # Acute relapse settings\n",
    "    acute_relapse_prune_amount: float = 0.30\n",
    "\n",
    "    # Dataset settings\n",
    "    finetune_dataset: str = \"databricks/databricks-dolly-15k\"\n",
    "    finetune_subset_size: int = 500\n",
    "    max_seq_length: int = 512\n",
    "\n",
    "\n",
    "# Initialize global config\n",
    "CONFIG = ExperimentConfig()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 4: TREATMENT DOSE DATACLASS\n",
    "# ============================================================================\n",
    "@dataclass\n",
    "class TreatmentDose:\n",
    "    \"\"\"\n",
    "    Represents a treatment's computational \"dose\" and parameters.\n",
    "    Extended with scoring and comparison methods for sweep optimization.\n",
    "    \"\"\"\n",
    "    treatment_name: str\n",
    "    lora_rank: int\n",
    "    lora_alpha: int\n",
    "    epochs: int\n",
    "    lr: float\n",
    "    dropout: float\n",
    "    target_modules: List[str]\n",
    "    trainable_params: int = 0\n",
    "    estimated_flops: float = 0.0\n",
    "\n",
    "    # Results storage\n",
    "    results: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    def compute_flops(self, dataset_size: int, seq_length: int,\n",
    "                      hidden_size: int = 2048, num_layers: int = 16) -> float:\n",
    "        \"\"\"Estimate training FLOPs for this treatment configuration.\"\"\"\n",
    "        params_per_module = 2 * self.lora_rank * hidden_size\n",
    "        num_adapted_modules = len(self.target_modules) * num_layers\n",
    "        self.trainable_params = params_per_module * num_adapted_modules\n",
    "\n",
    "        flops_per_sample = 6 * self.trainable_params * seq_length\n",
    "        self.estimated_flops = flops_per_sample * dataset_size * self.epochs\n",
    "\n",
    "        return self.estimated_flops\n",
    "\n",
    "    def get_config_key(self) -> str:\n",
    "        \"\"\"Generate a unique key for this configuration.\"\"\"\n",
    "        modules_key = \"_\".join(sorted([m.split(\"_\")[0] for m in self.target_modules]))\n",
    "        return f\"r{self.lora_rank}_e{self.epochs}_lr{self.lr:.0e}_d{self.dropout}_{modules_key}\"\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert to dictionary for serialization.\"\"\"\n",
    "        return {\n",
    "            'treatment_name': self.treatment_name,\n",
    "            'lora_rank': self.lora_rank,\n",
    "            'lora_alpha': self.lora_alpha,\n",
    "            'epochs': self.epochs,\n",
    "            'lr': self.lr,\n",
    "            'dropout': self.dropout,\n",
    "            'target_modules': self.target_modules,\n",
    "            'trainable_params': self.trainable_params,\n",
    "            'estimated_flops': self.estimated_flops,\n",
    "            'results': self.results\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SweepResult:\n",
    "    \"\"\"\n",
    "    Stores results from a single sweep configuration.\n",
    "    Includes composite scoring for optimization.\n",
    "    \"\"\"\n",
    "    dose: TreatmentDose\n",
    "    seed: int\n",
    "    model_id: str\n",
    "\n",
    "    # Core metrics\n",
    "    post_treatment_composite: float = 0.0\n",
    "    post_treatment_arc: float = 0.0\n",
    "    post_treatment_lambada: float = 0.0\n",
    "\n",
    "    # Recovery metrics\n",
    "    recovery_from_baseline: float = 0.0\n",
    "\n",
    "    # Relapse metrics\n",
    "    post_relapse_composite: float = 0.0\n",
    "    relapse_drop: float = 0.0\n",
    "\n",
    "    # Longitudinal metrics (optional)\n",
    "    longitudinal_final: float = 0.0\n",
    "    longitudinal_drop: float = 0.0\n",
    "\n",
    "    # Efficiency metrics\n",
    "    efficiency: float = 0.0  # Recovery per PetaFLOP\n",
    "\n",
    "    # Timing\n",
    "    train_time: float = 0.0\n",
    "\n",
    "    def calculate_composite_score(self,\n",
    "                                   recovery_weight: float = 0.4,\n",
    "                                   resilience_weight: float = 0.3,\n",
    "                                   efficiency_weight: float = 0.2,\n",
    "                                   durability_weight: float = 0.1) -> float:\n",
    "        \"\"\"\n",
    "        Calculate composite optimization score.\n",
    "\n",
    "        SCORING COMPONENTS:\n",
    "        1. Recovery: Primary measure of treatment effectiveness\n",
    "        2. Resilience: Inverse of relapse drop (resistance to acute stress)\n",
    "        3. Efficiency: Recovery per unit computational cost\n",
    "        4. Durability: Longitudinal stability (if measured)\n",
    "\n",
    "        Higher score = better treatment configuration.\n",
    "        \"\"\"\n",
    "        # Normalize components to similar scales\n",
    "        recovery_score = self.recovery_from_baseline  # Already in % points\n",
    "\n",
    "        # Resilience: lower drop is better, invert and scale\n",
    "        resilience_score = max(0, 10 - self.relapse_drop)  # 0-10 scale\n",
    "\n",
    "        # Efficiency: recovery per PetaFLOP, scale to similar range\n",
    "        efficiency_score = self.efficiency * 0.5  # Scale factor\n",
    "\n",
    "        # Durability: if not measured, use resilience as proxy\n",
    "        if self.longitudinal_drop > 0:\n",
    "            durability_score = max(0, 20 - self.longitudinal_drop)\n",
    "        else:\n",
    "            durability_score = resilience_score\n",
    "\n",
    "        composite = (\n",
    "            recovery_weight * recovery_score +\n",
    "            resilience_weight * resilience_score +\n",
    "            efficiency_weight * efficiency_score +\n",
    "            durability_weight * durability_score\n",
    "        )\n",
    "\n",
    "        return composite\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert to dictionary for serialization.\"\"\"\n",
    "        return {\n",
    "            'dose': self.dose.to_dict(),\n",
    "            'seed': self.seed,\n",
    "            'model_id': self.model_id,\n",
    "            'post_treatment_composite': self.post_treatment_composite,\n",
    "            'post_treatment_arc': self.post_treatment_arc,\n",
    "            'post_treatment_lambada': self.post_treatment_lambada,\n",
    "            'recovery_from_baseline': self.recovery_from_baseline,\n",
    "            'post_relapse_composite': self.post_relapse_composite,\n",
    "            'relapse_drop': self.relapse_drop,\n",
    "            'longitudinal_final': self.longitudinal_final,\n",
    "            'longitudinal_drop': self.longitudinal_drop,\n",
    "            'efficiency': self.efficiency,\n",
    "            'train_time': self.train_time,\n",
    "            'composite_score': self.calculate_composite_score()\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 5: MODEL LOADING UTILITIES\n",
    "# ============================================================================\n",
    "def load_model_for_sweep(model_id: str, config: ExperimentConfig) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"\n",
    "    Load a model for sweep experiments.\n",
    "    Handles different model architectures and provides architecture detection.\n",
    "    \"\"\"\n",
    "    print(f\"  Loading model: {model_id}\")\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_id,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        # Detect architecture\n",
    "        architecture = detect_model_architecture(model)\n",
    "\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"    Architecture: {architecture}\")\n",
    "        print(f\"    Parameters: {total_params:,}\")\n",
    "        print(f\"    Device: {next(model.parameters()).device}\")\n",
    "\n",
    "        return model, tokenizer, architecture\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    [ERROR] Failed to load {model_id}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def detect_model_architecture(model) -> str:\n",
    "    \"\"\"\n",
    "    Detect the model architecture type for appropriate module targeting.\n",
    "\n",
    "    Returns one of: 'llama', 'mistral', 'phi', 'qwen', 'gemma', 'unknown'\n",
    "    \"\"\"\n",
    "    model_type = getattr(model.config, 'model_type', 'unknown').lower()\n",
    "\n",
    "    architecture_map = {\n",
    "        'llama': 'llama',\n",
    "        'mistral': 'mistral',\n",
    "        'phi': 'phi',\n",
    "        'phi3': 'phi',\n",
    "        'qwen': 'qwen',\n",
    "        'qwen2': 'qwen',\n",
    "        'gemma': 'gemma',\n",
    "        'gemma2': 'gemma',\n",
    "    }\n",
    "\n",
    "    return architecture_map.get(model_type, 'unknown')\n",
    "\n",
    "\n",
    "def get_target_modules_for_architecture(architecture: str, config_name: str,\n",
    "                                         sweep_config: KetamineSweepConfig) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get appropriate target modules for a given architecture and configuration.\n",
    "\n",
    "    Different architectures may have different module naming conventions.\n",
    "    This function handles the mapping.\n",
    "    \"\"\"\n",
    "    base_modules = sweep_config.target_module_configs.get(\n",
    "        config_name,\n",
    "        sweep_config.target_module_configs['full']\n",
    "    )\n",
    "\n",
    "    # Architecture-specific module name mappings\n",
    "    # Most Llama-like models (Mistral, Qwen, etc.) use the same naming\n",
    "    # Phi models may use different names\n",
    "\n",
    "    if architecture == 'phi':\n",
    "        # Phi-3 uses different naming\n",
    "        name_map = {\n",
    "            'q_proj': 'qkv_proj',  # Phi combines QKV\n",
    "            'k_proj': 'qkv_proj',\n",
    "            'v_proj': 'qkv_proj',\n",
    "            'o_proj': 'o_proj',\n",
    "            'gate_proj': 'gate_up_proj',  # Phi combines gate and up\n",
    "            'up_proj': 'gate_up_proj',\n",
    "            'down_proj': 'down_proj'\n",
    "        }\n",
    "        # Get unique modules after mapping\n",
    "        mapped = list(set(name_map.get(m, m) for m in base_modules))\n",
    "        return mapped\n",
    "\n",
    "    # Default: return base modules (works for Llama, Mistral, Qwen, Gemma)\n",
    "    return base_modules\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 6: EVALUATION FUNCTIONS\n",
    "# ============================================================================\n",
    "def load_evaluation_datasets(config: ExperimentConfig = None,\n",
    "                              max_samples: int = None) -> Dict:\n",
    "    \"\"\"Load evaluation datasets for measuring knowledge fragility.\"\"\"\n",
    "    if config is None:\n",
    "        config = CONFIG\n",
    "\n",
    "    if max_samples is None:\n",
    "        max_samples = config.eval_samples\n",
    "\n",
    "    datasets = {}\n",
    "\n",
    "    try:\n",
    "        arc = load_dataset(\n",
    "            \"allenai/ai2_arc\",\n",
    "            \"ARC-Easy\",\n",
    "            split=\"test\"\n",
    "        ).shuffle(seed=42).select(range(min(max_samples, 2376)))\n",
    "        datasets['arc'] = arc\n",
    "        print(f\"    ARC-Easy: {len(arc)} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"    [WARNING] ARC-Easy load failed: {e}\")\n",
    "        datasets['arc'] = None\n",
    "\n",
    "    try:\n",
    "        lambada = load_dataset(\n",
    "            \"lambada\",\n",
    "            split=\"test\"\n",
    "        ).shuffle(seed=42).select(range(min(max_samples // 2, 2500)))\n",
    "        datasets['lambada'] = lambada\n",
    "        print(f\"    LAMBADA: {len(lambada)} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"    [WARNING] LAMBADA load failed: {e}\")\n",
    "        datasets['lambada'] = None\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def evaluate_arc_easy(model, tokenizer, dataset, max_samples: int = None) -> Dict:\n",
    "    \"\"\"Evaluate model on ARC-Easy multiple choice questions.\"\"\"\n",
    "    if dataset is None:\n",
    "        return {'accuracy': 0.0, 'error': 'Dataset not loaded'}\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    samples = dataset if max_samples is None else dataset.select(\n",
    "        range(min(max_samples, len(dataset)))\n",
    "    )\n",
    "\n",
    "    for example in samples:\n",
    "        question = example['question']\n",
    "        answer_key = example['answerKey']\n",
    "        choices = example['choices']\n",
    "\n",
    "        choice_text = \"\\n\".join([\n",
    "            f\"{label}: {text}\"\n",
    "            for label, text in zip(choices['label'], choices['text'])\n",
    "        ])\n",
    "\n",
    "        prompt = f\"Question: {question}\\n\\nChoices:\\n{choice_text}\\n\\nAnswer:\"\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,\n",
    "                temperature=0.0,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = generated[len(prompt):].strip().upper()\n",
    "\n",
    "        if answer_key.upper() in response[:5]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    accuracy = 100.0 * correct / total if total > 0 else 0.0\n",
    "    return {'accuracy': accuracy, 'correct': correct, 'total': total}\n",
    "\n",
    "\n",
    "def evaluate_lambada(model, tokenizer, dataset, max_samples: int = None) -> Dict:\n",
    "    \"\"\"Evaluate model on LAMBADA word prediction task.\"\"\"\n",
    "    if dataset is None:\n",
    "        return {'accuracy': 0.0, 'error': 'Dataset not loaded'}\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    samples = dataset if max_samples is None else dataset.select(\n",
    "        range(min(max_samples, len(dataset)))\n",
    "    )\n",
    "\n",
    "    for example in samples:\n",
    "        text = example['text']\n",
    "        words = text.split()\n",
    "\n",
    "        if len(words) < 2:\n",
    "            continue\n",
    "\n",
    "        target_word = words[-1].lower().strip('.,!?')\n",
    "        context = ' '.join(words[:-1])\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            context,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=5,\n",
    "                temperature=0.0,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        prediction = generated[len(context):].strip().lower().split()[0] if generated[len(context):].strip() else \"\"\n",
    "        prediction = prediction.strip('.,!?')\n",
    "\n",
    "        if target_word == prediction or target_word.startswith(prediction):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    accuracy = 100.0 * correct / total if total > 0 else 0.0\n",
    "    return {'accuracy': accuracy, 'correct': correct, 'total': total}\n",
    "\n",
    "\n",
    "def evaluate_model_quick(model, tokenizer, eval_datasets: Dict,\n",
    "                          max_samples: int = 50) -> Dict:\n",
    "    \"\"\"Quick evaluation for sweep (fewer samples).\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    arc_results = evaluate_arc_easy(model, tokenizer, eval_datasets.get('arc'), max_samples)\n",
    "    results['arc_easy'] = arc_results['accuracy']\n",
    "\n",
    "    lambada_results = evaluate_lambada(model, tokenizer, eval_datasets.get('lambada'), max_samples // 2)\n",
    "    results['lambada'] = lambada_results['accuracy']\n",
    "\n",
    "    results['composite'] = (results['arc_easy'] + results['lambada']) / 2\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 7: DATASET PREPARATION\n",
    "# ============================================================================\n",
    "def prepare_finetune_dataset(tokenizer, config: ExperimentConfig, seed: int):\n",
    "    \"\"\"Prepare fine-tuning dataset for knowledge recovery training.\"\"\"\n",
    "    dataset = load_dataset(\n",
    "        config.finetune_dataset,\n",
    "        split=\"train\"\n",
    "    )\n",
    "\n",
    "    dataset = dataset.shuffle(seed=seed).select(\n",
    "        range(min(config.finetune_subset_size, len(dataset)))\n",
    "    )\n",
    "\n",
    "    def format_example(example):\n",
    "        instruction = example.get('instruction', example.get('context', ''))\n",
    "        response = example.get('response', example.get('text', ''))\n",
    "        text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n",
    "        return {'text': text}\n",
    "\n",
    "    dataset = dataset.map(format_example)\n",
    "\n",
    "    def tokenize(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            truncation=True,\n",
    "            max_length=config.max_seq_length,\n",
    "            padding='max_length'\n",
    "        )\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "    dataset.set_format('torch')\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 8: TREATMENT APPLICATION\n",
    "# ============================================================================\n",
    "def apply_ketamine_dose(base_model, tokenizer, train_dataset,\n",
    "                         dose: TreatmentDose, config: ExperimentConfig,\n",
    "                         seed: int) -> Tuple[nn.Module, Dict]:\n",
    "    \"\"\"\n",
    "    Apply a Ketamine-like treatment with specified dose parameters.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    model = copy.deepcopy(base_model)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=dose.lora_rank,\n",
    "        lora_alpha=dose.lora_alpha,\n",
    "        target_modules=dose.target_modules,\n",
    "        lora_dropout=dose.dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=os.path.join(config.ketamine_sweep.sweep_output_dir,\n",
    "                                f\"sweep_{dose.get_config_key()}_seed{seed}\"),\n",
    "        num_train_epochs=dose.epochs,\n",
    "        learning_rate=dose.lr,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_ratio=0.1,\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"no\",\n",
    "        fp16=True,\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    trainer.train()\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    stats = {\n",
    "        'trainable_params': trainable_params,\n",
    "        'train_time': train_time,\n",
    "        'estimated_flops': dose.estimated_flops\n",
    "    }\n",
    "\n",
    "    return model, stats\n",
    "\n",
    "\n",
    "def apply_acute_relapse(model, prune_amount: float = 0.30):\n",
    "    \"\"\"Simulate acute relapse by applying unstructured pruning.\"\"\"\n",
    "    pruned_layers = 0\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            try:\n",
    "                prune.l1_unstructured(module, name='weight', amount=prune_amount)\n",
    "                prune.remove(module, 'weight')\n",
    "                pruned_layers += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 9: KETAMINE ISODOSE SWEEP\n",
    "# ============================================================================\n",
    "def generate_isodose_sweep_doses(config: ExperimentConfig,\n",
    "                                  base_flops: float) -> List[TreatmentDose]:\n",
    "    \"\"\"\n",
    "    Generate Ketamine doses for isodose sweep.\n",
    "\n",
    "    ISODOSE PRINCIPLE:\n",
    "    Vary the LoRA rank (capacity/structural regrowth strength) while\n",
    "    adjusting epochs to maintain constant computational cost (FLOPs).\n",
    "\n",
    "    This isolates the effect of STRUCTURAL CAPACITY from total effort.\n",
    "    \"\"\"\n",
    "    sweep_config = config.ketamine_sweep\n",
    "    doses = []\n",
    "\n",
    "    target_modules = sweep_config.target_module_configs[sweep_config.default_target_config]\n",
    "\n",
    "    for rank in sweep_config.ranks:\n",
    "        # Create dose with this rank\n",
    "        dose = TreatmentDose(\n",
    "            treatment_name='ketamine',\n",
    "            lora_rank=rank,\n",
    "            lora_alpha=int(rank * sweep_config.alpha_multiplier),\n",
    "            epochs=3,  # Placeholder, will be adjusted\n",
    "            lr=sweep_config.learning_rates[0],\n",
    "            dropout=sweep_config.dropouts[0],\n",
    "            target_modules=target_modules\n",
    "        )\n",
    "\n",
    "        # Calculate FLOPs for 1 epoch to determine epoch scaling\n",
    "        dose.compute_flops(config.finetune_subset_size, config.max_seq_length)\n",
    "        flops_per_epoch = dose.estimated_flops / dose.epochs\n",
    "\n",
    "        # Calculate epochs needed to match base_flops\n",
    "        isodose_epochs = max(1, int(round(base_flops / flops_per_epoch)))\n",
    "\n",
    "        # Update dose with isodose-calibrated epochs\n",
    "        dose.epochs = isodose_epochs\n",
    "        dose.estimated_flops = flops_per_epoch * isodose_epochs\n",
    "\n",
    "        doses.append(dose)\n",
    "\n",
    "    return doses\n",
    "\n",
    "\n",
    "def generate_budget_sweep_doses(config: ExperimentConfig,\n",
    "                                 optimal_rank: int,\n",
    "                                 base_flops: float) -> List[TreatmentDose]:\n",
    "    \"\"\"\n",
    "    Generate Ketamine doses for budget sweep.\n",
    "\n",
    "    BUDGET SWEEP PRINCIPLE:\n",
    "    At the optimal rank found from isodose sweep, vary the total\n",
    "    computational budget to find the point of diminishing returns.\n",
    "    \"\"\"\n",
    "    sweep_config = config.ketamine_sweep\n",
    "    doses = []\n",
    "\n",
    "    target_modules = sweep_config.target_module_configs[sweep_config.default_target_config]\n",
    "\n",
    "    for multiplier in sweep_config.budget_multipliers:\n",
    "        target_flops = base_flops * multiplier\n",
    "\n",
    "        dose = TreatmentDose(\n",
    "            treatment_name='ketamine',\n",
    "            lora_rank=optimal_rank,\n",
    "            lora_alpha=int(optimal_rank * sweep_config.alpha_multiplier),\n",
    "            epochs=3,  # Placeholder\n",
    "            lr=sweep_config.learning_rates[0],\n",
    "            dropout=sweep_config.dropouts[0],\n",
    "            target_modules=target_modules\n",
    "        )\n",
    "\n",
    "        dose.compute_flops(config.finetune_subset_size, config.max_seq_length)\n",
    "        flops_per_epoch = dose.estimated_flops / dose.epochs\n",
    "\n",
    "        budget_epochs = max(1, int(round(target_flops / flops_per_epoch)))\n",
    "        dose.epochs = budget_epochs\n",
    "        dose.estimated_flops = flops_per_epoch * budget_epochs\n",
    "\n",
    "        doses.append(dose)\n",
    "\n",
    "    return doses\n",
    "\n",
    "\n",
    "def generate_target_module_sweep_doses(config: ExperimentConfig,\n",
    "                                        optimal_rank: int,\n",
    "                                        base_flops: float) -> List[TreatmentDose]:\n",
    "    \"\"\"\n",
    "    Generate Ketamine doses varying target modules.\n",
    "\n",
    "    MODULE SWEEP PRINCIPLE:\n",
    "    Test whether the structural regrowth benefit comes from:\n",
    "    - Attention layers (q, k, v, o projections)\n",
    "    - MLP layers (gate, up, down projections)\n",
    "    - Or both together\n",
    "    \"\"\"\n",
    "    sweep_config = config.ketamine_sweep\n",
    "    doses = []\n",
    "\n",
    "    for config_name, modules in sweep_config.target_module_configs.items():\n",
    "        dose = TreatmentDose(\n",
    "            treatment_name=f'ketamine_{config_name}',\n",
    "            lora_rank=optimal_rank,\n",
    "            lora_alpha=int(optimal_rank * sweep_config.alpha_multiplier),\n",
    "            epochs=3,\n",
    "            lr=sweep_config.learning_rates[0],\n",
    "            dropout=sweep_config.dropouts[0],\n",
    "            target_modules=modules\n",
    "        )\n",
    "\n",
    "        dose.compute_flops(config.finetune_subset_size, config.max_seq_length)\n",
    "        flops_per_epoch = dose.estimated_flops / dose.epochs\n",
    "\n",
    "        isodose_epochs = max(1, int(round(base_flops / flops_per_epoch)))\n",
    "        dose.epochs = isodose_epochs\n",
    "        dose.estimated_flops = flops_per_epoch * isodose_epochs\n",
    "\n",
    "        doses.append(dose)\n",
    "\n",
    "    return doses\n",
    "\n",
    "\n",
    "def run_single_dose_evaluation(dose: TreatmentDose,\n",
    "                                model_id: str,\n",
    "                                config: ExperimentConfig,\n",
    "                                seed: int,\n",
    "                                eval_datasets: Dict,\n",
    "                                train_dataset,\n",
    "                                baseline_composite: float,\n",
    "                                test_relapse: bool = True,\n",
    "                                test_longitudinal: bool = False) -> SweepResult:\n",
    "    \"\"\"\n",
    "    Run evaluation for a single dose configuration.\n",
    "    Returns a SweepResult with all metrics.\n",
    "    \"\"\"\n",
    "    print(f\"\\n    Testing: rank={dose.lora_rank}, epochs={dose.epochs}, \"\n",
    "          f\"FLOPs={dose.estimated_flops:.2e}\")\n",
    "\n",
    "    # Load fresh model\n",
    "    model, tokenizer, architecture = load_model_for_sweep(model_id, config)\n",
    "\n",
    "    # Adapt target modules for architecture if needed\n",
    "    adapted_modules = get_target_modules_for_architecture(\n",
    "        architecture,\n",
    "        config.ketamine_sweep.default_target_config,\n",
    "        config.ketamine_sweep\n",
    "    )\n",
    "    dose.target_modules = adapted_modules\n",
    "\n",
    "    # Apply treatment\n",
    "    treated_model, stats = apply_ketamine_dose(\n",
    "        model, tokenizer, train_dataset, dose, config, seed\n",
    "    )\n",
    "\n",
    "    # Evaluate post-treatment\n",
    "    post_treatment = evaluate_model_quick(\n",
    "        treated_model, tokenizer, eval_datasets,\n",
    "        max_samples=config.ketamine_sweep.sweep_eval_samples\n",
    "    )\n",
    "\n",
    "    recovery = post_treatment['composite'] - baseline_composite\n",
    "    efficiency = recovery / (dose.estimated_flops / 1e15) if dose.estimated_flops > 0 else 0\n",
    "\n",
    "    print(f\"      Post-treatment: {post_treatment['composite']:.1f}% \"\n",
    "          f\"(recovery: {recovery:+.1f}%, efficiency: {efficiency:.2f})\")\n",
    "\n",
    "    # Create result object\n",
    "    result = SweepResult(\n",
    "        dose=dose,\n",
    "        seed=seed,\n",
    "        model_id=model_id,\n",
    "        post_treatment_composite=post_treatment['composite'],\n",
    "        post_treatment_arc=post_treatment['arc_easy'],\n",
    "        post_treatment_lambada=post_treatment['lambada'],\n",
    "        recovery_from_baseline=recovery,\n",
    "        efficiency=efficiency,\n",
    "        train_time=stats['train_time']\n",
    "    )\n",
    "\n",
    "    # Test relapse resilience\n",
    "    if test_relapse:\n",
    "        apply_acute_relapse(treated_model, config.acute_relapse_prune_amount)\n",
    "        post_relapse = evaluate_model_quick(\n",
    "            treated_model, tokenizer, eval_datasets,\n",
    "            max_samples=config.ketamine_sweep.sweep_eval_samples // 2\n",
    "        )\n",
    "        result.post_relapse_composite = post_relapse['composite']\n",
    "        result.relapse_drop = post_treatment['composite'] - post_relapse['composite']\n",
    "        print(f\"      Post-relapse: {post_relapse['composite']:.1f}% \"\n",
    "              f\"(drop: {result.relapse_drop:.1f}%)\")\n",
    "\n",
    "    # Clean up\n",
    "    del treated_model, model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def run_ketamine_isodose_sweep(config: ExperimentConfig) -> Dict:\n",
    "    \"\"\"\n",
    "    Run the Ketamine isodose sweep to find optimal rank.\n",
    "\n",
    "    SWEEP PROCEDURE:\n",
    "    1. Establish baseline (untreated pruned model performance)\n",
    "    2. Calculate base FLOPs from reference configuration\n",
    "    3. Generate isodose configurations varying rank\n",
    "    4. Evaluate each configuration\n",
    "    5. Rank by composite score\n",
    "    6. Return top configurations for validation\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"KETAMINE ISODOSE SWEEP\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    sweep_config = config.ketamine_sweep\n",
    "    os.makedirs(sweep_config.sweep_output_dir, exist_ok=True)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 1: Load model and establish baseline\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n[STEP 1] Establishing baseline\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    model_id = sweep_config.generalization_models[0][0]\n",
    "    print(f\"  Primary model: {model_id}\")\n",
    "\n",
    "    model, tokenizer, architecture = load_model_for_sweep(model_id, config)\n",
    "\n",
    "    print(\"  Loading evaluation datasets...\")\n",
    "    eval_datasets = load_evaluation_datasets(config, sweep_config.sweep_eval_samples)\n",
    "\n",
    "    print(\"  Evaluating untreated baseline...\")\n",
    "    baseline = evaluate_model_quick(model, tokenizer, eval_datasets,\n",
    "                                     sweep_config.sweep_eval_samples)\n",
    "    baseline_composite = baseline['composite']\n",
    "    print(f\"  Baseline composite: {baseline_composite:.1f}%\")\n",
    "\n",
    "    # Prepare training dataset\n",
    "    print(\"  Preparing training dataset...\")\n",
    "    train_dataset = prepare_finetune_dataset(tokenizer, config, seed=0)\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 2: Calculate base FLOPs\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n[STEP 2] Calibrating base FLOPs\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    reference_dose = TreatmentDose(\n",
    "        treatment_name='ketamine_reference',\n",
    "        lora_rank=config.ketamine_lora_rank,\n",
    "        lora_alpha=config.ketamine_lora_alpha,\n",
    "        epochs=config.ketamine_epochs,\n",
    "        lr=config.ketamine_lr,\n",
    "        dropout=config.ketamine_dropout,\n",
    "        target_modules=config.ketamine_target_modules\n",
    "    )\n",
    "    base_flops = reference_dose.compute_flops(\n",
    "        config.finetune_subset_size,\n",
    "        config.max_seq_length\n",
    "    )\n",
    "\n",
    "    print(f\"  Reference rank: {config.ketamine_lora_rank}\")\n",
    "    print(f\"  Reference epochs: {config.ketamine_epochs}\")\n",
    "    print(f\"  Base FLOPs: {base_flops:.2e}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 3: Generate isodose sweep configurations\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n[STEP 3] Generating isodose sweep configurations\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    doses = generate_isodose_sweep_doses(config, base_flops)\n",
    "\n",
    "    print(f\"  Configurations to test: {len(doses)}\")\n",
    "    print(f\"\\n  {'Rank':>6} {'Alpha':>8} {'Epochs':>8} {'FLOPs':>14} {'Ratio':>8}\")\n",
    "    print(\"  \" + \"-\" * 50)\n",
    "\n",
    "    for dose in doses:\n",
    "        ratio = dose.estimated_flops / base_flops\n",
    "        print(f\"  {dose.lora_rank:>6} {dose.lora_alpha:>8} {dose.epochs:>8} \"\n",
    "              f\"{dose.estimated_flops:>14.2e} {ratio:>7.2f}x\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 4: Run sweep evaluations\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n[STEP 4] Running isodose sweep\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    sweep_results = []\n",
    "\n",
    "    for i, dose in enumerate(doses):\n",
    "        print(f\"\\n  Configuration {i+1}/{len(doses)}\")\n",
    "\n",
    "        for seed in range(sweep_config.sweep_seeds):\n",
    "            result = run_single_dose_evaluation(\n",
    "                dose=copy.deepcopy(dose),\n",
    "                model_id=model_id,\n",
    "                config=config,\n",
    "                seed=seed,\n",
    "                eval_datasets=eval_datasets,\n",
    "                train_dataset=train_dataset,\n",
    "                baseline_composite=baseline_composite,\n",
    "                test_relapse=sweep_config.test_relapse_in_sweep,\n",
    "                test_longitudinal=sweep_config.test_longitudinal_in_sweep\n",
    "            )\n",
    "            sweep_results.append(result)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 5: Rank configurations\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n[STEP 5] Ranking configurations\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Calculate composite scores\n",
    "    for result in sweep_results:\n",
    "        result.composite_score = result.calculate_composite_score()\n",
    "\n",
    "    # Sort by composite score\n",
    "    sweep_results.sort(key=lambda x: x.composite_score, reverse=True)\n",
    "\n",
    "    print(f\"\\n  ISODOSE SWEEP RANKINGS (by composite score)\")\n",
    "    print(f\"\\n  {'Rank':>4} {'LoRA r':>8} {'Epochs':>8} {'Recovery':>10} \"\n",
    "          f\"{'Efficiency':>12} {'Relapse':>10} {'Score':>8}\")\n",
    "    print(\"  \" + \"-\" * 70)\n",
    "\n",
    "    for i, result in enumerate(sweep_results):\n",
    "        print(f\"  {i+1:>4} {result.dose.lora_rank:>8} {result.dose.epochs:>8} \"\n",
    "              f\"{result.recovery_from_baseline:>+9.1f}% \"\n",
    "              f\"{result.efficiency:>11.2f} \"\n",
    "              f\"{result.relapse_drop:>9.1f}% \"\n",
    "              f\"{result.composite_score:>8.2f}\")\n",
    "\n",
    "    # Identify optimal configuration\n",
    "    optimal_result = sweep_results[0]\n",
    "    optimal_rank = optimal_result.dose.lora_rank\n",
    "\n",
    "    print(f\"\\n  OPTIMAL CONFIGURATION:\")\n",
    "    print(f\"    LoRA rank: {optimal_rank}\")\n",
    "    print(f\"    Epochs: {optimal_result.dose.epochs}\")\n",
    "    print(f\"    Recovery: {optimal_result.recovery_from_baseline:+.1f}%\")\n",
    "    print(f\"    Efficiency: {optimal_result.efficiency:.2f} recovery/PetaFLOP\")\n",
    "    print(f\"    Relapse drop: {optimal_result.relapse_drop:.1f}%\")\n",
    "    print(f\"    Composite score: {optimal_result.composite_score:.2f}\")\n",
    "\n",
    "    # Save sweep results\n",
    "    sweep_data = {\n",
    "        'baseline_composite': baseline_composite,\n",
    "        'base_flops': base_flops,\n",
    "        'optimal_rank': optimal_rank,\n",
    "        'results': [r.to_dict() for r in sweep_results]\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(sweep_config.sweep_output_dir, 'isodose_sweep_results.json'), 'w') as f:\n",
    "        json.dump(sweep_data, f, indent=2)\n",
    "\n",
    "    print(f\"\\n  [SAVED] Results saved to {sweep_config.sweep_output_dir}/isodose_sweep_results.json\")\n",
    "\n",
    "    return {\n",
    "        'sweep_results': sweep_results,\n",
    "        'optimal_rank': optimal_rank,\n",
    "        'optimal_result': optimal_result,\n",
    "        'baseline_composite': baseline_composite,\n",
    "        'base_flops': base_flops,\n",
    "        'eval_datasets': eval_datasets,\n",
    "        'train_dataset': train_dataset\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 10: GENERALIZATION TEST\n",
    "# ============================================================================\n",
    "def run_generalization_test(optimal_dose: TreatmentDose,\n",
    "                             sweep_data: Dict,\n",
    "                             config: ExperimentConfig) -> Dict:\n",
    "    \"\"\"\n",
    "    Test the optimal Ketamine dose on additional model architectures.\n",
    "\n",
    "    GENERALIZATION HYPOTHESIS:\n",
    "    If the optimal Ketamine-like treatment generalizes across architectures:\n",
    "    - Structural regrowth (high-rank LoRA) is a fundamental recovery mechanism\n",
    "    - The approach can be recommended as a general pruning recovery strategy\n",
    "\n",
    "    If it doesn't generalize:\n",
    "    - The optimal dose may be architecture-specific\n",
    "    - Different pruning methods may require different recovery strategies\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"GENERALIZATION TEST\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    sweep_config = config.ketamine_sweep\n",
    "\n",
    "    if len(sweep_config.generalization_models) < 2:\n",
    "        print(\"  [SKIP] No additional models configured for generalization test\")\n",
    "        print(\"  To enable generalization testing, add models to:\")\n",
    "        print(\"    config.ketamine_sweep.generalization_models\")\n",
    "        return {'skipped': True, 'reason': 'No additional models configured'}\n",
    "\n",
    "    print(f\"  Optimal dose: rank={optimal_dose.lora_rank}, epochs={optimal_dose.epochs}\")\n",
    "    print(f\"  Models to test: {len(sweep_config.generalization_models) - 1}\")\n",
    "\n",
    "    generalization_results = {}\n",
    "\n",
    "    # Get primary model results for comparison\n",
    "    primary_model_id = sweep_config.generalization_models[0][0]\n",
    "    primary_result = sweep_data.get('optimal_result')\n",
    "\n",
    "    if primary_result:\n",
    "        generalization_results[primary_model_id] = {\n",
    "            'description': sweep_config.generalization_models[0][1],\n",
    "            'recovery': primary_result.recovery_from_baseline,\n",
    "            'efficiency': primary_result.efficiency,\n",
    "            'relapse_drop': primary_result.relapse_drop,\n",
    "            'is_primary': True\n",
    "        }\n",
    "\n",
    "    # Test on additional models\n",
    "    for model_id, description, architecture in sweep_config.generalization_models[1:]:\n",
    "        print(f\"\\n  Testing on: {description}\")\n",
    "        print(f\"    Model: {model_id}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        try:\n",
    "            # Load model\n",
    "            model, tokenizer, detected_arch = load_model_for_sweep(model_id, config)\n",
    "\n",
    "            # Establish baseline for this model\n",
    "            print(\"    Evaluating baseline...\")\n",
    "            eval_datasets = load_evaluation_datasets(config, sweep_config.sweep_eval_samples)\n",
    "            baseline = evaluate_model_quick(model, tokenizer, eval_datasets,\n",
    "                                            sweep_config.sweep_eval_samples)\n",
    "            baseline_composite = baseline['composite']\n",
    "            print(f\"    Baseline: {baseline_composite:.1f}%\")\n",
    "\n",
    "            # Prepare training data\n",
    "            train_dataset = prepare_finetune_dataset(tokenizer, config, seed=0)\n",
    "\n",
    "            del model\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # Create adapted dose for this architecture\n",
    "            adapted_dose = copy.deepcopy(optimal_dose)\n",
    "            adapted_dose.target_modules = get_target_modules_for_architecture(\n",
    "                detected_arch,\n",
    "                sweep_config.default_target_config,\n",
    "                sweep_config\n",
    "            )\n",
    "\n",
    "            # Recalculate FLOPs for this architecture (may differ slightly)\n",
    "            adapted_dose.compute_flops(config.finetune_subset_size, config.max_seq_length)\n",
    "\n",
    "            # Run validation seeds\n",
    "            seed_results = []\n",
    "            for seed in range(sweep_config.validation_seeds):\n",
    "                print(f\"\\n    Seed {seed + 1}/{sweep_config.validation_seeds}\")\n",
    "\n",
    "                result = run_single_dose_evaluation(\n",
    "                    dose=copy.deepcopy(adapted_dose),\n",
    "                    model_id=model_id,\n",
    "                    config=config,\n",
    "                    seed=seed,\n",
    "                    eval_datasets=eval_datasets,\n",
    "                    train_dataset=train_dataset,\n",
    "                    baseline_composite=baseline_composite,\n",
    "                    test_relapse=True,\n",
    "                    test_longitudinal=False\n",
    "                )\n",
    "                seed_results.append(result)\n",
    "\n",
    "            # Aggregate results\n",
    "            avg_recovery = np.mean([r.recovery_from_baseline for r in seed_results])\n",
    "            std_recovery = np.std([r.recovery_from_baseline for r in seed_results])\n",
    "            avg_efficiency = np.mean([r.efficiency for r in seed_results])\n",
    "            avg_relapse = np.mean([r.relapse_drop for r in seed_results])\n",
    "\n",
    "            generalization_results[model_id] = {\n",
    "                'description': description,\n",
    "                'architecture': detected_arch,\n",
    "                'baseline': baseline_composite,\n",
    "                'recovery': avg_recovery,\n",
    "                'recovery_std': std_recovery,\n",
    "                'efficiency': avg_efficiency,\n",
    "                'relapse_drop': avg_relapse,\n",
    "                'is_primary': False,\n",
    "                'n_seeds': len(seed_results)\n",
    "            }\n",
    "\n",
    "            print(f\"\\n    Results for {description}:\")\n",
    "            print(f\"      Recovery: {avg_recovery:+.1f}% ± {std_recovery:.1f}%\")\n",
    "            print(f\"      Efficiency: {avg_efficiency:.2f}\")\n",
    "            print(f\"      Relapse drop: {avg_relapse:.1f}%\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    [ERROR] Failed to test {model_id}: {e}\")\n",
    "            generalization_results[model_id] = {\n",
    "                'description': description,\n",
    "                'error': str(e),\n",
    "                'is_primary': False\n",
    "            }\n",
    "            continue\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # GENERALIZATION ANALYSIS\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"GENERALIZATION ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    successful_models = {k: v for k, v in generalization_results.items()\n",
    "                         if 'error' not in v and 'recovery' in v}\n",
    "\n",
    "    if len(successful_models) >= 2:\n",
    "        recoveries = [v['recovery'] for v in successful_models.values()]\n",
    "        efficiencies = [v['efficiency'] for v in successful_models.values()]\n",
    "\n",
    "        # Calculate generalization metrics\n",
    "        recovery_mean = np.mean(recoveries)\n",
    "        recovery_std = np.std(recoveries)\n",
    "        recovery_range = max(recoveries) - min(recoveries)\n",
    "\n",
    "        efficiency_mean = np.mean(efficiencies)\n",
    "        efficiency_std = np.std(efficiencies)\n",
    "\n",
    "        print(f\"\\n  CROSS-MODEL STATISTICS (n={len(successful_models)} models):\")\n",
    "        print(f\"    Recovery: {recovery_mean:+.1f}% ± {recovery_std:.1f}%\")\n",
    "        print(f\"    Recovery range: {recovery_range:.1f}%\")\n",
    "        print(f\"    Efficiency: {efficiency_mean:.2f} ± {efficiency_std:.2f}\")\n",
    "\n",
    "        # Generalization score: lower std relative to mean = better generalization\n",
    "        if recovery_mean != 0:\n",
    "            generalization_coefficient = recovery_std / abs(recovery_mean)\n",
    "            print(f\"\\n    Generalization coefficient (CV): {generalization_coefficient:.2f}\")\n",
    "\n",
    "            if generalization_coefficient < 0.3:\n",
    "                print(\"    → STRONG GENERALIZATION: Optimal dose transfers well\")\n",
    "            elif generalization_coefficient < 0.5:\n",
    "                print(\"    → MODERATE GENERALIZATION: Some architecture dependence\")\n",
    "            else:\n",
    "                print(\"    → WEAK GENERALIZATION: Dose may be architecture-specific\")\n",
    "\n",
    "        # Compare to primary model\n",
    "        if primary_model_id in successful_models:\n",
    "            primary_recovery = successful_models[primary_model_id]['recovery']\n",
    "            for model_id, data in successful_models.items():\n",
    "                if model_id != primary_model_id:\n",
    "                    delta = data['recovery'] - primary_recovery\n",
    "                    print(f\"\\n    {data['description']}:\")\n",
    "                    print(f\"      Delta vs primary: {delta:+.1f}%\")\n",
    "\n",
    "    # Save generalization results\n",
    "    with open(os.path.join(sweep_config.sweep_output_dir,\n",
    "                           'generalization_results.json'), 'w') as f:\n",
    "        json.dump(generalization_results, f, indent=2, default=str)\n",
    "\n",
    "    print(f\"\\n  [SAVED] Generalization results saved\")\n",
    "\n",
    "    return generalization_results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 11: MAIN SWEEP RUNNER\n",
    "# ============================================================================\n",
    "def run_full_ketamine_optimization(config: ExperimentConfig = None):\n",
    "    \"\"\"\n",
    "    Run the complete Ketamine optimization pipeline:\n",
    "    1. Isodose sweep to find optimal rank\n",
    "    2. (Optional) Budget sweep at optimal rank\n",
    "    3. (Optional) Target module sweep\n",
    "    4. Generalization test on other architectures\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = CONFIG\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"#\" * 80)\n",
    "    print(\"#\" * 80)\n",
    "    print(\"#\" + \" \" * 78 + \"#\")\n",
    "    print(\"#\" + \" KETAMINE ISODOSE OPTIMIZATION & GENERALIZATION TEST \".center(78) + \"#\")\n",
    "    print(\"#\" + \" \" * 78 + \"#\")\n",
    "    print(\"#\" * 80)\n",
    "    print(\"#\" * 80)\n",
    "\n",
    "    sweep_config = config.ketamine_sweep\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"OPTIMIZATION CONFIGURATION\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"  Sweep mode: {sweep_config.sweep_mode}\")\n",
    "    print(f\"  Ranks to test: {sweep_config.ranks}\")\n",
    "    print(f\"  Sweep seeds: {sweep_config.sweep_seeds}\")\n",
    "    print(f\"  Validation seeds: {sweep_config.validation_seeds}\")\n",
    "    print(f\"  Top-K to validate: {sweep_config.top_k_validate}\")\n",
    "    print(f\"  Test relapse: {sweep_config.test_relapse_in_sweep}\")\n",
    "    print(f\"  Test longitudinal: {sweep_config.test_longitudinal_in_sweep}\")\n",
    "    print(f\"  Generalization models: {len(sweep_config.generalization_models)}\")\n",
    "\n",
    "    for model_id, desc, arch in sweep_config.generalization_models:\n",
    "        print(f\"    - {desc} ({arch})\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PHASE 1: Isodose Sweep\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 1: ISODOSE SWEEP\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    sweep_data = run_ketamine_isodose_sweep(config)\n",
    "    results['isodose_sweep'] = sweep_data\n",
    "\n",
    "    optimal_rank = sweep_data['optimal_rank']\n",
    "    optimal_result = sweep_data['optimal_result']\n",
    "    base_flops = sweep_data['base_flops']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PHASE 2: Validation with more seeds (top configurations)\n",
    "    # -------------------------------------------------------------------------\n",
    "    if sweep_config.validation_seeds > sweep_config.sweep_seeds:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"PHASE 2: VALIDATION OF TOP CONFIGURATIONS\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        top_results = sweep_data['sweep_results'][:sweep_config.top_k_validate]\n",
    "\n",
    "        validation_results = []\n",
    "\n",
    "        for i, result in enumerate(top_results):\n",
    "            print(f\"\\n  Validating configuration {i+1}/{len(top_results)}: \"\n",
    "                  f\"rank={result.dose.lora_rank}\")\n",
    "\n",
    "            # Run additional seeds\n",
    "            model_id = sweep_config.generalization_models[0][0]\n",
    "\n",
    "            additional_seeds = []\n",
    "            for seed in range(sweep_config.sweep_seeds, sweep_config.validation_seeds):\n",
    "                val_result = run_single_dose_evaluation(\n",
    "                    dose=copy.deepcopy(result.dose),\n",
    "                    model_id=model_id,\n",
    "                    config=config,\n",
    "                    seed=seed,\n",
    "                    eval_datasets=sweep_data['eval_datasets'],\n",
    "                    train_dataset=sweep_data['train_dataset'],\n",
    "                    baseline_composite=sweep_data['baseline_composite'],\n",
    "                    test_relapse=True,\n",
    "                    test_longitudinal=False\n",
    "                )\n",
    "                additional_seeds.append(val_result)\n",
    "\n",
    "            # Combine with original seeds\n",
    "            all_seed_results = [result] + additional_seeds\n",
    "\n",
    "            avg_recovery = np.mean([r.recovery_from_baseline for r in all_seed_results])\n",
    "            std_recovery = np.std([r.recovery_from_baseline for r in all_seed_results])\n",
    "            avg_score = np.mean([r.calculate_composite_score() for r in all_seed_results])\n",
    "\n",
    "            validation_results.append({\n",
    "                'rank': result.dose.lora_rank,\n",
    "                'recovery_mean': avg_recovery,\n",
    "                'recovery_std': std_recovery,\n",
    "                'score_mean': avg_score,\n",
    "                'n_seeds': len(all_seed_results)\n",
    "            })\n",
    "\n",
    "            print(f\"    Validated recovery: {avg_recovery:+.1f}% ± {std_recovery:.1f}%\")\n",
    "            print(f\"    Validated score: {avg_score:.2f}\")\n",
    "\n",
    "        results['validation'] = validation_results\n",
    "\n",
    "        # Update optimal if validation changed ranking\n",
    "        validation_results.sort(key=lambda x: x['score_mean'], reverse=True)\n",
    "        validated_optimal_rank = validation_results[0]['rank']\n",
    "\n",
    "        if validated_optimal_rank != optimal_rank:\n",
    "            print(f\"\\n  [NOTE] Validation changed optimal rank: \"\n",
    "                  f\"{optimal_rank} -> {validated_optimal_rank}\")\n",
    "            optimal_rank = validated_optimal_rank\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PHASE 3: Generalization Test\n",
    "    # -------------------------------------------------------------------------\n",
    "    if config.run_generalization_test:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"PHASE 3: GENERALIZATION TEST\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # Create optimal dose for generalization\n",
    "        optimal_dose = TreatmentDose(\n",
    "            treatment_name='ketamine_optimal',\n",
    "            lora_rank=optimal_rank,\n",
    "            lora_alpha=int(optimal_rank * sweep_config.alpha_multiplier),\n",
    "            epochs=optimal_result.dose.epochs,\n",
    "            lr=sweep_config.learning_rates[0],\n",
    "            dropout=sweep_config.dropouts[0],\n",
    "            target_modules=sweep_config.target_module_configs[sweep_config.default_target_config]\n",
    "        )\n",
    "        optimal_dose.compute_flops(config.finetune_subset_size, config.max_seq_length)\n",
    "\n",
    "        gen_results = run_generalization_test(optimal_dose, sweep_data, config)\n",
    "        results['generalization'] = gen_results\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # FINAL SUMMARY\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"=\" * 80)\n",
    "    print(\"  KETAMINE OPTIMIZATION COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(f\"\\n  OPTIMAL KETAMINE DOSE:\")\n",
    "    print(f\"    LoRA rank: {optimal_rank}\")\n",
    "    print(f\"    Alpha: {int(optimal_rank * sweep_config.alpha_multiplier)}\")\n",
    "    print(f\"    Epochs (isodose): {optimal_result.dose.epochs}\")\n",
    "    print(f\"    Target FLOPs: {base_flops:.2e}\")\n",
    "\n",
    "    print(f\"\\n  PRIMARY MODEL PERFORMANCE:\")\n",
    "    print(f\"    Recovery: {optimal_result.recovery_from_baseline:+.1f}%\")\n",
    "    print(f\"    Efficiency: {optimal_result.efficiency:.2f} recovery/PetaFLOP\")\n",
    "    print(f\"    Relapse resilience: {optimal_result.relapse_drop:.1f}% drop\")\n",
    "\n",
    "    if 'generalization' in results and not results['generalization'].get('skipped'):\n",
    "        gen = results['generalization']\n",
    "        successful = {k: v for k, v in gen.items() if 'recovery' in v}\n",
    "        if len(successful) > 1:\n",
    "            recoveries = [v['recovery'] for v in successful.values()]\n",
    "            print(f\"\\n  GENERALIZATION:\")\n",
    "            print(f\"    Models tested: {len(successful)}\")\n",
    "            print(f\"    Recovery range: {min(recoveries):+.1f}% to {max(recoveries):+.1f}%\")\n",
    "            print(f\"    Mean recovery: {np.mean(recoveries):+.1f}%\")\n",
    "\n",
    "    print(\"\\n  RECOMMENDED CONFIGURATION FOR OTHER PRUNED LLMS:\")\n",
    "    print(f\"    lora_rank = {optimal_rank}\")\n",
    "    print(f\"    lora_alpha = {int(optimal_rank * sweep_config.alpha_multiplier)}\")\n",
    "    print(f\"    target_modules = {sweep_config.target_module_configs[sweep_config.default_target_config]}\")\n",
    "    print(f\"    dropout = {sweep_config.dropouts[0]}\")\n",
    "    print(f\"    learning_rate = {sweep_config.learning_rates[0]}\")\n",
    "    print(f\"    epochs = (calibrate to match {base_flops:.2e} FLOPs)\")\n",
    "\n",
    "    # Save final summary\n",
    "    summary = {\n",
    "        'optimal_rank': optimal_rank,\n",
    "        'optimal_alpha': int(optimal_rank * sweep_config.alpha_multiplier),\n",
    "        'optimal_epochs': optimal_result.dose.epochs,\n",
    "        'base_flops': base_flops,\n",
    "        'primary_recovery': optimal_result.recovery_from_baseline,\n",
    "        'primary_efficiency': optimal_result.efficiency,\n",
    "        'primary_relapse_drop': optimal_result.relapse_drop,\n",
    "        'target_modules': sweep_config.target_module_configs[sweep_config.default_target_config],\n",
    "        'dropout': sweep_config.dropouts[0],\n",
    "        'learning_rate': sweep_config.learning_rates[0]\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(sweep_config.sweep_output_dir, 'optimal_dose_summary.json'), 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(f\"\\n  [SAVED] All results saved to {sweep_config.sweep_output_dir}/\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 12: ENTRY POINT\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure experiment\n",
    "    config = ExperimentConfig()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # SWEEP CONFIGURATION\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Customize the sweep parameters here\n",
    "\n",
    "    # Ranks to test (capacity sweep)\n",
    "    config.ketamine_sweep.ranks = [16, 32, 64, 96, 128]\n",
    "\n",
    "    # Seeds for sweep (1 for fast, 3 for reliable)\n",
    "    config.ketamine_sweep.sweep_seeds = 1\n",
    "    config.ketamine_sweep.validation_seeds = 3\n",
    "\n",
    "    # Number of top configs to validate with more seeds\n",
    "    config.ketamine_sweep.top_k_validate = 2\n",
    "\n",
    "    # Generalization models\n",
    "    # Add additional pruned models here for cross-architecture testing\n",
    "    config.ketamine_sweep.generalization_models = [\n",
    "        (\"oopere/pruned60-llama-3.2-1B\", \"Llama-3.2-1B 60% pruned\", \"llama\"),  # Primary (keep)\n",
    "        (\"oopere/pruned40-gemma-2-2b\", \"Gemma-2-2B 40% MLP pruned (same pruning style/author)\", \"gemma\"),\n",
    "        (\"pszemraj/Mistral-7B-v0.3-prune6\", \"Mistral-7B layer-pruned (6 layers removed)\", \"mistral\"),\n",
    "        (\"pszemraj/Phi-3-small-8k-prune6\", \"Phi-3-small layer-pruned (6 layers removed)\", \"phi\"),\n",
    "    ]\n",
    "\n",
    "    # Run configuration\n",
    "    config.run_main_experiment = False  # Skip original 3-treatment comparison\n",
    "    config.run_ketamine_sweep = True    # Run Ketamine optimization\n",
    "    config.run_generalization_test = True  # Test on other architectures\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # RUN OPTIMIZATION\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXPERIMENT MODE SELECTION\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"  run_main_experiment: {config.run_main_experiment}\")\n",
    "    print(f\"  run_ketamine_sweep: {config.run_ketamine_sweep}\")\n",
    "    print(f\"  run_generalization_test: {config.run_generalization_test}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    if config.run_ketamine_sweep:\n",
    "        results = run_full_ketamine_optimization(config)\n",
    "    elif config.run_main_experiment:\n",
    "        # Run original experiment if needed\n",
    "        from dataclasses import replace\n",
    "        original_config = replace(config, isodose_mode=True)\n",
    "        # run_experiment(original_config)  # Uncomment to run\n",
    "        pass\n",
    "    else:\n",
    "        print(\"\\n[INFO] No experiment mode selected. Set run_ketamine_sweep=True or run_main_experiment=True\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b52c54ac3bce494d977a15b4d2e57c61",
      "81770e68c75d450691f4f573a9d98c7e",
      "9f709390e91545a1b107766176c460d1",
      "bc7656e7440e40fe8353cef68076e9ec",
      "d776662319614f57aa21cd6379ac0806",
      "1e94b81c7ae84d3e93be799deb461eb9",
      "cbb385d5e24b44b8b31471d3bd1d5f28",
      "b9808a620b6d4343ad9c5538034a3330",
      "627611efcca74b70a26279e2bc16df53",
      "d5f91bd0f2da425791c85fc907150344",
      "27035b9e09dd49fcbd21ef3f61a12887",
      "c1037d690def47089b358adfc58ca3d3",
      "f9f977f642aa422da9a4377dc2c9188d",
      "5aa1dab0c45e4f3ba729a5614bb072e6",
      "f7403f7eb07b48b7a3f12da13822a1f7",
      "03fcf7c2ce8e4586b1f3a0eaeee045b1",
      "33067f1326bc402c901e06e09f8ab1d5",
      "8e581dc0d8f641deb7060168322a7790",
      "2675d8fb8f224169a3901c1dd00f4b9a",
      "836a7994d236453fa198811fd66ce409",
      "612aa1d0611440cc92a493b97223d8b0",
      "ee959b8959cb427ca45f1de918033d0d",
      "44d3b6de46374b5f811a07bd6e8a4e52",
      "2d2d948b32e146abbf69bb4cfd402741",
      "b1f856e13aa44b9981281f5ba7594816",
      "888450c13f8e4577be0bef4a08a335b1",
      "bb9f5350a1784291bbb7aef57fb1b473",
      "cd414d03b90a45308a1305757437bc03",
      "8973702951e147bba694a13106e27542",
      "a63beab521be4a8c99f575282531e2ff",
      "c5c47e8fe4a54dcbbbe919c8437093cc",
      "9c164534d7404ba580dc23271e48c1b6",
      "62383fcad19c4da68d980fb26109c184",
      "e61a66b6b8dc49ddac19080e52999045",
      "69d7deb16f944d7086578f10cab9d495",
      "7782aeeb533d4f3991809918deb9c6b5",
      "cb144a1fadc042d8ba08237b56e3fa19",
      "9dc9e8a116a0441590f557829b6781f4",
      "a7cd08bbe9674f5ab1759fec4dd25cf8",
      "480bd1a745ac45c695d21e362212c2af",
      "dc0d291c516b415fa516104bb53cfe8e",
      "e1fee0fd07cc4515afc911d8805950f7",
      "d78095f6749149eea8f4547d9e8e7cee",
      "883f47b6899547628ff6b46e1cc53166",
      "65304f43a2564c298b2708db7cd6223a",
      "cfb71ce4c4904acaaf027aab642588be",
      "e0c7473ffabe4427b4e9f3726a86f87a",
      "3fe041299f1e4d6fba5cd4b73a900cf7",
      "8854b347f7d54a178e7e8d8482c5c3a9",
      "a92fab781cb64e0cb46858bfbd70c787",
      "b8e2a27a1ab9417595f6a7cc63b799b9",
      "9ff5b92511474d1e90c0ae811694275c",
      "af36d59e66584480a960791d79b39822",
      "83057128ebfc4b20b177e7d385bca98c",
      "abea970b36684963934eee2c431ce782",
      "05aca8585e5546d0b9618aaaa632ea73",
      "57fade2b873046a7b5aba39ab9136c36",
      "58cb422bd7564f6f8fe74b04451fb50f",
      "0ddd50c60f6042369807adf21e90417d",
      "470e7e18f249430ca800c3730944d3f1",
      "17d0913073934a2fa4ae2881790229bd",
      "5564ae6fe93c4775b3125996e76b5141",
      "358e2de3e584432fbbdf98c887f1a0d4",
      "28ae9282c12d49afb29a28eaa5b7d07d",
      "03e15a305b8d4e48b06571fc451ac842",
      "a5cd4bd08d594a95bfa503e142e5d16a",
      "f77bf9b4921e4053b5ff76ce2e44de55",
      "a4378ffbd1ba45139750b49bfcfdcd2e",
      "5d42f64af20b4c7496d27b240a34fc12",
      "708f0c2dc1c04fc7a6b436a98f263b08",
      "c0236a246cbd4d1e844c87dc8ab93375",
      "6c851b1637ce40a09da1323bbfe990bd",
      "149d10c8e751431a82d17dabed73a878",
      "bb27edfac04f4c68bfc4c6c4cacfbb85",
      "2e391fd5912b4580bc269f7ee0ae3985",
      "1ac87022fd7349119c2aca56ef0d788a",
      "950d667e67694e98a3f67c86d814d112",
      "6e3c1c9eb6234a4c9cd8d5fa56d59517",
      "a51e4ab69f344493a268f4d154f7e685",
      "fdcb64ca17134b19bd993067ee9ea9d7",
      "278e6c7ac7724655b4e444183fd3f4fa",
      "41d68c5f08ec444eb4fdd44b1d7df1de",
      "a8cd1001c0524ac6b2f1dcfb4379ccf4",
      "6fa98956fc5549c39b17aeac137b4cc6",
      "4938a61f8d3c4400bea4c11ce43ba641",
      "427d55cc743147258e6ddf642936c3c0",
      "adcaab96170e4f07b5a7af5ab4007b1f",
      "b2ffbde9a72848ad997c8eecc205a575",
      "fea24c27d0cf41e6a8897297c9675d2d",
      "1a28f9dd32534ad2bd06bc735e60277a",
      "96b6eb6a1f3f4d3f9bbe8755266dbf27",
      "c3aeb3138a324aa1a7d1e64e1151ebb6",
      "2f7b7ec6cfcd4b58b1410fcdbf9aef96",
      "a0493463c1a441cc9e58467787129992",
      "77d3d6129a5245ef8479348a776f9fff",
      "05fd992ad0374296a5412a7615b329e4",
      "64cfb1b2c2d34b38b412a877c4a89b1b",
      "26371713d0384b1b8f57a2a34bcc7a14",
      "8102d20845d24cd0a720daa0f7940f79",
      "91dcc08dd2994e08b6443dde58b3df4f",
      "1dfc534c560d49b498f90b4d52b15935",
      "5467e79e3a9f428189aa7b323b2009d6",
      "009abbf3b46c4ee5a827820584b70f82",
      "be236b366b3f4489a25a747be4ef1462",
      "15bf1531bf224fe6acaee988014b20c1",
      "fc867d26d8134787895b632df1d0db1b",
      "51e28861a509432e9ed946fe0a3cec8d",
      "6091fd908edc4e41a4ae7dc8069e425e",
      "b38f86e368bc4bdd98160e505d0e807e",
      "43a3164b02cc46ed8ed0219398a3be36",
      "724b29c8e3404a31886009ef5a0a938e",
      "cc57d0a1984d431d91c72a340d359c03",
      "bf66dd6ecca7408990f1e54bae1d6235",
      "f6b054667c914ff5ad44fddee6f79d0a",
      "214cd22d078c4e2dabba2a8330b53aab",
      "d3bc0306360e44e3bfc86e7cfe0be3c3",
      "d5204c6b04e445639cd5ce7ccc11fb3f",
      "2f25a37b94214841b0e4546fff98244e",
      "0c4b9ee42bf94baea66a63e0d4d79a1f",
      "6a0fce8aa3ae4b409cae604231b98ea2",
      "92e7cf9321454120b35aa619ae453757",
      "1d584694af46454f856f59d5b190602c",
      "798ff90a54c34454916a11bbc63f0626",
      "af4fc0171f1d4a10b3fb19f43ae059b4",
      "87f52628c87d4e17b81b921bd9b149ac",
      "275741987be44be09a403039dd9905c6",
      "3917611ec236453785728197747de59b",
      "0fff4e695d2443159ad1f5315811db31",
      "c949123a1faa4aea92732e2c3e0750f4",
      "3174ee0435764368bff5aeded5c229d1",
      "3dc77cb0db62437c86edb8e2fb7f99e2",
      "426fa8e64e0c4918bb3124747d096013",
      "4912bbd953504dbd932b831eff8109a8",
      "2131c0ffd67d4971850a66281123262e",
      "9e44a1d11da54601a775dc85fd641bfb",
      "5c5071ab4cde4db6a5ee43f39d81f423",
      "7a6777c4945642abb2fc461570d47088",
      "8eb7822f8f8340f8af9a0690fe2a07b2",
      "4958c91f49444645be5c777d0325534c",
      "f376fd52e2b34fac9071063a54a1ed6e",
      "33200e2268d74c99b609fb90434eb9cc",
      "f5cfede31065417a8eed604b9afdc4a9",
      "7ce8569fa08b46b9b3426debe6c3c3de",
      "270827b9f833471a9149debb34acd433",
      "4c1086a40ee249d2978960991133da26",
      "d19146f571b84243bdaa0e9ec7ca2d21",
      "cae0d522c2084134bb1c98d2fb160d38",
      "41e1f91152564131b55349190ba020af",
      "89c9035b61bc45de8427ad1cfa3a85c1",
      "5cc86d9ff36948c5be81025519c4b3c3",
      "78a58c79bbb044d7923fea156eaf7785",
      "f73d74a8a9dd45e6bfba4edbd5b949d0",
      "55229eca3ac44fe7887077da65e38fd0",
      "1548753e8f54403b8c0c9ca2ce3df591",
      "045e06aa59b944d794e3c9209a55bf7f",
      "bfc2a1efe8bb4a7089299b748a772836",
      "956609e5608f4783b501a70fb7553898",
      "810b05d2707249e3a58dbc913d362303",
      "3b3ddf63bc924a778075ec209d1566bc",
      "309b40f75b2843f59bb696358aba7ce4",
      "698b27e0e9c34d37b37c56efd6eb075d",
      "e26c00d228a546d7ae084b46aa84d962",
      "bc38adf8e5994a95aea5b44edabad0f4",
      "ee08e66c46504d4fa2b4e758e41c2d59",
      "6335768edac749619ed54fcbe2128794",
      "b7ce48deb3c74be6bd63a41a26d5e858",
      "f2cd7390f2614d16b844962eed8b7273",
      "00be12ce6e26436481204e96048ab013",
      "50a522c19011415a9b69e8c6e955046e",
      "daa87ffa97924d9eb7e18eb621a9f707",
      "588413b56c754d6287dc0fe271bc6fbe",
      "23f21ab468fa4b209dde6e675f3c1815",
      "f98da033a92c4a3b9e482f3e65b8e073",
      "d8b3d783e1644e3e91c031faf3a9ed79",
      "4dab268b65eb41239c9fad54ecc0542e",
      "da0458ca75b74c06bf77975574e9a036",
      "1505f5ae74674a24a640f6cea79344ce",
      "80ab2dbfd9134c719c7345e6709b2432",
      "96d40500445043428ca1ade3c8f25142",
      "9878704518b04ada9f9fccc409d039c5",
      "d059bb6bcf72442990faf2bf758b01bf",
      "4a1f29a3fac74065b693f79a21067ffa",
      "b2f03868bb704825b548677680e20df4",
      "4919b5f4b1af4289b728d2712982a25d",
      "2377236de7314121885e2e8cf44c3cb2",
      "0d60de3cf68541459ab74f684ae5cd93",
      "e221016ec51b419eb95d09b36defef41",
      "db6f8ddc71164e1d83386d0dc10f2ea1",
      "533976b6885a4a81aecd6197cf546bcf",
      "965618ecf31f46caa3519fa5c7b83f95",
      "5f82bf2c9b614a739a0425a0abd15329",
      "358bee1fa90841888d6b706edd55fbc7",
      "1e8733cc1a9d436885af73f10cf4f351",
      "4ce8724bb15d453395a3581faabe3f80",
      "084f5cd5430440e794caf166303490a1",
      "6eabecafcad54ad5935b07b165c6bda6",
      "fd5d69f39ac14bb184d88f3e36f34b8a",
      "7315fcb9107f4d30a82f108e8af53e14",
      "a950562162c2458c9ed31aea5243c0a1",
      "ba7d1074f1b04834b553828f603566db",
      "822377b0424e427f8c7aca65174a3b6d",
      "f50df3df00aa4b81a3035b5c35e32d18",
      "b774e0d43b6b495db08f1c708d09db23",
      "f59031d788b646f99a1a88ef01b1646f",
      "086d9aa1ad3c4084942db0154d6e97a3",
      "41871b312605497abd18c9f40cee2038",
      "c4fa2083c1b74a13a7eaa38edf9b6486",
      "123eb2ea2f7741feb5e179166b8d6808",
      "68aaeb91223044a7845fb6b422cb4f77",
      "e48bd9a84d954bddbf012baefd4818b4",
      "7237c2b1d6454adcb5110c61b9260cc3",
      "e1faece32e504829a99f7fefce05702c",
      "3c69c4e2520e4c3a93fc27cee786a65c",
      "5816c0ccd9fe4bd0899797f03b3b4b78",
      "2fa067dc34fa44eebcc7eb56cf7fd19e",
      "6061390a90b640bebb80a3cf05656cf2",
      "46147acfa48e42118bab8483ee469564",
      "ff9647da9a644b46b0a40e19e9cb8b7a",
      "a870a8652056413386cefcf789aebd9b",
      "97f3461a92bd49ee9cf679c8da1ad33a",
      "104ffc01a33c41c3b2645e78b83bf37e",
      "cd9802035858490fafa67642cede7db1",
      "d83eadf72e024de9afb8062468ca0fcb",
      "6f03b79407d04ecead24070b7ef28de6",
      "b3283f28dce1422b9ab8b85fde84a21b",
      "07e8c960cbf946ddb4c406156215f434",
      "522a93c3515944d3a59a091e733ad850",
      "459db8ba89774ce6bd2980a98f15f7db",
      "4faa242fe8d44c65bce9b0bc03cfe6db",
      "3a2a0335e75f4817be7613bd339a6664",
      "73c1f20996864a58a754d852bc296b8b",
      "57ae384e56f0400ca53427960207be9e",
      "77f3c1d946e849d3a9d0a2a0382bde09",
      "caf45dd34f2d49309d8f6143e47377f6",
      "c3c84e1b90774e5389b120400ca22df4",
      "8c31e58d124f4bcca6ebf3cded4364ea",
      "9c2b1fa0eb6a44b08b2e70e4d800652c",
      "24fdf6ae4a6441c49ececba38d53302f",
      "400e1926a4be4142ad3d7fbb98b5fa99",
      "92479e8ee8c94754abd6e66fbaf07da3",
      "9a3fe5db4fd749a18bede175e3eb9375",
      "2aa7c1f29b424f4f86a521c3e9e0ad30",
      "19951813612d43a1b849686a02a084c4",
      "5aecf1a9772d43e58a02088f058e176c",
      "ffef0a2a7eb84e23ba20f15265c0f872",
      "ea44274eb5cf4aa1ace261d16944e7a7",
      "7024638a31f8423daa3ef15c6c4c05cc",
      "c79b33156dc9438b8d05651f01de4246",
      "811cdfd5ee884532968b999f7955c1c8",
      "3767a382b60a4774ae9b4d5647aa172b",
      "dbfc105978b94b8894a9c4a1096a4a8a",
      "3ab8d9fd5e8c47ccbf00a94cfada7f9a",
      "b847fd08c9d14074b06084aa501ac299",
      "37df1cb171204b8691d5d6c8c020bbec",
      "4167cd101c4c4bf8bd42d1561e14aed3",
      "14109d5c546043bb95045d0becba8c6f",
      "7fae57473f034fa999e83c1635d8baf6",
      "4afc5f53aee04e09a249978d2d3465bb",
      "ee7fe6aae0094ce09e150485538adcbc",
      "1a717e6440d24bf0b5cb94c5ab902747",
      "cee7436a8a59424eacc1a9fad6b7acd3",
      "6092ee2358e54e008bb6be94ce1d1ec6",
      "c925ee7c7de8417598615202ae090156",
      "e4e4945a0ed5481e9ef11a6db4a935e7",
      "bba2c50c7bdb46feaaad6fc7e20bcec0",
      "5cac6f7e62e44b46886a965fa054a3cc",
      "580d5a42031842748d65c2dd574f5249",
      "abd2cac44e3d4fefa9c6934f53f60aca",
      "2b46ef86351945748ab5e9a2db8d1c10",
      "9ff785b0e0334610b0effaf7f4e91cac",
      "e9e5707105534da7a5de7d52defe5750",
      "725f0b84686b4f3eb5a42096263c4552",
      "9c11f11261034b11abba74a4b282d91c",
      "62bd5369bfdd47e6ba9a5a9cfdf04237",
      "ad14b1ca6adf4216825add6c8135c211",
      "aa468d508ecb49698518af6869103120",
      "8ed803fccce642d38f8b67bf77eaf3eb",
      "0e4a71721026427f911d26d5d05d0051",
      "b1b2bb6d6da74dd2ab7d79bc135651f4",
      "f23a66db56e94407a20a7fa7807d9b56",
      "5faeef1c50be467bb3171931072eea2b",
      "7868cf99aa174d5983d40a1d0896c2c4",
      "c870bdf2f99442b0a005f40ccf7ca9d3",
      "d8e13aeab81c40dea4bad32e3c8b26c9",
      "6d9bec756fb94b84a5151060d9ef0fcf",
      "d9fbf7588fd74f20953c7868ba15ec11",
      "a0ca6f84026f45888629011545a3bd0a",
      "138b1168801b40a99e3d08af69cdc4a8",
      "90449372926045c69d2f7abda4cf855c",
      "faf36b9e9fb64bfc8c2e21175feabb7a",
      "c0f82112f8944dc28a9e2aebadad8e82",
      "f2e3884bdd534acbb808f8d740db5c37",
      "67a031a0b7174d248a30d84e697855e3",
      "2cf0d218167548a1acd4f354c8f976a2",
      "56b20a276bb142eea38335b626544754",
      "f3ea000ea39f4dd4a286b76b62f6a071",
      "f8d8497fff3444da91091da417bcfe3b",
      "c0ab7ba9597a44d8a3e7e09bc91eac95",
      "0940c145b48643288d7e5839f9bf5ec5",
      "56b79c997cfe4b248cfd43061efaf503",
      "d02de173545c4c32a2761aff4bd39f23",
      "1c103f58f86640fc9738bebf36915d43",
      "19887f9d38464249942b01ef77ca9c96",
      "726681f770fb49869264fb50dd1f63dd",
      "bd71dc40a6b3446f8b84d11380f58bf5",
      "8ffe365c1cf74f2eb39001c1d4bfebca",
      "b4ed0c0a840e48eebd88624b1b5b6e42",
      "d50e2e8f6251409b83d5855e1ab72f7a",
      "16c1e8cfe6a8480e95476ba6d94c0982",
      "d596cd91d7fd4913b10a164a05f3ac80",
      "39f409d0cf7c42f18d2bf5407de73231",
      "e43057666ceb4f999656b54776518b99",
      "6260154e278b4836ad816c77afb9120b",
      "3dbdc5a0d1b74e3fbb08d5b6b2737b38",
      "f682f4f6caf24950b317a3d696644824",
      "f97bd6b76a134be2866f0323862628d2",
      "859fd4e64bf24b86bfe1e36121de1d85",
      "011a52ea3eb24a04a67d00e35e2fa838",
      "e090be2b19334b1c814d838010584c14",
      "12e59475192943398f7009cbb35fbbd8",
      "3833d37685814e648988ba5df4499036",
      "ed62b1cca55e4f6fb8a92a34eded2ebb",
      "ffa94fb9ffd34cfea696034cd8258391",
      "01afc77d8205412a830ab75c9c8806a2",
      "8e81ab9534b74e6186ffd78d1b197dd1",
      "eca86809de2c44639069dacdfc4cca8b",
      "c00f444776b24638bb2a66719a4fd4bd",
      "a8ea7bd0afd845cc9ed48ceca2f12285",
      "4c700ee93300447b97831f458df4777a",
      "aaa1debf12ad46adb5870cfbb16da4b8",
      "84d5926d6b48476dbdf35b30b68297e4",
      "7f9f1f4399a74aa1bf307641c89c3667",
      "937d51c063514989822c9a4ac6ac0263",
      "51650086365c4e3f93907a9df44f6862",
      "fea382f8be6a48f5800e59dce6123408",
      "e5ac335bc5f64928a621c9a6bc8f079d",
      "4e844445b2a04fefa8a07d2c4f9e0c00",
      "bea76ff95db04fe08a4e7bda7393a5d1",
      "6000fa3ea6924418afb9b0535b5a51bd",
      "a1cf4ba1da4744b0a861b55a9d1d2860",
      "6aa67c3f338f41cfad8c8c3a0029cbf0",
      "c94ce5c25f0b403f97fd3706170046e2",
      "e8cb1635c1d84a7eaaf594f7d22e38cd",
      "de5eb1f722ed469d8f53422326d09de2",
      "89ee979fef83435f9de20e1656114201",
      "a39a3ed6176e4039861fae048aa870ad",
      "ec405b9926134a6facb9471a066cd1f0",
      "d9c368f114a74d36af73d0251f253f93",
      "d09bf7a2a56f481aaab20da50d55ce27",
      "d5e655d584f640f4b06bda052fc1845f",
      "e3de65d7440c49cb974607cf9b76c28a",
      "ae1555d5b14942829e2659273815d65d",
      "876e595352864264875f19b3035914d5",
      "b60b533d654b4897ad4b1f3509b5b8ef",
      "98bbc3026f9e45b7add0b985a2037611",
      "52ed0d5cfd0b4480bb04e64b742db0a4",
      "32b3ae22b2fe47c981f3c14e48ecd11a",
      "18b1610edcbf4f11a0b6c76419dfbfe5",
      "6e49bda3df624e52ba59f4ba2850cf82",
      "bfb15512db1c47b0aae84f13ccbdb63c",
      "7faeae0b39e74e9fb58527cba1f85f3c",
      "7cc3c7484b1543f59b2eeec493896e02",
      "42a026017c194fcd80741cef5b445d4b",
      "b36bb99052244cabbdbffa45f5d43b20",
      "139fe4998c8541c5aaa286a4795a49e4",
      "0832de8d7bac42cbaecf006166623973",
      "9973830e63c141d6b8725dc7a561f6bd",
      "272bba0192d74e73b50283cc1df931c8",
      "e13a6867eb23402a9e7ceeac134bfe2a",
      "8402adc861d14a998a98503bf784103a",
      "cb5f11f6dfe74b47aca73476a55a6658",
      "5b95b604e9d64fba9542166e12a497d9",
      "a4da50869cc84d47a444ff400670eebb",
      "9467b5bbfe36491d8b2b5fb000650f91",
      "788c86eaa464482ab7bd26fce47589fb",
      "d27f7c0d98164512814172a29b1a457d",
      "e706821296044e80a385bed889e8272e",
      "739a892a227249af982269157e5759d5",
      "4b9a20a7b61a47e0a31027e143169e46",
      "69db1b6b52944694952e415af944dcf7",
      "2383ffc9df7a4d3c80d11fa55dea20d7",
      "987f1239238d4264b31129e1c56ad4dd",
      "264c6a86713140a6a3246e7e115958f6",
      "05330d2b43364b2c9cfbbc4f8958677c",
      "4c40ea2aaabf472694b5e02de430972e",
      "7376501605d84ba4a68dfcf3705b31c8",
      "74fbc82617794536aa0e6d8736315e13",
      "5524510386644ce0856fd70baea2e3a4",
      "e92a2f3b952244798e5d85a75feb9b48",
      "082890ae8e8f45f085318aceab1f80bb",
      "e81b7bd41a1d49a0a6fb82828da514e6",
      "989070ccd2564f20aca2511562be41e2",
      "d888b558d8404a1f8631bf870be59fc1",
      "f7c3490263204bb7988d48e970463d8c",
      "d7e509485cf84588b4e45b3df98e63c9",
      "b0a3161bf2304d04a8725935fc485bcf",
      "ae8dcd29761e42c395c75e9d107bc280",
      "a62aecaa2cbd43388b474574060b551e",
      "9cabd8e68e804967ae7301ca8cbe4744",
      "4cb2edfd20fb4b7ab04ca2ae4aa2c20d",
      "2adc7ac568d140f6a22742492697c060",
      "a90e5703b98b469180b00d5faba4fa3f",
      "241eacdeb33c48c2833cf80d9cabe2ce",
      "c2913a066db84712b65ac3aae84d5de1",
      "81d6fe5d4b6547149c1fe9863a3a8861",
      "2c72d51eaefe4abab142f31cbb7fb56a",
      "0dc9ecd4c9d045518b944c2ad6b2ff55"
     ]
    },
    "id": "PULm0S-evRQ1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1769778381862,
     "user_tz": -480,
     "elapsed": 2407848,
     "user": {
      "displayName": "Ngo Cheung",
      "userId": "02091267041339546959"
     }
    },
    "outputId": "2c8b3184-e885-46f6-d0db-1a1b9d0a00da"
   },
   "execution_count": null,
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXPERIMENT MODE SELECTION\n",
      "================================================================================\n",
      "  run_main_experiment: False\n",
      "  run_ketamine_sweep: True\n",
      "  run_generalization_test: True\n",
      "================================================================================\n",
      "\n",
      "\n",
      "################################################################################\n",
      "################################################################################\n",
      "#                                                                              #\n",
      "#             KETAMINE ISODOSE OPTIMIZATION & GENERALIZATION TEST              #\n",
      "#                                                                              #\n",
      "################################################################################\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "OPTIMIZATION CONFIGURATION\n",
      "================================================================================\n",
      "  Sweep mode: isodose\n",
      "  Ranks to test: [16, 32, 64, 96, 128]\n",
      "  Sweep seeds: 1\n",
      "  Validation seeds: 3\n",
      "  Top-K to validate: 2\n",
      "  Test relapse: True\n",
      "  Test longitudinal: False\n",
      "  Generalization models: 4\n",
      "    - Llama-3.2-1B 60% pruned (llama)\n",
      "    - Gemma-2-2B 40% MLP pruned (same pruning style/author) (gemma)\n",
      "    - Mistral-7B layer-pruned (6 layers removed) (mistral)\n",
      "    - Phi-3-small layer-pruned (6 layers removed) (phi)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PHASE 1: ISODOSE SWEEP\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "KETAMINE ISODOSE SWEEP\n",
      "================================================================================\n",
      "\n",
      "[STEP 1] Establishing baseline\n",
      "------------------------------------------------------------\n",
      "  Primary model: oopere/pruned60-llama-3.2-1B\n",
      "  Loading model: oopere/pruned60-llama-3.2-1B\n",
      "    Architecture: llama\n",
      "    Parameters: 752,650,240\n",
      "    Device: cuda:0\n",
      "  Loading evaluation datasets...\n",
      "    ARC-Easy: 100 samples\n",
      "    LAMBADA: 50 samples\n",
      "  Evaluating untreated baseline...\n",
      "  Baseline composite: 22.5%\n",
      "  Preparing training dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52c54ac3bce494d977a15b4d2e57c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1037d690def47089b358adfc58ca3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2] Calibrating base FLOPs\n",
      "------------------------------------------------------------\n",
      "  Reference rank: 64\n",
      "  Reference epochs: 3\n",
      "  Base FLOPs: 1.35e+14\n",
      "\n",
      "[STEP 3] Generating isodose sweep configurations\n",
      "------------------------------------------------------------\n",
      "  Configurations to test: 5\n",
      "\n",
      "    Rank    Alpha   Epochs          FLOPs    Ratio\n",
      "  --------------------------------------------------\n",
      "      16       32       12       1.35e+14    1.00x\n",
      "      32       64        6       1.35e+14    1.00x\n",
      "      64      128        3       1.35e+14    1.00x\n",
      "      96      192        2       1.35e+14    1.00x\n",
      "     128      256        2       1.80e+14    1.33x\n",
      "\n",
      "[STEP 4] Running isodose sweep\n",
      "------------------------------------------------------------\n",
      "\n",
      "  Configuration 1/5\n",
      "\n",
      "    Testing: rank=16, epochs=12, FLOPs=1.35e+14\n",
      "  Loading model: oopere/pruned60-llama-3.2-1B\n",
      "    Architecture: llama\n",
      "    Parameters: 752,650,240\n",
      "    Device: cuda:0\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='384' max='384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [384/384 04:02, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.909900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.760800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Post-treatment: 20.5% (recovery: -2.0%, efficiency: -14.78)\n",
      "      Post-relapse: 15.0% (drop: 5.5%)\n",
      "\n",
      "  Configuration 2/5\n",
      "\n",
      "    Testing: rank=32, epochs=6, FLOPs=1.35e+14\n",
      "  Loading model: oopere/pruned60-llama-3.2-1B\n",
      "    Architecture: llama\n",
      "    Parameters: 752,650,240\n",
      "    Device: cuda:0\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [192/192 02:02, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.649200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Post-treatment: 26.0% (recovery: +3.5%, efficiency: 25.87)\n",
      "      Post-relapse: 23.0% (drop: 3.0%)\n",
      "\n",
      "  Configuration 3/5\n",
      "\n",
      "    Testing: rank=64, epochs=3, FLOPs=1.35e+14\n",
      "  Loading model: oopere/pruned60-llama-3.2-1B\n",
      "    Architecture: llama\n",
      "    Parameters: 752,650,240\n",
      "    Device: cuda:0\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Post-treatment: 22.5% (recovery: +0.0%, efficiency: 0.00)\n",
      "      Post-relapse: 22.0% (drop: 0.5%)\n",
      "\n",
      "  Configuration 4/5\n",
      "\n",
      "    Testing: rank=96, epochs=2, FLOPs=1.35e+14\n",
      "  Loading model: oopere/pruned60-llama-3.2-1B\n",
      "    Architecture: llama\n",
      "    Parameters: 752,650,240\n",
      "    Device: cuda:0\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Post-treatment: 20.0% (recovery: -2.5%, efficiency: -18.48)\n",
      "      Post-relapse: 20.0% (drop: 0.0%)\n",
      "\n",
      "  Configuration 5/5\n",
      "\n",
      "    Testing: rank=128, epochs=2, FLOPs=1.80e+14\n",
      "  Loading model: oopere/pruned60-llama-3.2-1B\n",
      "    Architecture: llama\n",
      "    Parameters: 752,650,240\n",
      "    Device: cuda:0\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Post-treatment: 18.0% (recovery: -4.5%, efficiency: -24.95)\n",
      "      Post-relapse: 20.0% (drop: -2.0%)\n",
      "\n",
      "[STEP 5] Ranking configurations\n",
      "------------------------------------------------------------\n",
      "\n",
      "  ISODOSE SWEEP RANKINGS (by composite score)\n",
      "\n",
      "  Rank   LoRA r   Epochs   Recovery   Efficiency    Relapse    Score\n",
      "  ----------------------------------------------------------------------\n",
      "     1       32        6      +3.5%       25.87       3.0%     6.79\n",
      "     2       64        3      +0.0%        0.00       0.5%     3.80\n",
      "     3       96        2      -2.5%      -18.48       0.0%     1.15\n",
      "     4      128        2      -4.5%      -24.95      -2.0%     0.51\n",
      "     5       16       12      -2.0%      -14.78       5.5%    -0.48\n",
      "\n",
      "  OPTIMAL CONFIGURATION:\n",
      "    LoRA rank: 32\n",
      "    Epochs: 6\n",
      "    Recovery: +3.5%\n",
      "    Efficiency: 25.87 recovery/PetaFLOP\n",
      "    Relapse drop: 3.0%\n",
      "    Composite score: 6.79\n",
      "\n",
      "  [SAVED] Results saved to ./ketamine_sweep_results/isodose_sweep_results.json\n",
      "\n",
      "================================================================================\n",
      "PHASE 2: VALIDATION OF TOP CONFIGURATIONS\n",
      "================================================================================\n",
      "\n",
      "  Validating configuration 1/2: rank=32\n",
      "\n",
      "    Testing: rank=32, epochs=6, FLOPs=1.35e+14\n",
      "  Loading model: oopere/pruned60-llama-3.2-1B\n",
      "    Architecture: llama\n",
      "    Parameters: 752,650,240\n",
      "    Device: cuda:0\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [192/192 02:01, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.646900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Post-treatment: 23.0% (recovery: +0.5%, efficiency: 3.70)\n",
      "      Post-relapse: 21.0% (drop: 2.0%)\n",
      "\n",
      "    Testing: rank=32, epochs=6, FLOPs=1.35e+14\n",
      "  Loading model: oopere/pruned60-llama-3.2-1B\n",
      "    Architecture: llama\n",
      "    Parameters: 752,650,240\n",
      "    Device: cuda:0\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [192/192 02:01, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.649400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Post-treatment: 23.0% (recovery: +0.5%, efficiency: 3.70)\n",
      "      Post-relapse: 23.0% (drop: 0.0%)\n",
      "    Validated recovery: +1.5% ± 1.4%\n",
      "    Validated score: 5.04\n",
      "\n",
      "  Validating configuration 2/2: rank=64\n",
      "\n",
      "    Testing: rank=64, epochs=3, FLOPs=1.35e+14\n",
      "  Loading model: oopere/pruned60-llama-3.2-1B\n",
      "    Architecture: llama\n",
      "    Parameters: 752,650,240\n",
      "    Device: cuda:0\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Post-treatment: 22.5% (recovery: +0.0%, efficiency: 0.00)\n",
      "      Post-relapse: 17.0% (drop: 5.5%)\n",
      "\n",
      "    Testing: rank=64, epochs=3, FLOPs=1.35e+14\n",
      "  Loading model: oopere/pruned60-llama-3.2-1B\n",
      "    Architecture: llama\n",
      "    Parameters: 752,650,240\n",
      "    Device: cuda:0\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Post-treatment: 23.5% (recovery: +1.0%, efficiency: 7.39)\n",
      "      Post-relapse: 23.0% (drop: 0.5%)\n",
      "    Validated recovery: +0.3% ± 0.5%\n",
      "    Validated score: 3.51\n",
      "\n",
      "================================================================================\n",
      "PHASE 3: GENERALIZATION TEST\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "GENERALIZATION TEST\n",
      "================================================================================\n",
      "  Optimal dose: rank=32, epochs=6\n",
      "  Models to test: 3\n",
      "\n",
      "  Testing on: Gemma-2-2B 40% MLP pruned (same pruning style/author)\n",
      "    Model: oopere/pruned40-gemma-2-2b\n",
      "------------------------------------------------------------\n",
      "  Loading model: oopere/pruned40-gemma-2-2b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d3b6de46374b5f811a07bd6e8a4e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/858 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61a66b6b8dc49ddac19080e52999045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65304f43a2564c298b2708db7cd6223a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05aca8585e5546d0b9618aaaa632ea73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77bf9b4921e4053b5ff76ce2e44de55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3c1c9eb6234a4c9cd8d5fa56d59517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea24c27d0cf41e6a8897297c9675d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/522 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Architecture: gemma\n",
      "    Parameters: 1,951,923,456\n",
      "    Device: cuda:0\n",
      "    Evaluating baseline...\n",
      "    ARC-Easy: 100 samples\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    LAMBADA: 50 samples\n",
      "    Baseline: 32.0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91dcc08dd2994e08b6443dde58b3df4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Seed 1/3\n",
      "\n",
      "    Testing: rank=32, epochs=6, FLOPs=1.35e+14\n",
      "  Loading model: oopere/pruned40-gemma-2-2b\n",
      "    Architecture: gemma\n",
      "    Parameters: 1,951,923,456\n",
      "    Device: cuda:0\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [192/192 04:27, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.440000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      Post-treatment: 37.5% (recovery: +5.5%, efficiency: 40.65)\n",
      "      Post-relapse: 32.0% (drop: 5.5%)\n",
      "\n",
      "    Seed 2/3\n",
      "\n",
      "    Testing: rank=32, epochs=6, FLOPs=1.35e+14\n",
      "  Loading model: oopere/pruned40-gemma-2-2b\n",
      "    Architecture: gemma\n",
      "    Parameters: 1,951,923,456\n",
      "    Device: cuda:0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [192/192 04:27, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.441600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      Post-treatment: 35.0% (recovery: +3.0%, efficiency: 22.17)\n",
      "      Post-relapse: 31.0% (drop: 4.0%)\n",
      "\n",
      "    Seed 3/3\n",
      "\n",
      "    Testing: rank=32, epochs=6, FLOPs=1.35e+14\n",
      "  Loading model: oopere/pruned40-gemma-2-2b\n",
      "    Architecture: gemma\n",
      "    Parameters: 1,951,923,456\n",
      "    Device: cuda:0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [192/192 04:27, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.442100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      Post-treatment: 38.5% (recovery: +6.5%, efficiency: 48.04)\n",
      "      Post-relapse: 30.0% (drop: 8.5%)\n",
      "\n",
      "    Results for Gemma-2-2B 40% MLP pruned (same pruning style/author):\n",
      "      Recovery: +5.0% ± 1.5%\n",
      "      Efficiency: 36.96\n",
      "      Relapse drop: 6.0%\n",
      "\n",
      "  Testing on: Mistral-7B layer-pruned (6 layers removed)\n",
      "    Model: pszemraj/Mistral-7B-v0.3-prune6\n",
      "------------------------------------------------------------\n",
      "  Loading model: pszemraj/Mistral-7B-v0.3-prune6\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "724b29c8e3404a31886009ef5a0a938e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d584694af46454f856f59d5b190602c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4912bbd953504dbd932b831eff8109a8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "270827b9f833471a9149debb34acd433"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/2.06G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "045e06aa59b944d794e3c9209a55bf7f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7ce48deb3c74be6bd63a41a26d5e858"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1505f5ae74674a24a640f6cea79344ce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db6f8ddc71164e1d83386d0dc10f2ea1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a950562162c2458c9ed31aea5243c0a1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e48bd9a84d954bddbf012baefd4818b4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "104ffc01a33c41c3b2645e78b83bf37e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    Architecture: mistral\n",
      "    Parameters: 5,939,351,552\n",
      "    Device: cuda:0\n",
      "    Evaluating baseline...\n",
      "    ARC-Easy: 100 samples\n",
      "    LAMBADA: 50 samples\n",
      "    Baseline: 67.5%\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57ae384e56f0400ca53427960207be9e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "    Seed 1/3\n",
      "\n",
      "    Testing: rank=32, epochs=6, FLOPs=1.35e+14\n",
      "  Loading model: pszemraj/Mistral-7B-v0.3-prune6\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19951813612d43a1b849686a02a084c4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    Architecture: mistral\n",
      "    Parameters: 5,939,351,552\n",
      "    Device: cuda:0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [192/192 05:46, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.659100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      Post-treatment: 72.5% (recovery: +5.0%, efficiency: 36.96)\n",
      "      Post-relapse: 70.0% (drop: 2.5%)\n",
      "\n",
      "    Seed 2/3\n",
      "\n",
      "    Testing: rank=32, epochs=6, FLOPs=1.35e+14\n",
      "  Loading model: pszemraj/Mistral-7B-v0.3-prune6\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "37df1cb171204b8691d5d6c8c020bbec"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    Architecture: mistral\n",
      "    Parameters: 5,939,351,552\n",
      "    Device: cuda:0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [192/192 05:47, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.658800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      Post-treatment: 71.0% (recovery: +3.5%, efficiency: 25.87)\n",
      "      Post-relapse: 69.0% (drop: 2.0%)\n",
      "\n",
      "    Seed 3/3\n",
      "\n",
      "    Testing: rank=32, epochs=6, FLOPs=1.35e+14\n",
      "  Loading model: pszemraj/Mistral-7B-v0.3-prune6\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bba2c50c7bdb46feaaad6fc7e20bcec0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    Architecture: mistral\n",
      "    Parameters: 5,939,351,552\n",
      "    Device: cuda:0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [192/192 05:46, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.662100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      Post-treatment: 69.5% (recovery: +2.0%, efficiency: 14.78)\n",
      "      Post-relapse: 70.0% (drop: -0.5%)\n",
      "\n",
      "    Results for Mistral-7B layer-pruned (6 layers removed):\n",
      "      Recovery: +3.5% ± 1.2%\n",
      "      Efficiency: 25.87\n",
      "      Relapse drop: 1.3%\n",
      "\n",
      "  Testing on: Phi-3-small layer-pruned (6 layers removed)\n",
      "    Model: pszemraj/Phi-3-small-8k-prune6\n",
      "------------------------------------------------------------\n",
      "  Loading model: pszemraj/Phi-3-small-8k-prune6\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa468d508ecb49698518af6869103120"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "configuration_phi3_small.py: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0ca6f84026f45888629011545a3bd0a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenization_phi3_small.py: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c0ab7ba9597a44d8a3e7e09bc91eac95"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-small-8k-instruct:\n",
      "- tokenization_phi3_small.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-small-8k-instruct:\n",
      "- configuration_phi3_small.py\n",
      "- tokenization_phi3_small.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "modeling_phi3_small.py: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "16c1e8cfe6a8480e95476ba6d94c0982"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "positional_embedding.py: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "12e59475192943398f7009cbb35fbbd8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-small-8k-instruct:\n",
      "- positional_embedding.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "triton_blocksparse_attention_layer.py: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "84d5926d6b48476dbdf35b30b68297e4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "triton_flash_blocksparse_attn.py: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c94ce5c25f0b403f97fd3706170046e2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-small-8k-instruct:\n",
      "- triton_flash_blocksparse_attn.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-small-8k-instruct:\n",
      "- triton_blocksparse_attention_layer.py\n",
      "- triton_flash_blocksparse_attn.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-small-8k-instruct:\n",
      "- modeling_phi3_small.py\n",
      "- positional_embedding.py\n",
      "- triton_blocksparse_attention_layer.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "876e595352864264875f19b3035914d5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b36bb99052244cabbdbffa45f5d43b20"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "788c86eaa464482ab7bd26fce47589fb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/2.50G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7376501605d84ba4a68dfcf3705b31c8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.80G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae8dcd29761e42c395c75e9d107bc280"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    [ERROR] Failed to load pszemraj/Phi-3-small-8k-prune6: Flash Attention is not available, but is needed for dense attention\n",
      "    [ERROR] Failed to test pszemraj/Phi-3-small-8k-prune6: Flash Attention is not available, but is needed for dense attention\n",
      "\n",
      "================================================================================\n",
      "GENERALIZATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "  CROSS-MODEL STATISTICS (n=3 models):\n",
      "    Recovery: +4.0% ± 0.7%\n",
      "    Recovery range: 1.5%\n",
      "    Efficiency: 29.57 ± 5.23\n",
      "\n",
      "    Generalization coefficient (CV): 0.18\n",
      "    → STRONG GENERALIZATION: Optimal dose transfers well\n",
      "\n",
      "    Gemma-2-2B 40% MLP pruned (same pruning style/author):\n",
      "      Delta vs primary: +1.5%\n",
      "\n",
      "    Mistral-7B layer-pruned (6 layers removed):\n",
      "      Delta vs primary: +0.0%\n",
      "\n",
      "  [SAVED] Generalization results saved\n",
      "\n",
      "================================================================================\n",
      "================================================================================\n",
      "  KETAMINE OPTIMIZATION COMPLETE\n",
      "================================================================================\n",
      "================================================================================\n",
      "\n",
      "  OPTIMAL KETAMINE DOSE:\n",
      "    LoRA rank: 32\n",
      "    Alpha: 64\n",
      "    Epochs (isodose): 6\n",
      "    Target FLOPs: 1.35e+14\n",
      "\n",
      "  PRIMARY MODEL PERFORMANCE:\n",
      "    Recovery: +3.5%\n",
      "    Efficiency: 25.87 recovery/PetaFLOP\n",
      "    Relapse resilience: 3.0% drop\n",
      "\n",
      "  GENERALIZATION:\n",
      "    Models tested: 3\n",
      "    Recovery range: +3.5% to +5.0%\n",
      "    Mean recovery: +4.0%\n",
      "\n",
      "  RECOMMENDED CONFIGURATION FOR OTHER PRUNED LLMS:\n",
      "    lora_rank = 32\n",
      "    lora_alpha = 64\n",
      "    target_modules = ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
      "    dropout = 0.05\n",
      "    learning_rate = 5e-05\n",
      "    epochs = (calibrate to match 1.35e+14 FLOPs)\n",
      "\n",
      "  [SAVED] All results saved to ./ketamine_sweep_results/\n",
      "================================================================================\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The End"
   ],
   "metadata": {
    "id": "w-HVEGmIiRTx"
   }
  }
 ]
}
